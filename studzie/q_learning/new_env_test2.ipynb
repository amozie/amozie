{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import time\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from gym import Env, Space, spaces\n",
    "from gym.utils import seeding\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ListDiscrete(Space):\n",
    "    def __init__(self, l) -> None:\n",
    "        self.l = list(set(l))\n",
    "        self.n = len(self.l)\n",
    "    \n",
    "    def sample(self):\n",
    "        if self.n == 0:\n",
    "            return None\n",
    "        return np.random.choice(self.l)\n",
    "    \n",
    "    def contains(self, x):\n",
    "        return x in self.l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestEnv(Env):\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.__max_step = 9\n",
    "        \n",
    "        # 0-8分别代表此次落子位置\n",
    "        self.action_space = ListDiscrete(range(9))\n",
    "        # 初始全零，下标0为：1、2分别代表黑白两方（1黑方先行），下标1-9为：0空、1黑方、2白方\n",
    "        self.observation_space = spaces.Box(\n",
    "            np.zeros(10, int), np.zeros(10, int) + 2)\n",
    "        # 代表AI所属方（1或2）\n",
    "        self.ai = None\n",
    "        self._seed()\n",
    "        self.state = None\n",
    "        self.__step = None\n",
    "        # 当前落子方\n",
    "        self.current = None\n",
    "        # 已经落子格子\n",
    "        self.stated = None\n",
    "        self.__all_stated_set = set(range(9))\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        # 随机AI所属方\n",
    "        self.ai = np.random.choice([1, 2])\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.__step = 0\n",
    "        self.seed()\n",
    "        self.state = [self.ai]\n",
    "        self.state.extend(np.zeros(9, int).tolist())\n",
    "        self.current = 1\n",
    "        self.stated = []\n",
    "        print(self.state)\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def _step(self, action):\n",
    "        if action is None:\n",
    "            return np.array(self.state), 0, True, {}\n",
    "        grid = self.state[1:]\n",
    "        # 不可重复落子\n",
    "        if action in self.stated:\n",
    "            return np.array(self.state), -100, True, {}\n",
    "        # 黑方先手\n",
    "        grid = np.array(grid, int)\n",
    "        dif = np.sum(grid == 1) - np.sum(grid == 2)\n",
    "        if (dif == 0 and self.current == 1) or (dif == 1 and self.current == 2):\n",
    "            pass\n",
    "        else:\n",
    "            return np.array(self.state), -100, True, {}\n",
    "        # 胜负\n",
    "        current = self.current\n",
    "        grid[action] = current\n",
    "        self.state = [self.ai]\n",
    "        self.state.extend(grid.tolist())\n",
    "        self.current = 3 - current\n",
    "        self.stated.append(action)\n",
    "        self.action_space = ListDiscrete(self.__all_stated_set.difference(self.stated))\n",
    "        \n",
    "        is_win = lambda b: \\\n",
    "            b[0:3].all() or b[3:6].all() or b[6:9].all() or \\\n",
    "            b[0::3].all() or b[1::3].all() or b[2::3].all() or \\\n",
    "            b[0::4].all() or b[2:7:2].all()\n",
    "        if is_win(grid == current):\n",
    "            if self.ai == current:\n",
    "                reward = 20\n",
    "            else:\n",
    "                reward = -20\n",
    "            return np.array(self.state), reward, True, {}\n",
    "\n",
    "        reward = 0\n",
    "        self.__step += 1\n",
    "        if self.__step < self.__max_step:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def _render(self, mode='ansi', close=False):\n",
    "        table = PrettyTable()\n",
    "        table.header = False\n",
    "        grid = self.state[1:]\n",
    "        table.add_row(grid[0:3])\n",
    "        table.add_row(grid[3:6])\n",
    "        table.add_row(grid[6:9])\n",
    "        print(table)\n",
    "\n",
    "    def _close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "| 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "+---+---+---+\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "9\n",
      "+---+---+---+\n",
      "| 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "+---+---+---+\n"
     ]
    }
   ],
   "source": [
    "env = TestEnv();\n",
    "env.reset();\n",
    "nb_actions = env.action_space.n\n",
    "print(nb_actions)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "+---+---+---+\n",
      "| 0 | 1 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "+---+---+---+\n",
      "+---+---+---+\n",
      "| 0 | 1 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "| 2 | 0 | 0 |\n",
      "+---+---+---+\n",
      "+---+---+---+\n",
      "| 0 | 1 | 0 |\n",
      "| 0 | 1 | 0 |\n",
      "| 2 | 0 | 0 |\n",
      "+---+---+---+\n",
      "+---+---+---+\n",
      "| 0 | 1 | 0 |\n",
      "| 0 | 1 | 2 |\n",
      "| 2 | 0 | 0 |\n",
      "+---+---+---+\n",
      "+---+---+---+\n",
      "| 0 | 1 | 0 |\n",
      "| 0 | 1 | 2 |\n",
      "| 2 | 1 | 0 |\n",
      "+---+---+---+\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "env.reset();\n",
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    # print(action, reward, done, info)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 1, 0, 1, 2, 0, 0, 2, 1, 2]), 0, False, {})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_11 (Flatten)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 64)                704       \n",
      "_________________________________________________________________\n",
      "activation_37 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_38 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_39 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 9)                 585       \n",
      "_________________________________________________________________\n",
      "activation_40 (Activation)   (None, 9)                 0         \n",
      "=================================================================\n",
      "Total params: 9,609\n",
      "Trainable params: 9,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=1000,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 10000 steps ...\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    4/10000: episode: 1, duration: 0.234s, episode steps: 4, steps per second: 17, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "    6/10000: episode: 2, duration: 0.002s, episode steps: 2, steps per second: 1045, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   10/10000: episode: 3, duration: 0.003s, episode steps: 4, steps per second: 1283, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   13/10000: episode: 4, duration: 0.002s, episode steps: 3, steps per second: 1201, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   15/10000: episode: 5, duration: 0.002s, episode steps: 2, steps per second: 1091, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   20/10000: episode: 6, duration: 0.004s, episode steps: 5, steps per second: 1425, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   26/10000: episode: 7, duration: 0.004s, episode steps: 6, steps per second: 1441, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   32/10000: episode: 8, duration: 0.004s, episode steps: 6, steps per second: 1399, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   36/10000: episode: 9, duration: 0.003s, episode steps: 4, steps per second: 1176, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   40/10000: episode: 10, duration: 0.004s, episode steps: 4, steps per second: 1108, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   45/10000: episode: 11, duration: 0.005s, episode steps: 5, steps per second: 1045, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.200 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   51/10000: episode: 12, duration: 0.009s, episode steps: 6, steps per second: 692, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 4.167 [0.000, 8.000], mean observation: 0.700 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   57/10000: episode: 13, duration: 0.007s, episode steps: 6, steps per second: 893, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.500 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   64/10000: episode: 14, duration: 0.005s, episode steps: 7, steps per second: 1279, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.286 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   69/10000: episode: 15, duration: 0.005s, episode steps: 5, steps per second: 1102, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   76/10000: episode: 16, duration: 0.005s, episode steps: 7, steps per second: 1491, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   79/10000: episode: 17, duration: 0.002s, episode steps: 3, steps per second: 1240, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   81/10000: episode: 18, duration: 0.002s, episode steps: 2, steps per second: 1094, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   86/10000: episode: 19, duration: 0.004s, episode steps: 5, steps per second: 1359, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 1.800 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   91/10000: episode: 20, duration: 0.004s, episode steps: 5, steps per second: 1327, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "   97/10000: episode: 21, duration: 0.004s, episode steps: 6, steps per second: 1422, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  101/10000: episode: 22, duration: 0.003s, episode steps: 4, steps per second: 1330, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  105/10000: episode: 23, duration: 0.003s, episode steps: 4, steps per second: 1275, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.500 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  113/10000: episode: 24, duration: 0.005s, episode steps: 8, steps per second: 1456, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.625 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  117/10000: episode: 25, duration: 0.003s, episode steps: 4, steps per second: 1352, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [4.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  124/10000: episode: 26, duration: 0.005s, episode steps: 7, steps per second: 1427, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  128/10000: episode: 27, duration: 0.003s, episode steps: 4, steps per second: 1154, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  133/10000: episode: 28, duration: 0.004s, episode steps: 5, steps per second: 1294, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  136/10000: episode: 29, duration: 0.003s, episode steps: 3, steps per second: 1200, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  142/10000: episode: 30, duration: 0.004s, episode steps: 6, steps per second: 1479, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  147/10000: episode: 31, duration: 0.004s, episode steps: 5, steps per second: 1278, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  150/10000: episode: 32, duration: 0.003s, episode steps: 3, steps per second: 961, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [5.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  155/10000: episode: 33, duration: 0.004s, episode steps: 5, steps per second: 1347, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  157/10000: episode: 34, duration: 0.002s, episode steps: 2, steps per second: 1111, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  161/10000: episode: 35, duration: 0.003s, episode steps: 4, steps per second: 1381, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  167/10000: episode: 36, duration: 0.005s, episode steps: 6, steps per second: 1305, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  173/10000: episode: 37, duration: 0.004s, episode steps: 6, steps per second: 1448, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  180/10000: episode: 38, duration: 0.005s, episode steps: 7, steps per second: 1443, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  185/10000: episode: 39, duration: 0.004s, episode steps: 5, steps per second: 1340, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  187/10000: episode: 40, duration: 0.002s, episode steps: 2, steps per second: 1025, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  191/10000: episode: 41, duration: 0.003s, episode steps: 4, steps per second: 1386, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  195/10000: episode: 42, duration: 0.003s, episode steps: 4, steps per second: 1224, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.500 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  198/10000: episode: 43, duration: 0.003s, episode steps: 3, steps per second: 1195, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  201/10000: episode: 44, duration: 0.002s, episode steps: 3, steps per second: 1292, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  206/10000: episode: 45, duration: 0.004s, episode steps: 5, steps per second: 1167, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.620 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  211/10000: episode: 46, duration: 0.004s, episode steps: 5, steps per second: 1229, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  214/10000: episode: 47, duration: 0.002s, episode steps: 3, steps per second: 1209, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  217/10000: episode: 48, duration: 0.003s, episode steps: 3, steps per second: 1080, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [5.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  221/10000: episode: 49, duration: 0.003s, episode steps: 4, steps per second: 1252, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  223/10000: episode: 50, duration: 0.002s, episode steps: 2, steps per second: 1059, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  229/10000: episode: 51, duration: 0.004s, episode steps: 6, steps per second: 1463, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  234/10000: episode: 52, duration: 0.004s, episode steps: 5, steps per second: 1396, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  240/10000: episode: 53, duration: 0.004s, episode steps: 6, steps per second: 1406, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.167 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  243/10000: episode: 54, duration: 0.003s, episode steps: 3, steps per second: 1160, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  246/10000: episode: 55, duration: 0.002s, episode steps: 3, steps per second: 1244, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  250/10000: episode: 56, duration: 0.003s, episode steps: 4, steps per second: 1305, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  254/10000: episode: 57, duration: 0.003s, episode steps: 4, steps per second: 1295, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  259/10000: episode: 58, duration: 0.004s, episode steps: 5, steps per second: 1410, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  262/10000: episode: 59, duration: 0.003s, episode steps: 3, steps per second: 1190, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  265/10000: episode: 60, duration: 0.003s, episode steps: 3, steps per second: 997, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [3.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  269/10000: episode: 61, duration: 0.003s, episode steps: 4, steps per second: 1146, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.750 [5.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  272/10000: episode: 62, duration: 0.003s, episode steps: 3, steps per second: 1168, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  275/10000: episode: 63, duration: 0.003s, episode steps: 3, steps per second: 1195, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [4.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  280/10000: episode: 64, duration: 0.004s, episode steps: 5, steps per second: 1320, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  282/10000: episode: 65, duration: 0.002s, episode steps: 2, steps per second: 1065, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  287/10000: episode: 66, duration: 0.004s, episode steps: 5, steps per second: 1234, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  292/10000: episode: 67, duration: 0.004s, episode steps: 5, steps per second: 1354, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.200 [1.000, 7.000], mean observation: 0.620 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  296/10000: episode: 68, duration: 0.003s, episode steps: 4, steps per second: 1273, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  303/10000: episode: 69, duration: 0.009s, episode steps: 7, steps per second: 808, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.286 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  307/10000: episode: 70, duration: 0.005s, episode steps: 4, steps per second: 731, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  313/10000: episode: 71, duration: 0.007s, episode steps: 6, steps per second: 866, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.833 [4.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  317/10000: episode: 72, duration: 0.003s, episode steps: 4, steps per second: 1197, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  320/10000: episode: 73, duration: 0.003s, episode steps: 3, steps per second: 1132, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [1.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  326/10000: episode: 74, duration: 0.005s, episode steps: 6, steps per second: 1182, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  331/10000: episode: 75, duration: 0.004s, episode steps: 5, steps per second: 1312, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  338/10000: episode: 76, duration: 0.005s, episode steps: 7, steps per second: 1468, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 3.857 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  344/10000: episode: 77, duration: 0.004s, episode steps: 6, steps per second: 1423, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  348/10000: episode: 78, duration: 0.003s, episode steps: 4, steps per second: 1236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  352/10000: episode: 79, duration: 0.003s, episode steps: 4, steps per second: 1323, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  357/10000: episode: 80, duration: 0.004s, episode steps: 5, steps per second: 1427, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  361/10000: episode: 81, duration: 0.003s, episode steps: 4, steps per second: 1366, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  364/10000: episode: 82, duration: 0.002s, episode steps: 3, steps per second: 1271, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  368/10000: episode: 83, duration: 0.003s, episode steps: 4, steps per second: 1427, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  372/10000: episode: 84, duration: 0.003s, episode steps: 4, steps per second: 1397, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  377/10000: episode: 85, duration: 0.003s, episode steps: 5, steps per second: 1440, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [3.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  380/10000: episode: 86, duration: 0.003s, episode steps: 3, steps per second: 1195, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  382/10000: episode: 87, duration: 0.002s, episode steps: 2, steps per second: 1130, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  388/10000: episode: 88, duration: 0.005s, episode steps: 6, steps per second: 1322, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 6.167 [4.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  392/10000: episode: 89, duration: 0.003s, episode steps: 4, steps per second: 1298, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  396/10000: episode: 90, duration: 0.003s, episode steps: 4, steps per second: 1290, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.250 [5.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  401/10000: episode: 91, duration: 0.004s, episode steps: 5, steps per second: 1360, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  407/10000: episode: 92, duration: 0.004s, episode steps: 6, steps per second: 1342, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  411/10000: episode: 93, duration: 0.003s, episode steps: 4, steps per second: 1329, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  419/10000: episode: 94, duration: 0.005s, episode steps: 8, steps per second: 1464, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.375 [0.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  421/10000: episode: 95, duration: 0.002s, episode steps: 2, steps per second: 1051, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  423/10000: episode: 96, duration: 0.002s, episode steps: 2, steps per second: 895, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  427/10000: episode: 97, duration: 0.003s, episode steps: 4, steps per second: 1277, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  431/10000: episode: 98, duration: 0.003s, episode steps: 4, steps per second: 1319, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  433/10000: episode: 99, duration: 0.002s, episode steps: 2, steps per second: 1121, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  437/10000: episode: 100, duration: 0.003s, episode steps: 4, steps per second: 1345, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  442/10000: episode: 101, duration: 0.004s, episode steps: 5, steps per second: 1412, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  447/10000: episode: 102, duration: 0.004s, episode steps: 5, steps per second: 1272, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.620 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  452/10000: episode: 103, duration: 0.004s, episode steps: 5, steps per second: 1375, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  456/10000: episode: 104, duration: 0.003s, episode steps: 4, steps per second: 1316, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  460/10000: episode: 105, duration: 0.003s, episode steps: 4, steps per second: 1345, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  463/10000: episode: 106, duration: 0.003s, episode steps: 3, steps per second: 1183, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  468/10000: episode: 107, duration: 0.004s, episode steps: 5, steps per second: 1383, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [3.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  472/10000: episode: 108, duration: 0.003s, episode steps: 4, steps per second: 1335, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.750 [6.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  478/10000: episode: 109, duration: 0.004s, episode steps: 6, steps per second: 1396, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  483/10000: episode: 110, duration: 0.004s, episode steps: 5, steps per second: 1321, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.800 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  489/10000: episode: 111, duration: 0.004s, episode steps: 6, steps per second: 1434, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  494/10000: episode: 112, duration: 0.004s, episode steps: 5, steps per second: 1208, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [3.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  500/10000: episode: 113, duration: 0.005s, episode steps: 6, steps per second: 1255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  504/10000: episode: 114, duration: 0.003s, episode steps: 4, steps per second: 1177, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  509/10000: episode: 115, duration: 0.004s, episode steps: 5, steps per second: 1241, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  512/10000: episode: 116, duration: 0.002s, episode steps: 3, steps per second: 1218, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [0.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  518/10000: episode: 117, duration: 0.004s, episode steps: 6, steps per second: 1434, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  523/10000: episode: 118, duration: 0.004s, episode steps: 5, steps per second: 1352, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.800 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  525/10000: episode: 119, duration: 0.002s, episode steps: 2, steps per second: 1119, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  528/10000: episode: 120, duration: 0.003s, episode steps: 3, steps per second: 867, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  531/10000: episode: 121, duration: 0.004s, episode steps: 3, steps per second: 833, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  534/10000: episode: 122, duration: 0.003s, episode steps: 3, steps per second: 980, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  536/10000: episode: 123, duration: 0.002s, episode steps: 2, steps per second: 890, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  542/10000: episode: 124, duration: 0.006s, episode steps: 6, steps per second: 971, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  546/10000: episode: 125, duration: 0.005s, episode steps: 4, steps per second: 849, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  548/10000: episode: 126, duration: 0.002s, episode steps: 2, steps per second: 1044, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  551/10000: episode: 127, duration: 0.003s, episode steps: 3, steps per second: 938, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  553/10000: episode: 128, duration: 0.002s, episode steps: 2, steps per second: 1107, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  556/10000: episode: 129, duration: 0.003s, episode steps: 3, steps per second: 1196, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  560/10000: episode: 130, duration: 0.003s, episode steps: 4, steps per second: 1366, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  564/10000: episode: 131, duration: 0.003s, episode steps: 4, steps per second: 1376, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  569/10000: episode: 132, duration: 0.004s, episode steps: 5, steps per second: 1389, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  572/10000: episode: 133, duration: 0.002s, episode steps: 3, steps per second: 1263, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  575/10000: episode: 134, duration: 0.002s, episode steps: 3, steps per second: 1288, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [0.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  579/10000: episode: 135, duration: 0.003s, episode steps: 4, steps per second: 1442, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  582/10000: episode: 136, duration: 0.002s, episode steps: 3, steps per second: 1313, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  589/10000: episode: 137, duration: 0.005s, episode steps: 7, steps per second: 1520, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  591/10000: episode: 138, duration: 0.002s, episode steps: 2, steps per second: 1024, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  597/10000: episode: 139, duration: 0.004s, episode steps: 6, steps per second: 1384, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  602/10000: episode: 140, duration: 0.004s, episode steps: 5, steps per second: 1358, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 2.800 [0.000, 6.000], mean observation: 0.520 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  605/10000: episode: 141, duration: 0.002s, episode steps: 3, steps per second: 1248, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [7.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  612/10000: episode: 142, duration: 0.005s, episode steps: 7, steps per second: 1354, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.571 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  616/10000: episode: 143, duration: 0.003s, episode steps: 4, steps per second: 1171, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  622/10000: episode: 144, duration: 0.004s, episode steps: 6, steps per second: 1500, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 1.833 [0.000, 4.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  628/10000: episode: 145, duration: 0.004s, episode steps: 6, steps per second: 1457, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  630/10000: episode: 146, duration: 0.002s, episode steps: 2, steps per second: 1031, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  636/10000: episode: 147, duration: 0.004s, episode steps: 6, steps per second: 1415, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  640/10000: episode: 148, duration: 0.003s, episode steps: 4, steps per second: 1272, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  643/10000: episode: 149, duration: 0.003s, episode steps: 3, steps per second: 1183, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [7.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  650/10000: episode: 150, duration: 0.005s, episode steps: 7, steps per second: 1395, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.714 [0.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  656/10000: episode: 151, duration: 0.004s, episode steps: 6, steps per second: 1395, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  661/10000: episode: 152, duration: 0.004s, episode steps: 5, steps per second: 1409, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  664/10000: episode: 153, duration: 0.003s, episode steps: 3, steps per second: 1168, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  669/10000: episode: 154, duration: 0.004s, episode steps: 5, steps per second: 1235, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  674/10000: episode: 155, duration: 0.004s, episode steps: 5, steps per second: 1329, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  679/10000: episode: 156, duration: 0.004s, episode steps: 5, steps per second: 1371, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  682/10000: episode: 157, duration: 0.002s, episode steps: 3, steps per second: 1238, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  688/10000: episode: 158, duration: 0.004s, episode steps: 6, steps per second: 1398, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  691/10000: episode: 159, duration: 0.002s, episode steps: 3, steps per second: 1259, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  696/10000: episode: 160, duration: 0.004s, episode steps: 5, steps per second: 1324, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  701/10000: episode: 161, duration: 0.004s, episode steps: 5, steps per second: 1372, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  706/10000: episode: 162, duration: 0.004s, episode steps: 5, steps per second: 1321, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  710/10000: episode: 163, duration: 0.003s, episode steps: 4, steps per second: 1276, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  717/10000: episode: 164, duration: 0.004s, episode steps: 7, steps per second: 1578, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.857 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  725/10000: episode: 165, duration: 0.005s, episode steps: 8, steps per second: 1489, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.875 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  728/10000: episode: 166, duration: 0.003s, episode steps: 3, steps per second: 1048, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  733/10000: episode: 167, duration: 0.005s, episode steps: 5, steps per second: 1039, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  740/10000: episode: 168, duration: 0.006s, episode steps: 7, steps per second: 1272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.857 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  744/10000: episode: 169, duration: 0.003s, episode steps: 4, steps per second: 1190, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  749/10000: episode: 170, duration: 0.004s, episode steps: 5, steps per second: 1336, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  754/10000: episode: 171, duration: 0.006s, episode steps: 5, steps per second: 870, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  758/10000: episode: 172, duration: 0.003s, episode steps: 4, steps per second: 1147, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  762/10000: episode: 173, duration: 0.003s, episode steps: 4, steps per second: 1246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  766/10000: episode: 174, duration: 0.004s, episode steps: 4, steps per second: 1060, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  770/10000: episode: 175, duration: 0.003s, episode steps: 4, steps per second: 1271, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  777/10000: episode: 176, duration: 0.005s, episode steps: 7, steps per second: 1436, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.286 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  781/10000: episode: 177, duration: 0.003s, episode steps: 4, steps per second: 1294, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  786/10000: episode: 178, duration: 0.004s, episode steps: 5, steps per second: 1289, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  790/10000: episode: 179, duration: 0.003s, episode steps: 4, steps per second: 1280, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  794/10000: episode: 180, duration: 0.003s, episode steps: 4, steps per second: 1340, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.500 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  797/10000: episode: 181, duration: 0.002s, episode steps: 3, steps per second: 1262, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [3.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  802/10000: episode: 182, duration: 0.004s, episode steps: 5, steps per second: 1364, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  807/10000: episode: 183, duration: 0.004s, episode steps: 5, steps per second: 1389, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  811/10000: episode: 184, duration: 0.003s, episode steps: 4, steps per second: 1295, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [3.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  816/10000: episode: 185, duration: 0.004s, episode steps: 5, steps per second: 1426, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  819/10000: episode: 186, duration: 0.002s, episode steps: 3, steps per second: 1223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  824/10000: episode: 187, duration: 0.004s, episode steps: 5, steps per second: 1342, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.200 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  829/10000: episode: 188, duration: 0.004s, episode steps: 5, steps per second: 1313, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.200 [0.000, 7.000], mean observation: 0.620 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  834/10000: episode: 189, duration: 0.004s, episode steps: 5, steps per second: 1233, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  839/10000: episode: 190, duration: 0.004s, episode steps: 5, steps per second: 1379, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  844/10000: episode: 191, duration: 0.004s, episode steps: 5, steps per second: 1166, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  849/10000: episode: 192, duration: 0.004s, episode steps: 5, steps per second: 1345, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.400 [2.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  853/10000: episode: 193, duration: 0.003s, episode steps: 4, steps per second: 1314, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  859/10000: episode: 194, duration: 0.004s, episode steps: 6, steps per second: 1452, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  862/10000: episode: 195, duration: 0.003s, episode steps: 3, steps per second: 1124, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  865/10000: episode: 196, duration: 0.002s, episode steps: 3, steps per second: 1245, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  870/10000: episode: 197, duration: 0.004s, episode steps: 5, steps per second: 1385, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.400 [2.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  873/10000: episode: 198, duration: 0.002s, episode steps: 3, steps per second: 1250, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  877/10000: episode: 199, duration: 0.003s, episode steps: 4, steps per second: 1330, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  882/10000: episode: 200, duration: 0.004s, episode steps: 5, steps per second: 1264, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  888/10000: episode: 201, duration: 0.004s, episode steps: 6, steps per second: 1415, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  890/10000: episode: 202, duration: 0.002s, episode steps: 2, steps per second: 1065, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  896/10000: episode: 203, duration: 0.004s, episode steps: 6, steps per second: 1396, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  901/10000: episode: 204, duration: 0.004s, episode steps: 5, steps per second: 1394, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  903/10000: episode: 205, duration: 0.002s, episode steps: 2, steps per second: 1136, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  909/10000: episode: 206, duration: 0.004s, episode steps: 6, steps per second: 1403, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  913/10000: episode: 207, duration: 0.003s, episode steps: 4, steps per second: 1296, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.500 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  917/10000: episode: 208, duration: 0.003s, episode steps: 4, steps per second: 1362, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  921/10000: episode: 209, duration: 0.003s, episode steps: 4, steps per second: 1196, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  924/10000: episode: 210, duration: 0.003s, episode steps: 3, steps per second: 1183, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [3.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  928/10000: episode: 211, duration: 0.003s, episode steps: 4, steps per second: 1289, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  934/10000: episode: 212, duration: 0.004s, episode steps: 6, steps per second: 1435, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  938/10000: episode: 213, duration: 0.003s, episode steps: 4, steps per second: 1238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  944/10000: episode: 214, duration: 0.004s, episode steps: 6, steps per second: 1382, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  948/10000: episode: 215, duration: 0.003s, episode steps: 4, steps per second: 1301, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  952/10000: episode: 216, duration: 0.003s, episode steps: 4, steps per second: 1238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  958/10000: episode: 217, duration: 0.004s, episode steps: 6, steps per second: 1417, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  962/10000: episode: 218, duration: 0.004s, episode steps: 4, steps per second: 1100, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  969/10000: episode: 219, duration: 0.005s, episode steps: 7, steps per second: 1317, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  971/10000: episode: 220, duration: 0.002s, episode steps: 2, steps per second: 998, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  975/10000: episode: 221, duration: 0.003s, episode steps: 4, steps per second: 1229, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.250 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  981/10000: episode: 222, duration: 0.005s, episode steps: 6, steps per second: 1276, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  986/10000: episode: 223, duration: 0.004s, episode steps: 5, steps per second: 1377, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  989/10000: episode: 224, duration: 0.002s, episode steps: 3, steps per second: 1256, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  994/10000: episode: 225, duration: 0.004s, episode steps: 5, steps per second: 1400, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "  997/10000: episode: 226, duration: 0.005s, episode steps: 3, steps per second: 607, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1000/10000: episode: 227, duration: 0.003s, episode steps: 3, steps per second: 1102, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1006/10000: episode: 228, duration: 1.286s, episode steps: 6, steps per second: 5, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 727.718262, mean_absolute_error: 1.786743, mean_q: 0.165723\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1013/10000: episode: 229, duration: 0.027s, episode steps: 7, steps per second: 258, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.143 [1.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 987.604004, mean_absolute_error: 2.329226, mean_q: 0.188269\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1017/10000: episode: 230, duration: 0.020s, episode steps: 4, steps per second: 195, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 901.940430, mean_absolute_error: 2.121941, mean_q: 0.199772\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1025/10000: episode: 231, duration: 0.031s, episode steps: 8, steps per second: 257, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 943.726624, mean_absolute_error: 2.242260, mean_q: 0.205327\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1030/10000: episode: 232, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1197.278564, mean_absolute_error: 2.827892, mean_q: 0.249935\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1035/10000: episode: 233, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1315.951172, mean_absolute_error: 3.038726, mean_q: 0.310401\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1041/10000: episode: 234, duration: 0.024s, episode steps: 6, steps per second: 251, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1046.418091, mean_absolute_error: 2.458248, mean_q: 0.395990\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1047/10000: episode: 235, duration: 0.024s, episode steps: 6, steps per second: 254, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1019.421204, mean_absolute_error: 2.390733, mean_q: 0.464423\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1052/10000: episode: 236, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1097.582031, mean_absolute_error: 2.556938, mean_q: 0.521989\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1057/10000: episode: 237, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.800 [3.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1067.716797, mean_absolute_error: 2.516310, mean_q: 0.539831\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1060/10000: episode: 238, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [0.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1253.333130, mean_absolute_error: 2.916647, mean_q: 0.511376\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1066/10000: episode: 239, duration: 0.026s, episode steps: 6, steps per second: 234, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 992.728271, mean_absolute_error: 2.335653, mean_q: 0.490929\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1073/10000: episode: 240, duration: 0.026s, episode steps: 7, steps per second: 269, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.143 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1140.512939, mean_absolute_error: 2.654985, mean_q: 0.520939\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1079/10000: episode: 241, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1253.844360, mean_absolute_error: 2.917155, mean_q: 0.478370\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1083/10000: episode: 242, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 789.440857, mean_absolute_error: 1.937382, mean_q: 0.464238\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1088/10000: episode: 243, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1033.135254, mean_absolute_error: 2.421786, mean_q: 0.446482\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1093/10000: episode: 244, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1192.138184, mean_absolute_error: 2.799047, mean_q: 0.487157\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1095/10000: episode: 245, duration: 0.010s, episode steps: 2, steps per second: 199, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 1102.720947, mean_absolute_error: 2.620034, mean_q: 0.480531\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1102/10000: episode: 246, duration: 0.027s, episode steps: 7, steps per second: 263, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [1.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1275.701172, mean_absolute_error: 2.974576, mean_q: 0.520429\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1104/10000: episode: 247, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 781.558167, mean_absolute_error: 1.853154, mean_q: 0.504531\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1112/10000: episode: 248, duration: 0.034s, episode steps: 8, steps per second: 236, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 5.250 [2.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1017.557129, mean_absolute_error: 2.396193, mean_q: 0.530712\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1115/10000: episode: 249, duration: 0.016s, episode steps: 3, steps per second: 191, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1462.823364, mean_absolute_error: 3.408637, mean_q: 0.578324\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1119/10000: episode: 250, duration: 0.019s, episode steps: 4, steps per second: 212, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 982.041931, mean_absolute_error: 2.344426, mean_q: 0.572111\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1124/10000: episode: 251, duration: 0.022s, episode steps: 5, steps per second: 231, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 847.740601, mean_absolute_error: 2.040664, mean_q: 0.588587\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1130/10000: episode: 252, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1203.757812, mean_absolute_error: 2.847063, mean_q: 0.609560\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1133/10000: episode: 253, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 731.569031, mean_absolute_error: 1.774937, mean_q: 0.606859\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1138/10000: episode: 254, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 846.379517, mean_absolute_error: 2.027951, mean_q: 0.582067\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1144/10000: episode: 255, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.667 [3.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 940.902344, mean_absolute_error: 2.249181, mean_q: 0.572129\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1149/10000: episode: 256, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1095.527832, mean_absolute_error: 2.578489, mean_q: 0.594286\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1153/10000: episode: 257, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1097.537842, mean_absolute_error: 2.597370, mean_q: 0.626006\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1157/10000: episode: 258, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1250.223511, mean_absolute_error: 2.907970, mean_q: 0.587312\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1161/10000: episode: 259, duration: 0.018s, episode steps: 4, steps per second: 224, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 898.546692, mean_absolute_error: 2.126106, mean_q: 0.606614\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1166/10000: episode: 260, duration: 0.023s, episode steps: 5, steps per second: 219, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1221.942505, mean_absolute_error: 2.869506, mean_q: 0.638510\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1172/10000: episode: 261, duration: 0.024s, episode steps: 6, steps per second: 247, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1042.907837, mean_absolute_error: 2.455925, mean_q: 0.605160\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1177/10000: episode: 262, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1064.194092, mean_absolute_error: 2.506732, mean_q: 0.575032\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1181/10000: episode: 263, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 861.292053, mean_absolute_error: 2.062456, mean_q: 0.641189\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1185/10000: episode: 264, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 863.040894, mean_absolute_error: 2.077538, mean_q: 0.643239\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1192/10000: episode: 265, duration: 0.026s, episode steps: 7, steps per second: 274, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.571 [2.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1027.835083, mean_absolute_error: 2.425762, mean_q: 0.628428\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1195/10000: episode: 266, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1041.786377, mean_absolute_error: 2.451674, mean_q: 0.639936\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1202/10000: episode: 267, duration: 0.028s, episode steps: 7, steps per second: 253, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 852.191223, mean_absolute_error: 2.061523, mean_q: 0.623129\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1209/10000: episode: 268, duration: 0.026s, episode steps: 7, steps per second: 266, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.143 [3.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1117.233521, mean_absolute_error: 2.623727, mean_q: 0.627948\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1211/10000: episode: 269, duration: 0.010s, episode steps: 2, steps per second: 205, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 1175.414795, mean_absolute_error: 2.779780, mean_q: 0.661140\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1214/10000: episode: 270, duration: 0.015s, episode steps: 3, steps per second: 200, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1097.966309, mean_absolute_error: 2.609365, mean_q: 0.625559\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1219/10000: episode: 271, duration: 0.023s, episode steps: 5, steps per second: 220, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 973.990417, mean_absolute_error: 2.338812, mean_q: 0.635939\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1222/10000: episode: 272, duration: 0.015s, episode steps: 3, steps per second: 206, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 939.749329, mean_absolute_error: 2.240839, mean_q: 0.637660\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1229/10000: episode: 273, duration: 0.027s, episode steps: 7, steps per second: 261, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.571 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1008.127808, mean_absolute_error: 2.404235, mean_q: 0.643478\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1235/10000: episode: 274, duration: 0.024s, episode steps: 6, steps per second: 249, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 967.052429, mean_absolute_error: 2.318456, mean_q: 0.656494\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1238/10000: episode: 275, duration: 0.013s, episode steps: 3, steps per second: 235, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [3.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1043.939331, mean_absolute_error: 2.480654, mean_q: 0.700839\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1242/10000: episode: 276, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [3.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 941.027954, mean_absolute_error: 2.263965, mean_q: 0.726945\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1246/10000: episode: 277, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1214.150879, mean_absolute_error: 2.870882, mean_q: 0.771962\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1250/10000: episode: 278, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 898.570801, mean_absolute_error: 2.133931, mean_q: 0.759538\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1256/10000: episode: 279, duration: 0.025s, episode steps: 6, steps per second: 245, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1045.272339, mean_absolute_error: 2.493359, mean_q: 0.713178\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1262/10000: episode: 280, duration: 0.022s, episode steps: 6, steps per second: 269, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 3.667 [0.000, 8.000], mean observation: 0.700 [0.000, 2.000], loss: 1226.156616, mean_absolute_error: 2.872899, mean_q: 0.672477\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1267/10000: episode: 281, duration: 0.022s, episode steps: 5, steps per second: 225, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1315.140869, mean_absolute_error: 3.075064, mean_q: 0.740313\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1273/10000: episode: 282, duration: 0.024s, episode steps: 6, steps per second: 249, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1018.827576, mean_absolute_error: 2.422865, mean_q: 0.734430\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1280/10000: episode: 283, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.714 [3.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1050.588989, mean_absolute_error: 2.474882, mean_q: 0.699548\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1284/10000: episode: 284, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.750 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 981.843750, mean_absolute_error: 2.367852, mean_q: 0.748450\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1287/10000: episode: 285, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1044.304077, mean_absolute_error: 2.478803, mean_q: 0.800625\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1294/10000: episode: 286, duration: 0.026s, episode steps: 7, steps per second: 270, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 985.996094, mean_absolute_error: 2.362855, mean_q: 0.760534\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1299/10000: episode: 287, duration: 0.019s, episode steps: 5, steps per second: 265, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1281.613525, mean_absolute_error: 2.994532, mean_q: 0.745961\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1304/10000: episode: 288, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1125.577881, mean_absolute_error: 2.641349, mean_q: 0.763418\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1308/10000: episode: 289, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 626.859985, mean_absolute_error: 1.548949, mean_q: 0.673082\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1313/10000: episode: 290, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1219.090332, mean_absolute_error: 2.849468, mean_q: 0.685865\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1316/10000: episode: 291, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1254.713623, mean_absolute_error: 2.962250, mean_q: 0.745553\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1320/10000: episode: 292, duration: 0.019s, episode steps: 4, steps per second: 211, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [3.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1450.861084, mean_absolute_error: 3.405776, mean_q: 0.703215\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1325/10000: episode: 293, duration: 0.022s, episode steps: 5, steps per second: 229, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1251.435547, mean_absolute_error: 2.933254, mean_q: 0.738380\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1329/10000: episode: 294, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1095.742065, mean_absolute_error: 2.587796, mean_q: 0.735872\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1333/10000: episode: 295, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [3.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 825.233704, mean_absolute_error: 2.016627, mean_q: 0.726472\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1336/10000: episode: 296, duration: 0.013s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [5.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 577.245667, mean_absolute_error: 1.465673, mean_q: 0.751501\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1341/10000: episode: 297, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1440.185547, mean_absolute_error: 3.357306, mean_q: 0.780820\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1346/10000: episode: 298, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 877.518250, mean_absolute_error: 2.109778, mean_q: 0.809211\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1348/10000: episode: 299, duration: 0.010s, episode steps: 2, steps per second: 199, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 706.375366, mean_absolute_error: 1.734797, mean_q: 0.817604\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1351/10000: episode: 300, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 1145.934204, mean_absolute_error: 2.674810, mean_q: 0.830862\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1356/10000: episode: 301, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1407.647217, mean_absolute_error: 3.280879, mean_q: 0.812730\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1361/10000: episode: 302, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 815.256470, mean_absolute_error: 1.970061, mean_q: 0.817767\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1364/10000: episode: 303, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [4.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 885.563660, mean_absolute_error: 2.113889, mean_q: 0.827603\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1367/10000: episode: 304, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1252.097168, mean_absolute_error: 2.929513, mean_q: 0.820458\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1370/10000: episode: 305, duration: 0.015s, episode steps: 3, steps per second: 196, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [3.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1250.236328, mean_absolute_error: 2.913662, mean_q: 0.783448\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1377/10000: episode: 306, duration: 0.029s, episode steps: 7, steps per second: 239, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.286 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1096.625610, mean_absolute_error: 2.602922, mean_q: 0.780360\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1383/10000: episode: 307, duration: 0.023s, episode steps: 6, steps per second: 260, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 3.833 [1.000, 8.000], mean observation: 0.700 [0.000, 2.000], loss: 1329.393066, mean_absolute_error: 3.104040, mean_q: 0.707476\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1385/10000: episode: 308, duration: 0.010s, episode steps: 2, steps per second: 206, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 1018.592896, mean_absolute_error: 2.430380, mean_q: 0.712804\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1389/10000: episode: 309, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1020.507751, mean_absolute_error: 2.443056, mean_q: 0.702672\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1391/10000: episode: 310, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 1015.757812, mean_absolute_error: 2.399213, mean_q: 0.730583\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1398/10000: episode: 311, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.286 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1229.418335, mean_absolute_error: 2.883095, mean_q: 0.775549\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1402/10000: episode: 312, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1328.687744, mean_absolute_error: 3.098325, mean_q: 0.792978\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1405/10000: episode: 313, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1202.247681, mean_absolute_error: 2.850754, mean_q: 0.744033\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1412/10000: episode: 314, duration: 0.026s, episode steps: 7, steps per second: 270, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.429 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 829.501831, mean_absolute_error: 2.013273, mean_q: 0.729858\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1416/10000: episode: 315, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 821.886597, mean_absolute_error: 1.982574, mean_q: 0.774276\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1422/10000: episode: 316, duration: 0.024s, episode steps: 6, steps per second: 246, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1303.309204, mean_absolute_error: 3.052988, mean_q: 0.778237\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1427/10000: episode: 317, duration: 0.023s, episode steps: 5, steps per second: 220, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 813.953430, mean_absolute_error: 1.962452, mean_q: 0.795465\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1432/10000: episode: 318, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 658.906555, mean_absolute_error: 1.631989, mean_q: 0.734168\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1436/10000: episode: 319, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1057.900513, mean_absolute_error: 2.525709, mean_q: 0.766510\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1439/10000: episode: 320, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [0.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 781.400085, mean_absolute_error: 1.874420, mean_q: 0.739656\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1443/10000: episode: 321, duration: 0.018s, episode steps: 4, steps per second: 217, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1017.351501, mean_absolute_error: 2.421668, mean_q: 0.754147\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1445/10000: episode: 322, duration: 0.011s, episode steps: 2, steps per second: 188, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1181.536499, mean_absolute_error: 2.854478, mean_q: 0.765310\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1449/10000: episode: 323, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 862.655151, mean_absolute_error: 2.094507, mean_q: 0.737544\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1452/10000: episode: 324, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 939.758118, mean_absolute_error: 2.257394, mean_q: 0.781449\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1456/10000: episode: 325, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 979.700562, mean_absolute_error: 2.352171, mean_q: 0.763615\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1459/10000: episode: 326, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1252.276489, mean_absolute_error: 2.944578, mean_q: 0.757456\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1462/10000: episode: 327, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [3.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1148.249878, mean_absolute_error: 2.704753, mean_q: 0.755827\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1469/10000: episode: 328, duration: 0.026s, episode steps: 7, steps per second: 271, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.571 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1387.558838, mean_absolute_error: 3.251520, mean_q: 0.771902\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1473/10000: episode: 329, duration: 0.023s, episode steps: 4, steps per second: 175, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1136.133789, mean_absolute_error: 2.693548, mean_q: 0.764564\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1477/10000: episode: 330, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [3.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 860.276123, mean_absolute_error: 2.058215, mean_q: 0.729339\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1481/10000: episode: 331, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.750 [5.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 937.699341, mean_absolute_error: 2.240204, mean_q: 0.725317\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1486/10000: episode: 332, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.200 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 942.840698, mean_absolute_error: 2.282538, mean_q: 0.723422\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1490/10000: episode: 333, duration: 0.018s, episode steps: 4, steps per second: 224, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.750 [5.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1173.600098, mean_absolute_error: 2.766738, mean_q: 0.758449\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1492/10000: episode: 334, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 937.678345, mean_absolute_error: 2.234450, mean_q: 0.792552\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1499/10000: episode: 335, duration: 0.026s, episode steps: 7, steps per second: 273, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 6.000], mean observation: 0.657 [0.000, 2.000], loss: 960.863403, mean_absolute_error: 2.285411, mean_q: 0.716922\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1505/10000: episode: 336, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 756.434082, mean_absolute_error: 1.840985, mean_q: 0.723983\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1507/10000: episode: 337, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 1.000], loss: 1328.304321, mean_absolute_error: 3.092913, mean_q: 0.781096\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1511/10000: episode: 338, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1372.026489, mean_absolute_error: 3.229141, mean_q: 0.782130\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1515/10000: episode: 339, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.000 [0.000, 2.000], mean observation: 0.400 [0.000, 2.000], loss: 864.253601, mean_absolute_error: 2.117198, mean_q: 0.708476\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1520/10000: episode: 340, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1066.314819, mean_absolute_error: 2.550261, mean_q: 0.728112\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1526/10000: episode: 341, duration: 0.024s, episode steps: 6, steps per second: 249, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1070.992554, mean_absolute_error: 2.548081, mean_q: 0.692102\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1528/10000: episode: 342, duration: 0.013s, episode steps: 2, steps per second: 159, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 1178.028809, mean_absolute_error: 2.824930, mean_q: 0.747077\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1531/10000: episode: 343, duration: 0.014s, episode steps: 3, steps per second: 209, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1304.333862, mean_absolute_error: 3.060239, mean_q: 0.757915\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1533/10000: episode: 344, duration: 0.010s, episode steps: 2, steps per second: 192, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 1175.173340, mean_absolute_error: 2.786657, mean_q: 0.727562\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1537/10000: episode: 345, duration: 0.018s, episode steps: 4, steps per second: 226, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [5.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1176.588257, mean_absolute_error: 2.800055, mean_q: 0.723288\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1540/10000: episode: 346, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [1.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 1406.529785, mean_absolute_error: 3.274442, mean_q: 0.721287\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1548/10000: episode: 347, duration: 0.030s, episode steps: 8, steps per second: 269, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.875 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 940.040283, mean_absolute_error: 2.259241, mean_q: 0.703656\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1554/10000: episode: 348, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 860.651672, mean_absolute_error: 2.074529, mean_q: 0.714110\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1559/10000: episode: 349, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 971.308777, mean_absolute_error: 2.322979, mean_q: 0.742993\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1565/10000: episode: 350, duration: 0.023s, episode steps: 6, steps per second: 255, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 4.667 [2.000, 8.000], mean observation: 0.700 [0.000, 2.000], loss: 885.585938, mean_absolute_error: 2.116344, mean_q: 0.749835\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1571/10000: episode: 351, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 1094.890503, mean_absolute_error: 2.589010, mean_q: 0.773926\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1574/10000: episode: 352, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 729.358398, mean_absolute_error: 1.769948, mean_q: 0.776727\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1578/10000: episode: 353, duration: 0.018s, episode steps: 4, steps per second: 217, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1256.598267, mean_absolute_error: 2.990412, mean_q: 0.741487\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1580/10000: episode: 354, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.300 [0.000, 2.000], loss: 937.695679, mean_absolute_error: 2.230226, mean_q: 0.765059\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1584/10000: episode: 355, duration: 0.020s, episode steps: 4, steps per second: 200, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 861.129517, mean_absolute_error: 2.071286, mean_q: 0.725603\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1589/10000: episode: 356, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1031.457397, mean_absolute_error: 2.444297, mean_q: 0.726254\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1595/10000: episode: 357, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1121.027466, mean_absolute_error: 2.654763, mean_q: 0.715987\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1600/10000: episode: 358, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.200 [2.000, 7.000], mean observation: 0.520 [0.000, 2.000], loss: 1347.022583, mean_absolute_error: 3.167257, mean_q: 0.732359\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1606/10000: episode: 359, duration: 0.022s, episode steps: 6, steps per second: 271, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1046.002441, mean_absolute_error: 2.506449, mean_q: 0.715225\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1608/10000: episode: 360, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 946.752686, mean_absolute_error: 2.327273, mean_q: 0.775438\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1613/10000: episode: 361, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [1.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1223.890625, mean_absolute_error: 2.902782, mean_q: 0.703304\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1615/10000: episode: 362, duration: 0.010s, episode steps: 2, steps per second: 197, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 1175.158813, mean_absolute_error: 2.778811, mean_q: 0.763828\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1619/10000: episode: 363, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1056.393799, mean_absolute_error: 2.501460, mean_q: 0.724735\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1624/10000: episode: 364, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 783.695435, mean_absolute_error: 1.910718, mean_q: 0.765799\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1626/10000: episode: 365, duration: 0.012s, episode steps: 2, steps per second: 173, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 1492.988037, mean_absolute_error: 3.525832, mean_q: 0.846491\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1630/10000: episode: 366, duration: 0.020s, episode steps: 4, steps per second: 200, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1175.281738, mean_absolute_error: 2.763465, mean_q: 0.730038\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1635/10000: episode: 367, duration: 0.021s, episode steps: 5, steps per second: 243, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1095.824951, mean_absolute_error: 2.593804, mean_q: 0.715060\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1639/10000: episode: 368, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1097.197754, mean_absolute_error: 2.618762, mean_q: 0.747234\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1642/10000: episode: 369, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 939.776855, mean_absolute_error: 2.254782, mean_q: 0.751409\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1646/10000: episode: 370, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.250 [5.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1291.520996, mean_absolute_error: 3.031810, mean_q: 0.785807\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1652/10000: episode: 371, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1125.692017, mean_absolute_error: 2.688662, mean_q: 0.818790\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1657/10000: episode: 372, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [3.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 939.029419, mean_absolute_error: 2.234057, mean_q: 0.845083\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1663/10000: episode: 373, duration: 0.022s, episode steps: 6, steps per second: 272, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 990.785889, mean_absolute_error: 2.354825, mean_q: 0.830358\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1667/10000: episode: 374, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1172.892090, mean_absolute_error: 2.748977, mean_q: 0.792671\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1672/10000: episode: 375, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1002.703491, mean_absolute_error: 2.399247, mean_q: 0.796074\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1678/10000: episode: 376, duration: 0.025s, episode steps: 6, steps per second: 242, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 860.613098, mean_absolute_error: 2.067018, mean_q: 0.772957\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1685/10000: episode: 377, duration: 0.030s, episode steps: 7, steps per second: 232, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [1.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 896.542786, mean_absolute_error: 2.174698, mean_q: 0.794972\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1689/10000: episode: 378, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1330.022949, mean_absolute_error: 3.116968, mean_q: 0.751220\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1693/10000: episode: 379, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [3.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1213.461914, mean_absolute_error: 2.857699, mean_q: 0.724370\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1698/10000: episode: 380, duration: 0.019s, episode steps: 5, steps per second: 264, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1031.440063, mean_absolute_error: 2.441634, mean_q: 0.733637\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1703/10000: episode: 381, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1033.897705, mean_absolute_error: 2.469598, mean_q: 0.727875\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1707/10000: episode: 382, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 978.358582, mean_absolute_error: 2.336433, mean_q: 0.753871\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1714/10000: episode: 383, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1229.576294, mean_absolute_error: 2.895438, mean_q: 0.754569\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1717/10000: episode: 384, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [1.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1098.068970, mean_absolute_error: 2.615476, mean_q: 0.759932\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1721/10000: episode: 385, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1253.303833, mean_absolute_error: 2.956425, mean_q: 0.766236\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1728/10000: episode: 386, duration: 0.029s, episode steps: 7, steps per second: 242, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.671 [0.000, 2.000], loss: 1186.798950, mean_absolute_error: 2.817387, mean_q: 0.736974\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1734/10000: episode: 387, duration: 0.028s, episode steps: 6, steps per second: 218, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 890.264465, mean_absolute_error: 2.166493, mean_q: 0.703658\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1736/10000: episode: 388, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 943.894531, mean_absolute_error: 2.306019, mean_q: 0.738748\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1741/10000: episode: 389, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1190.334229, mean_absolute_error: 2.820208, mean_q: 0.753178\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1744/10000: episode: 390, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 941.650330, mean_absolute_error: 2.270467, mean_q: 0.753183\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1747/10000: episode: 391, duration: 0.013s, episode steps: 3, steps per second: 235, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1562.678589, mean_absolute_error: 3.618467, mean_q: 0.786537\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1752/10000: episode: 392, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1195.080078, mean_absolute_error: 2.860064, mean_q: 0.796104\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1758/10000: episode: 393, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1098.090210, mean_absolute_error: 2.623229, mean_q: 0.775682\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1762/10000: episode: 394, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1132.989868, mean_absolute_error: 2.663520, mean_q: 0.751714\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1767/10000: episode: 395, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 941.311218, mean_absolute_error: 2.268276, mean_q: 0.753883\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1774/10000: episode: 396, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.286 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1073.386597, mean_absolute_error: 2.550725, mean_q: 0.826169\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1779/10000: episode: 397, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1320.458374, mean_absolute_error: 3.142778, mean_q: 0.862553\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1784/10000: episode: 398, duration: 0.023s, episode steps: 5, steps per second: 220, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.200 [4.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 969.774414, mean_absolute_error: 2.296622, mean_q: 0.868210\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1790/10000: episode: 399, duration: 0.027s, episode steps: 6, steps per second: 224, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.167 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 939.590332, mean_absolute_error: 2.245799, mean_q: 0.813583\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1795/10000: episode: 400, duration: 0.021s, episode steps: 5, steps per second: 237, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1315.260620, mean_absolute_error: 3.088768, mean_q: 0.848785\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1800/10000: episode: 401, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [1.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1192.199951, mean_absolute_error: 2.825390, mean_q: 0.808191\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1803/10000: episode: 402, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [0.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 729.411621, mean_absolute_error: 1.774758, mean_q: 0.722566\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1806/10000: episode: 403, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [1.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 940.584778, mean_absolute_error: 2.253448, mean_q: 0.703442\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1810/10000: episode: 404, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1097.195923, mean_absolute_error: 2.591805, mean_q: 0.808249\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1815/10000: episode: 405, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.600 [0.000, 2.000], loss: 784.071899, mean_absolute_error: 1.917621, mean_q: 0.841933\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1819/10000: episode: 406, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1214.689819, mean_absolute_error: 2.863003, mean_q: 0.848675\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1822/10000: episode: 407, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 992.895935, mean_absolute_error: 2.367251, mean_q: 0.787889\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1825/10000: episode: 408, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [0.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 783.306824, mean_absolute_error: 1.897377, mean_q: 0.749937\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1833/10000: episode: 409, duration: 0.035s, episode steps: 8, steps per second: 229, episode reward: -20.000, mean reward: -2.500 [-20.000, 0.000], mean action: 4.250 [0.000, 8.000], mean observation: 0.750 [0.000, 2.000], loss: 842.397095, mean_absolute_error: 2.041050, mean_q: 0.820098\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1839/10000: episode: 410, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 756.967041, mean_absolute_error: 1.838932, mean_q: 0.810287\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1843/10000: episode: 411, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 862.771606, mean_absolute_error: 2.098622, mean_q: 0.783102\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1850/10000: episode: 412, duration: 0.026s, episode steps: 7, steps per second: 271, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1029.577148, mean_absolute_error: 2.462915, mean_q: 0.770579\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1852/10000: episode: 413, duration: 0.010s, episode steps: 2, steps per second: 208, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 1015.799744, mean_absolute_error: 2.403891, mean_q: 0.736778\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1856/10000: episode: 414, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 978.309509, mean_absolute_error: 2.335246, mean_q: 0.766703\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1862/10000: episode: 415, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.333 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 994.437012, mean_absolute_error: 2.388502, mean_q: 0.804668\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1865/10000: episode: 416, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1148.073364, mean_absolute_error: 2.709041, mean_q: 0.829167\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1871/10000: episode: 417, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1045.039429, mean_absolute_error: 2.491702, mean_q: 0.830214\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1875/10000: episode: 418, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 783.758484, mean_absolute_error: 1.894673, mean_q: 0.916983\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1880/10000: episode: 419, duration: 0.024s, episode steps: 5, steps per second: 212, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.600 [0.000, 6.000], mean observation: 0.520 [0.000, 2.000], loss: 941.653320, mean_absolute_error: 2.264108, mean_q: 0.915149\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1883/10000: episode: 420, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1303.300781, mean_absolute_error: 3.042794, mean_q: 0.898501\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1887/10000: episode: 421, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.750 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 900.954224, mean_absolute_error: 2.156638, mean_q: 0.906268\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1890/10000: episode: 422, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 893.726379, mean_absolute_error: 2.201758, mean_q: 0.899353\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1897/10000: episode: 423, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1009.556641, mean_absolute_error: 2.430517, mean_q: 0.873620\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1905/10000: episode: 424, duration: 0.030s, episode steps: 8, steps per second: 264, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.375 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 805.535278, mean_absolute_error: 1.973210, mean_q: 0.852519\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1911/10000: episode: 425, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 943.896667, mean_absolute_error: 2.298176, mean_q: 0.847156\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1914/10000: episode: 426, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1043.929565, mean_absolute_error: 2.480801, mean_q: 0.820775\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1917/10000: episode: 427, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 989.788818, mean_absolute_error: 2.353845, mean_q: 0.786210\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1923/10000: episode: 428, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.333 [3.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 914.763367, mean_absolute_error: 2.206696, mean_q: 0.785522\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1925/10000: episode: 429, duration: 0.011s, episode steps: 2, steps per second: 179, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 1020.441895, mean_absolute_error: 2.427505, mean_q: 0.781427\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1931/10000: episode: 430, duration: 0.025s, episode steps: 6, steps per second: 238, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1304.306274, mean_absolute_error: 3.062008, mean_q: 0.816248\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1935/10000: episode: 431, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1409.573730, mean_absolute_error: 3.304061, mean_q: 0.842941\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1941/10000: episode: 432, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 916.868225, mean_absolute_error: 2.237283, mean_q: 0.841068\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1946/10000: episode: 433, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1376.441162, mean_absolute_error: 3.226077, mean_q: 0.851140\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1951/10000: episode: 434, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.200 [1.000, 6.000], mean observation: 0.520 [0.000, 2.000], loss: 1221.899170, mean_absolute_error: 2.873448, mean_q: 0.829645\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1957/10000: episode: 435, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1203.075806, mean_absolute_error: 2.856082, mean_q: 0.838112\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1962/10000: episode: 436, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 913.820496, mean_absolute_error: 2.240400, mean_q: 0.852378\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1966/10000: episode: 437, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 979.824097, mean_absolute_error: 2.343581, mean_q: 0.808722\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1971/10000: episode: 438, duration: 0.021s, episode steps: 5, steps per second: 244, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1131.636475, mean_absolute_error: 2.703211, mean_q: 0.833517\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1978/10000: episode: 439, duration: 0.027s, episode steps: 7, steps per second: 261, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 919.912231, mean_absolute_error: 2.226435, mean_q: 0.814452\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1981/10000: episode: 440, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 940.531555, mean_absolute_error: 2.259967, mean_q: 0.849072\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1984/10000: episode: 441, duration: 0.016s, episode steps: 3, steps per second: 186, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 833.525146, mean_absolute_error: 2.002353, mean_q: 0.781978\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1990/10000: episode: 442, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1017.933655, mean_absolute_error: 2.419298, mean_q: 0.825972\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1994/10000: episode: 443, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1292.977051, mean_absolute_error: 3.039145, mean_q: 0.830603\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 1998/10000: episode: 444, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 786.188354, mean_absolute_error: 1.942555, mean_q: 0.786637\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2004/10000: episode: 445, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 889.893494, mean_absolute_error: 2.165965, mean_q: 0.809393\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2007/10000: episode: 446, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [0.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 783.606506, mean_absolute_error: 1.919432, mean_q: 0.781445\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2010/10000: episode: 447, duration: 0.014s, episode steps: 3, steps per second: 219, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [1.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 993.723877, mean_absolute_error: 2.388419, mean_q: 0.754750\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2017/10000: episode: 448, duration: 0.028s, episode steps: 7, steps per second: 249, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 920.308960, mean_absolute_error: 2.237555, mean_q: 0.752886\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2022/10000: episode: 449, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.620 [0.000, 2.000], loss: 1191.313232, mean_absolute_error: 2.819301, mean_q: 0.822455\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2029/10000: episode: 450, duration: 0.029s, episode steps: 7, steps per second: 238, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.857 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1050.670776, mean_absolute_error: 2.495519, mean_q: 0.784640\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2035/10000: episode: 451, duration: 0.027s, episode steps: 6, steps per second: 224, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [3.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 966.054138, mean_absolute_error: 2.323621, mean_q: 0.809505\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2039/10000: episode: 452, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 976.775391, mean_absolute_error: 2.323771, mean_q: 0.828447\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2043/10000: episode: 453, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 705.829895, mean_absolute_error: 1.729824, mean_q: 0.836015\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2050/10000: episode: 454, duration: 0.027s, episode steps: 7, steps per second: 260, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 986.137329, mean_absolute_error: 2.376329, mean_q: 0.846237\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2053/10000: episode: 455, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1406.478516, mean_absolute_error: 3.273290, mean_q: 0.834355\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2060/10000: episode: 456, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1184.526978, mean_absolute_error: 2.796804, mean_q: 0.806616\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2063/10000: episode: 457, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.333 [0.000, 2.000], loss: 1150.083618, mean_absolute_error: 2.744919, mean_q: 0.833525\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2067/10000: episode: 458, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1056.441162, mean_absolute_error: 2.507233, mean_q: 0.831492\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2069/10000: episode: 459, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 1096.784058, mean_absolute_error: 2.620391, mean_q: 0.837004\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2071/10000: episode: 460, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 1019.274475, mean_absolute_error: 2.439700, mean_q: 0.840266\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2076/10000: episode: 461, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 849.367371, mean_absolute_error: 2.089875, mean_q: 0.815139\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2081/10000: episode: 462, duration: 0.024s, episode steps: 5, steps per second: 209, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1188.343872, mean_absolute_error: 2.790889, mean_q: 0.786601\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2085/10000: episode: 463, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1289.287109, mean_absolute_error: 3.020612, mean_q: 0.796513\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2088/10000: episode: 464, duration: 0.014s, episode steps: 3, steps per second: 217, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1152.456909, mean_absolute_error: 2.753672, mean_q: 0.767733\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2091/10000: episode: 465, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1093.984985, mean_absolute_error: 2.589792, mean_q: 0.783525\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2094/10000: episode: 466, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 681.486145, mean_absolute_error: 1.708779, mean_q: 0.779388\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2098/10000: episode: 467, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1057.094849, mean_absolute_error: 2.505136, mean_q: 0.835217\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2102/10000: episode: 468, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1095.551636, mean_absolute_error: 2.608399, mean_q: 0.804338\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2107/10000: episode: 469, duration: 0.022s, episode steps: 5, steps per second: 230, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 972.227417, mean_absolute_error: 2.334523, mean_q: 0.786815\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2113/10000: episode: 470, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 1304.280762, mean_absolute_error: 3.056229, mean_q: 0.785190\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2116/10000: episode: 471, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 1045.866333, mean_absolute_error: 2.518563, mean_q: 0.801222\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2124/10000: episode: 472, duration: 0.029s, episode steps: 8, steps per second: 271, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.875 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1212.555054, mean_absolute_error: 2.856018, mean_q: 0.803406\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2129/10000: episode: 473, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1001.478210, mean_absolute_error: 2.389664, mean_q: 0.786485\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2134/10000: episode: 474, duration: 0.023s, episode steps: 5, steps per second: 218, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.800 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1032.887939, mean_absolute_error: 2.463436, mean_q: 0.812687\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2138/10000: episode: 475, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1097.875732, mean_absolute_error: 2.619733, mean_q: 0.790033\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2145/10000: episode: 476, duration: 0.028s, episode steps: 7, steps per second: 253, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [1.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 939.885864, mean_absolute_error: 2.261032, mean_q: 0.790876\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2150/10000: episode: 477, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [3.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1003.986145, mean_absolute_error: 2.418654, mean_q: 0.789462\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2156/10000: episode: 478, duration: 0.025s, episode steps: 6, steps per second: 236, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1049.417358, mean_absolute_error: 2.541611, mean_q: 0.777335\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2158/10000: episode: 479, duration: 0.010s, episode steps: 2, steps per second: 199, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 937.743652, mean_absolute_error: 2.244854, mean_q: 0.790625\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2165/10000: episode: 480, duration: 0.027s, episode steps: 7, steps per second: 262, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.571 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 984.156921, mean_absolute_error: 2.355678, mean_q: 0.782558\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2168/10000: episode: 481, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.985718, mean_absolute_error: 2.589527, mean_q: 0.778668\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2172/10000: episode: 482, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1251.019043, mean_absolute_error: 2.937376, mean_q: 0.813319\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2179/10000: episode: 483, duration: 0.029s, episode steps: 7, steps per second: 242, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1030.739868, mean_absolute_error: 2.477720, mean_q: 0.754431\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2182/10000: episode: 484, duration: 0.016s, episode steps: 3, steps per second: 186, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 736.488770, mean_absolute_error: 1.846731, mean_q: 0.764172\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2185/10000: episode: 485, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1253.471558, mean_absolute_error: 2.954090, mean_q: 0.815432\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2189/10000: episode: 486, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [4.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 978.358643, mean_absolute_error: 2.332207, mean_q: 0.803745\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2194/10000: episode: 487, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 5.600 [3.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1069.536255, mean_absolute_error: 2.576960, mean_q: 0.771718\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2199/10000: episode: 488, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1127.772095, mean_absolute_error: 2.681180, mean_q: 0.776459\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2206/10000: episode: 489, duration: 0.026s, episode steps: 7, steps per second: 266, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.757 [0.000, 2.000], loss: 1073.989502, mean_absolute_error: 2.558939, mean_q: 0.755985\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2209/10000: episode: 490, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [5.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 842.928223, mean_absolute_error: 2.098196, mean_q: 0.750399\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2214/10000: episode: 491, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 1.600 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 845.176392, mean_absolute_error: 2.047293, mean_q: 0.756655\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2217/10000: episode: 492, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1095.822754, mean_absolute_error: 2.594509, mean_q: 0.739786\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2221/10000: episode: 493, duration: 0.021s, episode steps: 4, steps per second: 188, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 704.918152, mean_absolute_error: 1.738144, mean_q: 0.733508\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2228/10000: episode: 494, duration: 0.028s, episode steps: 7, steps per second: 246, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 895.309082, mean_absolute_error: 2.158283, mean_q: 0.787484\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2231/10000: episode: 495, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1043.976929, mean_absolute_error: 2.491658, mean_q: 0.833925\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2236/10000: episode: 496, duration: 0.021s, episode steps: 5, steps per second: 235, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1157.771606, mean_absolute_error: 2.738924, mean_q: 0.820751\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2241/10000: episode: 497, duration: 0.025s, episode steps: 5, steps per second: 204, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 626.497131, mean_absolute_error: 1.561532, mean_q: 0.749407\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2244/10000: episode: 498, duration: 0.031s, episode steps: 3, steps per second: 98, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1044.175659, mean_absolute_error: 2.489556, mean_q: 0.752323\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2249/10000: episode: 499, duration: 0.040s, episode steps: 5, steps per second: 125, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 849.575317, mean_absolute_error: 2.074419, mean_q: 0.762212\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2255/10000: episode: 500, duration: 0.032s, episode steps: 6, steps per second: 186, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1068.979370, mean_absolute_error: 2.537479, mean_q: 0.764816\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2258/10000: episode: 501, duration: 0.015s, episode steps: 3, steps per second: 204, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1095.846191, mean_absolute_error: 2.607505, mean_q: 0.778243\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2267/10000: episode: 502, duration: 0.038s, episode steps: 9, steps per second: 237, episode reward: -100.000, mean reward: -11.111 [-100.000, 0.000], mean action: 3.222 [0.000, 8.000], mean observation: 0.811 [0.000, 2.000], loss: 1113.611816, mean_absolute_error: 2.640855, mean_q: 0.770698\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2272/10000: episode: 503, duration: 0.020s, episode steps: 5, steps per second: 244, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1099.067139, mean_absolute_error: 2.628549, mean_q: 0.793777\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2277/10000: episode: 504, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 849.116089, mean_absolute_error: 2.086440, mean_q: 0.793279\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2282/10000: episode: 505, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 847.731445, mean_absolute_error: 2.070731, mean_q: 0.770817\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2285/10000: episode: 506, duration: 0.014s, episode steps: 3, steps per second: 218, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [3.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 731.691223, mean_absolute_error: 1.798085, mean_q: 0.776672\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2292/10000: episode: 507, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1005.600586, mean_absolute_error: 2.400883, mean_q: 0.800096\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2294/10000: episode: 508, duration: 0.010s, episode steps: 2, steps per second: 199, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 1250.209961, mean_absolute_error: 2.926647, mean_q: 0.771386\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2298/10000: episode: 509, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 864.450317, mean_absolute_error: 2.118712, mean_q: 0.795340\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2302/10000: episode: 510, duration: 0.019s, episode steps: 4, steps per second: 216, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 903.346619, mean_absolute_error: 2.201133, mean_q: 0.805619\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2308/10000: episode: 511, duration: 0.027s, episode steps: 6, steps per second: 225, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1309.459351, mean_absolute_error: 3.115862, mean_q: 0.819551\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2312/10000: episode: 512, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.500 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 789.919006, mean_absolute_error: 1.971854, mean_q: 0.792724\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2318/10000: episode: 513, duration: 0.024s, episode steps: 6, steps per second: 249, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 992.800720, mean_absolute_error: 2.376604, mean_q: 0.776781\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2325/10000: episode: 514, duration: 0.029s, episode steps: 7, steps per second: 244, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [1.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 805.584290, mean_absolute_error: 1.962032, mean_q: 0.786238\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2327/10000: episode: 515, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 1253.341919, mean_absolute_error: 2.969982, mean_q: 0.796685\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2334/10000: episode: 516, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.429 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1032.347290, mean_absolute_error: 2.477388, mean_q: 0.790861\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2337/10000: episode: 517, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [1.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 841.675964, mean_absolute_error: 2.098888, mean_q: 0.748136\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2340/10000: episode: 518, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 942.905090, mean_absolute_error: 2.284099, mean_q: 0.794116\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2342/10000: episode: 519, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 468.982239, mean_absolute_error: 1.188158, mean_q: 0.763475\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2345/10000: episode: 520, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [1.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 939.773376, mean_absolute_error: 2.254278, mean_q: 0.791356\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2352/10000: episode: 521, duration: 0.027s, episode steps: 7, steps per second: 262, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.286 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 985.397583, mean_absolute_error: 2.361347, mean_q: 0.770595\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2355/10000: episode: 522, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 887.517395, mean_absolute_error: 2.139449, mean_q: 0.785687\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2359/10000: episode: 523, duration: 0.035s, episode steps: 4, steps per second: 114, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [3.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 978.995422, mean_absolute_error: 2.339249, mean_q: 0.788285\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2363/10000: episode: 524, duration: 0.036s, episode steps: 4, steps per second: 112, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 976.758667, mean_absolute_error: 2.315609, mean_q: 0.766007\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2365/10000: episode: 525, duration: 0.020s, episode steps: 2, steps per second: 101, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 1253.902832, mean_absolute_error: 2.958948, mean_q: 0.800814\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2367/10000: episode: 526, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 937.728149, mean_absolute_error: 2.242007, mean_q: 0.790409\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2373/10000: episode: 527, duration: 0.038s, episode steps: 6, steps per second: 157, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 968.851501, mean_absolute_error: 2.344234, mean_q: 0.799374\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2377/10000: episode: 528, duration: 0.028s, episode steps: 4, steps per second: 141, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1178.326294, mean_absolute_error: 2.816866, mean_q: 0.835357\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2382/10000: episode: 529, duration: 0.040s, episode steps: 5, steps per second: 125, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 977.352905, mean_absolute_error: 2.389046, mean_q: 0.843946\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2389/10000: episode: 530, duration: 0.050s, episode steps: 7, steps per second: 139, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 1206.426025, mean_absolute_error: 2.831904, mean_q: 0.898220\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2392/10000: episode: 531, duration: 0.033s, episode steps: 3, steps per second: 92, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [1.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1408.693237, mean_absolute_error: 3.289602, mean_q: 0.896944\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2394/10000: episode: 532, duration: 0.022s, episode steps: 2, steps per second: 90, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 868.823547, mean_absolute_error: 2.169091, mean_q: 0.910162\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2397/10000: episode: 533, duration: 0.019s, episode steps: 3, steps per second: 156, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 834.573181, mean_absolute_error: 2.000382, mean_q: 0.945376\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2402/10000: episode: 534, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1063.856812, mean_absolute_error: 2.528763, mean_q: 0.930714\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2406/10000: episode: 535, duration: 0.028s, episode steps: 4, steps per second: 142, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1059.712280, mean_absolute_error: 2.530968, mean_q: 0.922854\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2409/10000: episode: 536, duration: 0.026s, episode steps: 3, steps per second: 116, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1146.045288, mean_absolute_error: 2.701788, mean_q: 0.878346\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2414/10000: episode: 537, duration: 0.031s, episode steps: 5, steps per second: 160, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.200 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 972.122192, mean_absolute_error: 2.339251, mean_q: 0.845217\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2420/10000: episode: 538, duration: 0.037s, episode steps: 6, steps per second: 163, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1020.663147, mean_absolute_error: 2.458835, mean_q: 0.781041\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2424/10000: episode: 539, duration: 0.035s, episode steps: 4, steps per second: 114, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1136.990356, mean_absolute_error: 2.701176, mean_q: 0.787626\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2429/10000: episode: 540, duration: 0.033s, episode steps: 5, steps per second: 153, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 812.697021, mean_absolute_error: 1.950797, mean_q: 0.764928\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2435/10000: episode: 541, duration: 0.030s, episode steps: 6, steps per second: 200, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.500 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1095.143555, mean_absolute_error: 2.591862, mean_q: 0.763829\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2439/10000: episode: 542, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 7.000 [5.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1058.025757, mean_absolute_error: 2.532040, mean_q: 0.761399\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2444/10000: episode: 543, duration: 0.021s, episode steps: 5, steps per second: 236, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1066.968750, mean_absolute_error: 2.549973, mean_q: 0.803422\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2448/10000: episode: 544, duration: 0.015s, episode steps: 4, steps per second: 261, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [3.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1059.907959, mean_absolute_error: 2.535799, mean_q: 0.793621\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2453/10000: episode: 545, duration: 0.023s, episode steps: 5, steps per second: 221, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1066.330322, mean_absolute_error: 2.547795, mean_q: 0.780744\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2457/10000: episode: 546, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1212.711914, mean_absolute_error: 2.862320, mean_q: 0.780254\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2463/10000: episode: 547, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 863.144592, mean_absolute_error: 2.095926, mean_q: 0.780838\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2468/10000: episode: 548, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 813.358704, mean_absolute_error: 1.960353, mean_q: 0.783758\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2473/10000: episode: 549, duration: 0.023s, episode steps: 5, steps per second: 216, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1063.949219, mean_absolute_error: 2.523085, mean_q: 0.786163\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2476/10000: episode: 550, duration: 0.018s, episode steps: 3, steps per second: 165, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 837.743958, mean_absolute_error: 2.059498, mean_q: 0.805158\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2478/10000: episode: 551, duration: 0.011s, episode steps: 2, steps per second: 181, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1251.770264, mean_absolute_error: 2.926587, mean_q: 0.801057\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2485/10000: episode: 552, duration: 0.027s, episode steps: 7, steps per second: 260, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1184.334595, mean_absolute_error: 2.793863, mean_q: 0.773721\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2490/10000: episode: 553, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 969.644348, mean_absolute_error: 2.314709, mean_q: 0.772115\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2493/10000: episode: 554, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [1.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1356.448242, mean_absolute_error: 3.185625, mean_q: 0.782193\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2496/10000: episode: 555, duration: 0.014s, episode steps: 3, steps per second: 215, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [3.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 989.763000, mean_absolute_error: 2.343303, mean_q: 0.783451\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2501/10000: episode: 556, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1000.211243, mean_absolute_error: 2.367231, mean_q: 0.782097\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2504/10000: episode: 557, duration: 0.014s, episode steps: 3, steps per second: 209, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 947.059570, mean_absolute_error: 2.310903, mean_q: 0.799360\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2509/10000: episode: 558, duration: 0.022s, episode steps: 5, steps per second: 224, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [2.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 843.961304, mean_absolute_error: 2.025131, mean_q: 0.806974\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2515/10000: episode: 559, duration: 0.027s, episode steps: 6, steps per second: 223, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 861.033020, mean_absolute_error: 2.061658, mean_q: 0.795070\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2518/10000: episode: 560, duration: 0.020s, episode steps: 3, steps per second: 154, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 991.903503, mean_absolute_error: 2.375389, mean_q: 0.802486\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2523/10000: episode: 561, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1189.584229, mean_absolute_error: 2.806149, mean_q: 0.805921\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2531/10000: episode: 562, duration: 0.032s, episode steps: 8, steps per second: 251, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.500 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 820.828857, mean_absolute_error: 1.979528, mean_q: 0.835253\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2536/10000: episode: 563, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 909.308716, mean_absolute_error: 2.185984, mean_q: 0.896922\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2539/10000: episode: 564, duration: 0.014s, episode steps: 3, steps per second: 212, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1302.290894, mean_absolute_error: 3.046805, mean_q: 0.880367\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2542/10000: episode: 565, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [1.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1152.099121, mean_absolute_error: 2.761323, mean_q: 0.853380\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2546/10000: episode: 566, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1447.084351, mean_absolute_error: 3.377532, mean_q: 0.873002\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2551/10000: episode: 567, duration: 0.021s, episode steps: 5, steps per second: 240, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1255.704834, mean_absolute_error: 2.986371, mean_q: 0.844884\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2555/10000: episode: 568, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1057.868408, mean_absolute_error: 2.527910, mean_q: 0.807222\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2560/10000: episode: 569, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.200 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1190.098389, mean_absolute_error: 2.816231, mean_q: 0.799902\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2563/10000: episode: 570, duration: 0.017s, episode steps: 3, steps per second: 180, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 990.866394, mean_absolute_error: 2.354315, mean_q: 0.805641\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2569/10000: episode: 571, duration: 0.025s, episode steps: 6, steps per second: 242, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 785.351868, mean_absolute_error: 1.928202, mean_q: 0.788352\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2572/10000: episode: 572, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [4.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1671.001953, mean_absolute_error: 3.890790, mean_q: 0.819880\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2574/10000: episode: 573, duration: 0.010s, episode steps: 2, steps per second: 193, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 1019.006470, mean_absolute_error: 2.459970, mean_q: 0.804847\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2579/10000: episode: 574, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [4.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1065.230103, mean_absolute_error: 2.547236, mean_q: 0.786025\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2585/10000: episode: 575, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1023.244812, mean_absolute_error: 2.474348, mean_q: 0.797537\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2589/10000: episode: 576, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1172.079346, mean_absolute_error: 2.751800, mean_q: 0.830385\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2595/10000: episode: 577, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1203.652710, mean_absolute_error: 2.865712, mean_q: 0.815568\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2600/10000: episode: 578, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1037.072998, mean_absolute_error: 2.493654, mean_q: 0.864922\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2605/10000: episode: 579, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 973.325562, mean_absolute_error: 2.346493, mean_q: 0.852779\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2612/10000: episode: 580, duration: 0.029s, episode steps: 7, steps per second: 244, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1119.225708, mean_absolute_error: 2.651137, mean_q: 0.867572\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2620/10000: episode: 581, duration: 0.031s, episode steps: 8, steps per second: 254, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.250 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1235.215454, mean_absolute_error: 2.912895, mean_q: 0.919409\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2623/10000: episode: 582, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [3.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1152.072388, mean_absolute_error: 2.757916, mean_q: 0.842175\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2625/10000: episode: 583, duration: 0.010s, episode steps: 2, steps per second: 197, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1253.357178, mean_absolute_error: 2.973567, mean_q: 0.824769\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2629/10000: episode: 584, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1333.018066, mean_absolute_error: 3.136535, mean_q: 0.870408\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2631/10000: episode: 585, duration: 0.010s, episode steps: 2, steps per second: 191, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.300 [0.000, 2.000], loss: 1250.203857, mean_absolute_error: 2.932392, mean_q: 0.894382\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2636/10000: episode: 586, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.600 [0.000, 2.000], loss: 970.073425, mean_absolute_error: 2.310252, mean_q: 0.863279\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2641/10000: episode: 587, duration: 0.019s, episode steps: 5, steps per second: 266, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1065.612305, mean_absolute_error: 2.533799, mean_q: 0.886813\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2643/10000: episode: 588, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.300 [0.000, 2.000], loss: 866.461609, mean_absolute_error: 2.125221, mean_q: 0.907737\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2648/10000: episode: 589, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 816.462891, mean_absolute_error: 1.998410, mean_q: 0.865832\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2651/10000: episode: 590, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [1.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 1200.234497, mean_absolute_error: 2.828840, mean_q: 0.823634\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2657/10000: episode: 591, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 964.807434, mean_absolute_error: 2.289809, mean_q: 0.793345\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2661/10000: episode: 592, duration: 0.018s, episode steps: 4, steps per second: 226, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 820.528259, mean_absolute_error: 1.978748, mean_q: 0.769983\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2667/10000: episode: 593, duration: 0.027s, episode steps: 6, steps per second: 224, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 1173.353394, mean_absolute_error: 2.753147, mean_q: 0.785933\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2673/10000: episode: 594, duration: 0.024s, episode steps: 6, steps per second: 254, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 938.703674, mean_absolute_error: 2.230292, mean_q: 0.867045\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2675/10000: episode: 595, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 1331.963501, mean_absolute_error: 3.121611, mean_q: 0.911856\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2679/10000: episode: 596, duration: 0.022s, episode steps: 4, steps per second: 185, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 979.718079, mean_absolute_error: 2.351863, mean_q: 0.911355\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2684/10000: episode: 597, duration: 0.030s, episode steps: 5, steps per second: 165, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1128.321167, mean_absolute_error: 2.679302, mean_q: 0.864071\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2688/10000: episode: 598, duration: 0.026s, episode steps: 4, steps per second: 157, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1134.806885, mean_absolute_error: 2.687917, mean_q: 0.856079\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2693/10000: episode: 599, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [3.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 942.031067, mean_absolute_error: 2.275741, mean_q: 0.898967\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2698/10000: episode: 600, duration: 0.033s, episode steps: 5, steps per second: 150, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1226.219482, mean_absolute_error: 2.939016, mean_q: 0.874351\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2701/10000: episode: 601, duration: 0.015s, episode steps: 3, steps per second: 203, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1200.396973, mean_absolute_error: 2.816854, mean_q: 0.863821\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2706/10000: episode: 602, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 969.415710, mean_absolute_error: 2.310202, mean_q: 0.810490\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2710/10000: episode: 603, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.250 [3.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1176.055176, mean_absolute_error: 2.795169, mean_q: 0.859455\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2713/10000: episode: 604, duration: 0.017s, episode steps: 3, steps per second: 173, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1204.203491, mean_absolute_error: 2.849064, mean_q: 0.875446\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2717/10000: episode: 605, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 824.289246, mean_absolute_error: 2.009303, mean_q: 0.862783\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2721/10000: episode: 606, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1212.714355, mean_absolute_error: 2.858253, mean_q: 0.871689\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2723/10000: episode: 607, duration: 0.010s, episode steps: 2, steps per second: 206, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 1250.211670, mean_absolute_error: 2.925953, mean_q: 0.859198\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2729/10000: episode: 608, duration: 0.023s, episode steps: 6, steps per second: 258, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 1.667 [0.000, 4.000], mean observation: 0.567 [0.000, 2.000], loss: 1020.649231, mean_absolute_error: 2.457087, mean_q: 0.858551\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2736/10000: episode: 609, duration: 0.027s, episode steps: 7, steps per second: 257, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [1.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1345.171875, mean_absolute_error: 3.176260, mean_q: 0.858279\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2739/10000: episode: 610, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [4.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 938.775574, mean_absolute_error: 2.242719, mean_q: 0.847425\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2743/10000: episode: 611, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 744.008545, mean_absolute_error: 1.827871, mean_q: 0.815521\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2747/10000: episode: 612, duration: 0.024s, episode steps: 4, steps per second: 164, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1016.671021, mean_absolute_error: 2.420794, mean_q: 0.814422\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2751/10000: episode: 613, duration: 0.029s, episode steps: 4, steps per second: 136, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 865.899231, mean_absolute_error: 2.149213, mean_q: 0.866737\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2753/10000: episode: 614, duration: 0.017s, episode steps: 2, steps per second: 120, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 1093.965332, mean_absolute_error: 2.585742, mean_q: 0.790267\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2756/10000: episode: 615, duration: 0.021s, episode steps: 3, steps per second: 145, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [4.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1355.420288, mean_absolute_error: 3.156026, mean_q: 0.839403\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2760/10000: episode: 616, duration: 0.024s, episode steps: 4, steps per second: 168, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1294.049683, mean_absolute_error: 3.059694, mean_q: 0.877392\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2764/10000: episode: 617, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1016.660950, mean_absolute_error: 2.423712, mean_q: 0.870127\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2770/10000: episode: 618, duration: 0.031s, episode steps: 6, steps per second: 196, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 967.558289, mean_absolute_error: 2.328390, mean_q: 0.886423\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2772/10000: episode: 619, duration: 0.013s, episode steps: 2, steps per second: 157, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 1414.605835, mean_absolute_error: 3.345098, mean_q: 0.907248\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2777/10000: episode: 620, duration: 0.022s, episode steps: 5, steps per second: 231, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 941.963257, mean_absolute_error: 2.282950, mean_q: 0.877694\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2780/10000: episode: 621, duration: 0.017s, episode steps: 3, steps per second: 177, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.433 [0.000, 2.000], loss: 1250.243652, mean_absolute_error: 2.939135, mean_q: 0.852294\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2782/10000: episode: 622, duration: 0.011s, episode steps: 2, steps per second: 188, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1484.606812, mean_absolute_error: 3.458517, mean_q: 0.893777\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2786/10000: episode: 623, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1175.239136, mean_absolute_error: 2.798135, mean_q: 0.869119\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2792/10000: episode: 624, duration: 0.024s, episode steps: 6, steps per second: 251, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [3.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1096.213745, mean_absolute_error: 2.604729, mean_q: 0.867836\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2798/10000: episode: 625, duration: 0.024s, episode steps: 6, steps per second: 246, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1149.247437, mean_absolute_error: 2.738642, mean_q: 0.851276\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2801/10000: episode: 626, duration: 0.015s, episode steps: 3, steps per second: 202, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 989.863098, mean_absolute_error: 2.359276, mean_q: 0.912085\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2805/10000: episode: 627, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [1.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 782.948059, mean_absolute_error: 1.920312, mean_q: 0.871541\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2809/10000: episode: 628, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1021.367432, mean_absolute_error: 2.469195, mean_q: 0.880614\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2814/10000: episode: 629, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 943.925598, mean_absolute_error: 2.299161, mean_q: 0.837124\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2820/10000: episode: 630, duration: 0.026s, episode steps: 6, steps per second: 231, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 4.667 [1.000, 8.000], mean observation: 0.700 [0.000, 2.000], loss: 1151.854370, mean_absolute_error: 2.772329, mean_q: 0.861611\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2825/10000: episode: 631, duration: 0.023s, episode steps: 5, steps per second: 214, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1127.623291, mean_absolute_error: 2.683102, mean_q: 0.882253\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2830/10000: episode: 632, duration: 0.026s, episode steps: 5, steps per second: 190, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1064.036987, mean_absolute_error: 2.536275, mean_q: 0.935562\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2834/10000: episode: 633, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 861.993713, mean_absolute_error: 2.092662, mean_q: 0.907326\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2840/10000: episode: 634, duration: 0.028s, episode steps: 6, steps per second: 218, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1122.073120, mean_absolute_error: 2.659105, mean_q: 0.912318\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2847/10000: episode: 635, duration: 0.028s, episode steps: 7, steps per second: 247, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1049.841187, mean_absolute_error: 2.489972, mean_q: 0.919533\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2850/10000: episode: 636, duration: 0.014s, episode steps: 3, steps per second: 214, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.333 [0.000, 2.000], loss: 1097.966187, mean_absolute_error: 2.622292, mean_q: 0.908672\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2853/10000: episode: 637, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1307.561890, mean_absolute_error: 3.095277, mean_q: 0.918364\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2858/10000: episode: 638, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 879.034851, mean_absolute_error: 2.148300, mean_q: 0.906666\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2864/10000: episode: 639, duration: 0.026s, episode steps: 6, steps per second: 235, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1433.637085, mean_absolute_error: 3.352055, mean_q: 0.901252\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2868/10000: episode: 640, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [1.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 981.554993, mean_absolute_error: 2.395263, mean_q: 0.875781\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2870/10000: episode: 641, duration: 0.011s, episode steps: 2, steps per second: 186, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.300 [0.000, 2.000], loss: 626.882935, mean_absolute_error: 1.559676, mean_q: 0.809378\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2875/10000: episode: 642, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [3.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1062.781494, mean_absolute_error: 2.524917, mean_q: 0.833487\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2877/10000: episode: 643, duration: 0.011s, episode steps: 2, steps per second: 186, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 937.748413, mean_absolute_error: 2.229975, mean_q: 0.862706\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2884/10000: episode: 644, duration: 0.027s, episode steps: 7, steps per second: 255, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.429 [3.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1051.997559, mean_absolute_error: 2.514790, mean_q: 0.862043\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2891/10000: episode: 645, duration: 0.027s, episode steps: 7, steps per second: 255, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.143 [3.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 917.290833, mean_absolute_error: 2.207467, mean_q: 0.886237\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2895/10000: episode: 646, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.500 [0.000, 2.000], loss: 941.575317, mean_absolute_error: 2.263856, mean_q: 0.906213\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2900/10000: episode: 647, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1062.768799, mean_absolute_error: 2.509189, mean_q: 0.899775\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2902/10000: episode: 648, duration: 0.011s, episode steps: 2, steps per second: 189, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 1175.210449, mean_absolute_error: 2.748085, mean_q: 0.853394\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2906/10000: episode: 649, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [3.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 937.737793, mean_absolute_error: 2.239793, mean_q: 0.898312\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2910/10000: episode: 650, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [3.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 901.804199, mean_absolute_error: 2.185638, mean_q: 0.864011\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2915/10000: episode: 651, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1067.691040, mean_absolute_error: 2.560442, mean_q: 0.846354\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2920/10000: episode: 652, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.800 [2.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1191.745483, mean_absolute_error: 2.838022, mean_q: 0.873956\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2924/10000: episode: 653, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 900.323547, mean_absolute_error: 2.174178, mean_q: 0.874602\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2927/10000: episode: 654, duration: 0.015s, episode steps: 3, steps per second: 197, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 991.898743, mean_absolute_error: 2.374694, mean_q: 0.859487\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2933/10000: episode: 655, duration: 0.028s, episode steps: 6, steps per second: 214, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [1.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 733.073975, mean_absolute_error: 1.814794, mean_q: 0.851801\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2937/10000: episode: 656, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [3.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1173.486328, mean_absolute_error: 2.756883, mean_q: 0.822070\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2942/10000: episode: 657, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1031.502319, mean_absolute_error: 2.454787, mean_q: 0.805735\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2947/10000: episode: 658, duration: 0.021s, episode steps: 5, steps per second: 243, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1255.257812, mean_absolute_error: 2.980180, mean_q: 0.823062\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2953/10000: episode: 659, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.167 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1044.002319, mean_absolute_error: 2.499498, mean_q: 0.779945\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2957/10000: episode: 660, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1017.413025, mean_absolute_error: 2.434290, mean_q: 0.739051\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2960/10000: episode: 661, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 989.815430, mean_absolute_error: 2.354745, mean_q: 0.766034\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2965/10000: episode: 662, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 845.407410, mean_absolute_error: 2.058347, mean_q: 0.752312\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2967/10000: episode: 663, duration: 0.011s, episode steps: 2, steps per second: 186, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.300 [0.000, 2.000], loss: 945.266418, mean_absolute_error: 2.306565, mean_q: 0.824409\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2970/10000: episode: 664, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1202.494263, mean_absolute_error: 2.875843, mean_q: 0.815798\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2974/10000: episode: 665, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [1.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 1136.335815, mean_absolute_error: 2.712115, mean_q: 0.785101\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2979/10000: episode: 666, duration: 0.023s, episode steps: 5, steps per second: 214, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1096.003662, mean_absolute_error: 2.606886, mean_q: 0.768862\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2982/10000: episode: 667, duration: 0.014s, episode steps: 3, steps per second: 215, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [0.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1302.292236, mean_absolute_error: 3.045772, mean_q: 0.754870\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2988/10000: episode: 668, duration: 0.027s, episode steps: 6, steps per second: 222, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 939.916504, mean_absolute_error: 2.260909, mean_q: 0.759887\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2992/10000: episode: 669, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1253.348145, mean_absolute_error: 2.966769, mean_q: 0.763156\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2995/10000: episode: 670, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1465.048218, mean_absolute_error: 3.453569, mean_q: 0.766543\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 2998/10000: episode: 671, duration: 0.017s, episode steps: 3, steps per second: 182, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1354.386719, mean_absolute_error: 3.162638, mean_q: 0.773862\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3006/10000: episode: 672, duration: 0.032s, episode steps: 8, steps per second: 247, episode reward: 20.000, mean reward: 2.500 [0.000, 20.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.850 [0.000, 2.000], loss: 1057.326660, mean_absolute_error: 2.523577, mean_q: 0.776987\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3011/10000: episode: 673, duration: 0.023s, episode steps: 5, steps per second: 214, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.600 [0.000, 7.000], mean observation: 0.620 [0.000, 2.000], loss: 1002.906860, mean_absolute_error: 2.412362, mean_q: 0.811268\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3015/10000: episode: 674, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.500 [0.000, 2.000], loss: 1056.469971, mean_absolute_error: 2.513686, mean_q: 0.781662\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3021/10000: episode: 675, duration: 0.046s, episode steps: 6, steps per second: 132, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.333 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1068.975098, mean_absolute_error: 2.539830, mean_q: 0.773013\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3026/10000: episode: 676, duration: 0.021s, episode steps: 5, steps per second: 234, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [3.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 880.248657, mean_absolute_error: 2.146128, mean_q: 0.758236\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3030/10000: episode: 677, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1212.728271, mean_absolute_error: 2.866468, mean_q: 0.771171\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3035/10000: episode: 678, duration: 0.023s, episode steps: 5, steps per second: 220, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1004.379272, mean_absolute_error: 2.421865, mean_q: 0.767694\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3037/10000: episode: 679, duration: 0.011s, episode steps: 2, steps per second: 190, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 784.608398, mean_absolute_error: 1.925966, mean_q: 0.767486\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3042/10000: episode: 680, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 847.203430, mean_absolute_error: 2.059584, mean_q: 0.744642\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3047/10000: episode: 681, duration: 0.023s, episode steps: 5, steps per second: 216, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1312.702393, mean_absolute_error: 3.068381, mean_q: 0.749833\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3051/10000: episode: 682, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1173.686279, mean_absolute_error: 2.781925, mean_q: 0.742077\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3053/10000: episode: 683, duration: 0.010s, episode steps: 2, steps per second: 197, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 1021.826782, mean_absolute_error: 2.489431, mean_q: 0.761917\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3057/10000: episode: 684, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1564.139404, mean_absolute_error: 3.635638, mean_q: 0.800586\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3060/10000: episode: 685, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [3.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 785.480042, mean_absolute_error: 1.940432, mean_q: 0.747916\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3065/10000: episode: 686, duration: 0.021s, episode steps: 5, steps per second: 240, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1160.730713, mean_absolute_error: 2.768136, mean_q: 0.755151\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3068/10000: episode: 687, duration: 0.015s, episode steps: 3, steps per second: 207, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [3.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 785.451172, mean_absolute_error: 1.936513, mean_q: 0.778518\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3072/10000: episode: 688, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1253.229004, mean_absolute_error: 2.956712, mean_q: 0.808701\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3077/10000: episode: 689, duration: 0.021s, episode steps: 5, steps per second: 236, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 5.400 [2.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1064.004883, mean_absolute_error: 2.520539, mean_q: 0.789957\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3083/10000: episode: 690, duration: 0.025s, episode steps: 6, steps per second: 243, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1070.017090, mean_absolute_error: 2.549165, mean_q: 0.759250\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3086/10000: episode: 691, duration: 0.015s, episode steps: 3, steps per second: 205, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1041.883667, mean_absolute_error: 2.463005, mean_q: 0.762250\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3091/10000: episode: 692, duration: 0.021s, episode steps: 5, steps per second: 234, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1158.988037, mean_absolute_error: 2.755950, mean_q: 0.792985\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3097/10000: episode: 693, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 991.913818, mean_absolute_error: 2.381026, mean_q: 0.748750\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3101/10000: episode: 694, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [3.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1214.575562, mean_absolute_error: 2.842142, mean_q: 0.815374\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3105/10000: episode: 695, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 1094.751343, mean_absolute_error: 2.578651, mean_q: 0.833862\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3107/10000: episode: 696, duration: 0.011s, episode steps: 2, steps per second: 184, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 865.832031, mean_absolute_error: 2.124556, mean_q: 0.808099\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3111/10000: episode: 697, duration: 0.018s, episode steps: 4, steps per second: 217, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1133.832764, mean_absolute_error: 2.674834, mean_q: 0.783880\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3118/10000: episode: 698, duration: 0.028s, episode steps: 7, steps per second: 247, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1208.792236, mean_absolute_error: 2.861260, mean_q: 0.800846\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3122/10000: episode: 699, duration: 0.019s, episode steps: 4, steps per second: 210, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1136.166016, mean_absolute_error: 2.704829, mean_q: 0.787128\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3127/10000: episode: 700, duration: 0.021s, episode steps: 5, steps per second: 237, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1095.889404, mean_absolute_error: 2.601645, mean_q: 0.806555\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3134/10000: episode: 701, duration: 0.028s, episode steps: 7, steps per second: 252, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.857 [2.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1010.495544, mean_absolute_error: 2.442193, mean_q: 0.805433\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3137/10000: episode: 702, duration: 0.014s, episode steps: 3, steps per second: 215, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 941.950500, mean_absolute_error: 2.298416, mean_q: 0.787954\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3139/10000: episode: 703, duration: 0.016s, episode steps: 2, steps per second: 122, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 1328.351074, mean_absolute_error: 3.102512, mean_q: 0.856103\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3143/10000: episode: 704, duration: 0.020s, episode steps: 4, steps per second: 195, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [2.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 784.461182, mean_absolute_error: 1.922611, mean_q: 0.776686\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3147/10000: episode: 705, duration: 0.018s, episode steps: 4, steps per second: 224, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1022.066040, mean_absolute_error: 2.483263, mean_q: 0.801634\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3153/10000: episode: 706, duration: 0.025s, episode steps: 6, steps per second: 237, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 810.655701, mean_absolute_error: 1.985611, mean_q: 0.795533\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3158/10000: episode: 707, duration: 0.032s, episode steps: 5, steps per second: 158, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1348.747803, mean_absolute_error: 3.177986, mean_q: 0.789537\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3161/10000: episode: 708, duration: 0.015s, episode steps: 3, steps per second: 201, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 941.858093, mean_absolute_error: 2.284372, mean_q: 0.830592\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3169/10000: episode: 709, duration: 0.032s, episode steps: 8, steps per second: 250, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.125 [1.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 1036.672607, mean_absolute_error: 2.458517, mean_q: 0.806931\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3173/10000: episode: 710, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 898.911621, mean_absolute_error: 2.150558, mean_q: 0.787232\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3176/10000: episode: 711, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [0.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1043.984375, mean_absolute_error: 2.493820, mean_q: 0.801276\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3180/10000: episode: 712, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.250 [5.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 747.101379, mean_absolute_error: 1.852638, mean_q: 0.784778\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3186/10000: episode: 713, duration: 0.023s, episode steps: 6, steps per second: 260, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1147.544067, mean_absolute_error: 2.703524, mean_q: 0.811643\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3193/10000: episode: 714, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.286 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1118.003784, mean_absolute_error: 2.654446, mean_q: 0.788538\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3198/10000: episode: 715, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1064.599365, mean_absolute_error: 2.530366, mean_q: 0.766509\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3205/10000: episode: 716, duration: 0.026s, episode steps: 7, steps per second: 274, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.714 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1275.297852, mean_absolute_error: 3.011029, mean_q: 0.753410\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3209/10000: episode: 717, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1371.297852, mean_absolute_error: 3.223795, mean_q: 0.727227\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3215/10000: episode: 718, duration: 0.027s, episode steps: 6, steps per second: 224, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.333 [3.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1016.368835, mean_absolute_error: 2.410224, mean_q: 0.722852\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3217/10000: episode: 719, duration: 0.010s, episode steps: 2, steps per second: 196, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 1412.707764, mean_absolute_error: 3.342462, mean_q: 0.804390\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3220/10000: episode: 720, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [1.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 888.783936, mean_absolute_error: 2.147459, mean_q: 0.746336\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3224/10000: episode: 721, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1289.271240, mean_absolute_error: 3.013791, mean_q: 0.786686\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3230/10000: episode: 722, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.500 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 889.279968, mean_absolute_error: 2.159300, mean_q: 0.719272\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3232/10000: episode: 723, duration: 0.010s, episode steps: 2, steps per second: 197, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 1337.687012, mean_absolute_error: 3.206448, mean_q: 0.741862\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3236/10000: episode: 724, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 940.706055, mean_absolute_error: 2.271350, mean_q: 0.746671\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3238/10000: episode: 725, duration: 0.010s, episode steps: 2, steps per second: 209, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 1093.936401, mean_absolute_error: 2.580184, mean_q: 0.730551\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3241/10000: episode: 726, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1414.403931, mean_absolute_error: 3.357802, mean_q: 0.774912\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3248/10000: episode: 727, duration: 0.028s, episode steps: 7, steps per second: 254, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.571 [2.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1050.215820, mean_absolute_error: 2.495534, mean_q: 0.756458\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3253/10000: episode: 728, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1251.431030, mean_absolute_error: 2.936144, mean_q: 0.759387\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3257/10000: episode: 729, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [3.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1136.562988, mean_absolute_error: 2.695585, mean_q: 0.770738\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3263/10000: episode: 730, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1201.112671, mean_absolute_error: 2.827489, mean_q: 0.776030\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3267/10000: episode: 731, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 901.633545, mean_absolute_error: 2.180086, mean_q: 0.790363\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3269/10000: episode: 732, duration: 0.012s, episode steps: 2, steps per second: 163, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 1328.324219, mean_absolute_error: 3.103534, mean_q: 0.763860\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3273/10000: episode: 733, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1135.367310, mean_absolute_error: 2.685903, mean_q: 0.794794\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3276/10000: episode: 734, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 733.552734, mean_absolute_error: 1.825821, mean_q: 0.767997\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3279/10000: episode: 735, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 939.607239, mean_absolute_error: 2.258885, mean_q: 0.778789\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3283/10000: episode: 736, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [3.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 940.054932, mean_absolute_error: 2.256916, mean_q: 0.748407\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3288/10000: episode: 737, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.800 [1.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1128.767456, mean_absolute_error: 2.673675, mean_q: 0.773556\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3292/10000: episode: 738, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1057.858765, mean_absolute_error: 2.514863, mean_q: 0.821731\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3297/10000: episode: 739, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 877.558594, mean_absolute_error: 2.122120, mean_q: 0.849434\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3305/10000: episode: 740, duration: 0.029s, episode steps: 8, steps per second: 276, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.625 [0.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 881.558533, mean_absolute_error: 2.121974, mean_q: 0.829378\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3309/10000: episode: 741, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1093.931274, mean_absolute_error: 2.580972, mean_q: 0.791642\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3314/10000: episode: 742, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1255.152222, mean_absolute_error: 2.975459, mean_q: 0.775104\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3320/10000: episode: 743, duration: 0.024s, episode steps: 6, steps per second: 245, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1174.142090, mean_absolute_error: 2.772843, mean_q: 0.787498\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3326/10000: episode: 744, duration: 0.025s, episode steps: 6, steps per second: 236, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1250.318726, mean_absolute_error: 2.930824, mean_q: 0.786934\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3330/10000: episode: 745, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [3.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1099.271973, mean_absolute_error: 2.635816, mean_q: 0.764633\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3336/10000: episode: 746, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1096.163208, mean_absolute_error: 2.598661, mean_q: 0.756195\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3341/10000: episode: 747, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 812.693665, mean_absolute_error: 1.957487, mean_q: 0.805036\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3347/10000: episode: 748, duration: 0.022s, episode steps: 6, steps per second: 272, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1095.500854, mean_absolute_error: 2.588372, mean_q: 0.820171\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3350/10000: episode: 749, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 887.703125, mean_absolute_error: 2.147496, mean_q: 0.839007\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3355/10000: episode: 750, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1002.688965, mean_absolute_error: 2.385010, mean_q: 0.861416\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3361/10000: episode: 751, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 886.751709, mean_absolute_error: 2.123572, mean_q: 0.864415\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3363/10000: episode: 752, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 1100.399658, mean_absolute_error: 2.648536, mean_q: 0.860159\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3368/10000: episode: 753, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 938.253540, mean_absolute_error: 2.234489, mean_q: 0.817369\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3375/10000: episode: 754, duration: 0.029s, episode steps: 7, steps per second: 240, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.857 [0.000, 6.000], mean observation: 0.657 [0.000, 2.000], loss: 1030.501709, mean_absolute_error: 2.475464, mean_q: 0.821296\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3379/10000: episode: 755, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 782.088623, mean_absolute_error: 1.888220, mean_q: 0.797059\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3384/10000: episode: 756, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 875.811707, mean_absolute_error: 2.095263, mean_q: 0.813963\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3386/10000: episode: 757, duration: 0.009s, episode steps: 2, steps per second: 211, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.300 [0.000, 2.000], loss: 1253.290039, mean_absolute_error: 2.953107, mean_q: 0.802318\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3389/10000: episode: 758, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: 785.618164, mean_absolute_error: 1.932147, mean_q: 0.848701\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3393/10000: episode: 759, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [3.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1017.374878, mean_absolute_error: 2.422375, mean_q: 0.812045\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3399/10000: episode: 760, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1150.319824, mean_absolute_error: 2.742626, mean_q: 0.838744\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3404/10000: episode: 761, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 816.350342, mean_absolute_error: 2.005190, mean_q: 0.822352\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3409/10000: episode: 762, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1286.714111, mean_absolute_error: 3.048272, mean_q: 0.831423\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3416/10000: episode: 763, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.429 [1.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 1028.791260, mean_absolute_error: 2.458601, mean_q: 0.831679\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3421/10000: episode: 764, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1001.579895, mean_absolute_error: 2.393577, mean_q: 0.815389\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3425/10000: episode: 765, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 3.000], mean observation: 0.500 [0.000, 2.000], loss: 1017.396118, mean_absolute_error: 2.430553, mean_q: 0.816272\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3427/10000: episode: 766, duration: 0.011s, episode steps: 2, steps per second: 186, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 1098.518799, mean_absolute_error: 2.615237, mean_q: 0.832371\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3429/10000: episode: 767, duration: 0.014s, episode steps: 2, steps per second: 148, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 1094.391968, mean_absolute_error: 2.585626, mean_q: 0.790881\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3434/10000: episode: 768, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1127.754883, mean_absolute_error: 2.686152, mean_q: 0.759459\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3440/10000: episode: 769, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1148.657349, mean_absolute_error: 2.720520, mean_q: 0.814325\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3445/10000: episode: 770, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 970.183594, mean_absolute_error: 2.314743, mean_q: 0.814897\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3449/10000: episode: 771, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1134.979248, mean_absolute_error: 2.686914, mean_q: 0.829279\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3453/10000: episode: 772, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1018.993896, mean_absolute_error: 2.446343, mean_q: 0.814565\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3459/10000: episode: 773, duration: 0.022s, episode steps: 6, steps per second: 271, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 809.206970, mean_absolute_error: 1.967852, mean_q: 0.820501\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3465/10000: episode: 774, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1122.079956, mean_absolute_error: 2.664015, mean_q: 0.823289\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3470/10000: episode: 775, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1094.602539, mean_absolute_error: 2.584594, mean_q: 0.833889\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3475/10000: episode: 776, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1282.215088, mean_absolute_error: 2.997817, mean_q: 0.818409\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3480/10000: episode: 777, duration: 0.024s, episode steps: 5, steps per second: 211, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1220.214111, mean_absolute_error: 2.879192, mean_q: 0.789143\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3485/10000: episode: 778, duration: 0.020s, episode steps: 5, steps per second: 244, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1220.196533, mean_absolute_error: 2.869168, mean_q: 0.759369\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3489/10000: episode: 779, duration: 0.018s, episode steps: 4, steps per second: 224, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [1.000, 3.000], mean observation: 0.500 [0.000, 2.000], loss: 1331.605835, mean_absolute_error: 3.144130, mean_q: 0.814616\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3491/10000: episode: 780, duration: 0.011s, episode steps: 2, steps per second: 189, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 859.592468, mean_absolute_error: 2.066242, mean_q: 0.744880\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3496/10000: episode: 781, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1221.473389, mean_absolute_error: 2.890130, mean_q: 0.779645\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3503/10000: episode: 782, duration: 0.026s, episode steps: 7, steps per second: 264, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1167.774170, mean_absolute_error: 2.799894, mean_q: 0.846729\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3508/10000: episode: 783, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1065.882202, mean_absolute_error: 2.554474, mean_q: 0.873586\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3511/10000: episode: 784, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 839.593262, mean_absolute_error: 2.072504, mean_q: 0.850977\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3519/10000: episode: 785, duration: 0.029s, episode steps: 8, steps per second: 272, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.375 [1.000, 7.000], mean observation: 0.825 [0.000, 2.000], loss: 1156.288818, mean_absolute_error: 2.754098, mean_q: 0.867940\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3521/10000: episode: 786, duration: 0.010s, episode steps: 2, steps per second: 194, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 1093.970947, mean_absolute_error: 2.589804, mean_q: 0.830932\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3527/10000: episode: 787, duration: 0.024s, episode steps: 6, steps per second: 250, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 911.789856, mean_absolute_error: 2.188522, mean_q: 0.825969\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3534/10000: episode: 788, duration: 0.030s, episode steps: 7, steps per second: 237, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [1.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1365.074341, mean_absolute_error: 3.208205, mean_q: 0.836541\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3541/10000: episode: 789, duration: 0.027s, episode steps: 7, steps per second: 263, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.714 [2.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1074.332642, mean_absolute_error: 2.568033, mean_q: 0.883235\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3544/10000: episode: 790, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1150.196655, mean_absolute_error: 2.743811, mean_q: 0.901520\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3547/10000: episode: 791, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 937.731445, mean_absolute_error: 2.237507, mean_q: 0.889863\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3551/10000: episode: 792, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 860.993347, mean_absolute_error: 2.077694, mean_q: 0.876520\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3556/10000: episode: 793, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 816.411499, mean_absolute_error: 2.003992, mean_q: 0.867012\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3559/10000: episode: 794, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [3.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1302.322388, mean_absolute_error: 3.055057, mean_q: 0.892435\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3563/10000: episode: 795, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 782.217773, mean_absolute_error: 1.909561, mean_q: 0.893281\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3567/10000: episode: 796, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 823.943604, mean_absolute_error: 2.010803, mean_q: 0.874371\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3571/10000: episode: 797, duration: 0.020s, episode steps: 4, steps per second: 199, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1136.160767, mean_absolute_error: 2.713969, mean_q: 0.813241\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3576/10000: episode: 798, duration: 0.041s, episode steps: 5, steps per second: 123, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.600 [1.000, 6.000], mean observation: 0.520 [0.000, 2.000], loss: 875.803101, mean_absolute_error: 2.087236, mean_q: 0.784688\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3579/10000: episode: 799, duration: 0.019s, episode steps: 3, steps per second: 159, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1202.278320, mean_absolute_error: 2.858269, mean_q: 0.858805\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3585/10000: episode: 800, duration: 0.041s, episode steps: 6, steps per second: 145, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 915.805420, mean_absolute_error: 2.214409, mean_q: 0.848761\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3589/10000: episode: 801, duration: 0.025s, episode steps: 4, steps per second: 160, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1368.329102, mean_absolute_error: 3.174069, mean_q: 0.848178\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3594/10000: episode: 802, duration: 0.034s, episode steps: 5, steps per second: 149, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.000 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1252.842529, mean_absolute_error: 2.951856, mean_q: 0.828266\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3596/10000: episode: 803, duration: 0.019s, episode steps: 2, steps per second: 104, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 703.320862, mean_absolute_error: 1.709857, mean_q: 0.795816\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3599/10000: episode: 804, duration: 0.024s, episode steps: 3, steps per second: 126, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 783.547424, mean_absolute_error: 1.911670, mean_q: 0.796298\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3604/10000: episode: 805, duration: 0.035s, episode steps: 5, steps per second: 144, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [4.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 910.107117, mean_absolute_error: 2.215025, mean_q: 0.789910\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3608/10000: episode: 806, duration: 0.027s, episode steps: 4, steps per second: 149, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1054.921143, mean_absolute_error: 2.506108, mean_q: 0.810825\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3613/10000: episode: 807, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1098.998291, mean_absolute_error: 2.648266, mean_q: 0.794844\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3618/10000: episode: 808, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1034.437622, mean_absolute_error: 2.467957, mean_q: 0.769362\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3622/10000: episode: 809, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1212.716553, mean_absolute_error: 2.854383, mean_q: 0.786901\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3627/10000: episode: 810, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.000 [2.000, 6.000], mean observation: 0.520 [0.000, 2.000], loss: 1068.979736, mean_absolute_error: 2.578288, mean_q: 0.762021\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3633/10000: episode: 811, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1093.957397, mean_absolute_error: 2.575670, mean_q: 0.823920\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3636/10000: episode: 812, duration: 0.014s, episode steps: 3, steps per second: 213, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 677.304016, mean_absolute_error: 1.663620, mean_q: 0.794570\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3642/10000: episode: 813, duration: 0.024s, episode steps: 6, steps per second: 249, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 4.833 [2.000, 8.000], mean observation: 0.700 [0.000, 2.000], loss: 989.763428, mean_absolute_error: 2.345315, mean_q: 0.816513\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3645/10000: episode: 814, duration: 0.014s, episode steps: 3, steps per second: 218, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 626.243408, mean_absolute_error: 1.539339, mean_q: 0.805749\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3652/10000: episode: 815, duration: 0.028s, episode steps: 7, steps per second: 253, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1162.348389, mean_absolute_error: 2.739541, mean_q: 0.847655\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3656/10000: episode: 816, duration: 0.021s, episode steps: 4, steps per second: 195, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1055.698242, mean_absolute_error: 2.497684, mean_q: 0.847138\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3661/10000: episode: 817, duration: 0.022s, episode steps: 5, steps per second: 224, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1187.694824, mean_absolute_error: 2.786444, mean_q: 0.852505\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3663/10000: episode: 818, duration: 0.010s, episode steps: 2, steps per second: 193, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 859.588074, mean_absolute_error: 2.064372, mean_q: 0.841907\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3670/10000: episode: 819, duration: 0.028s, episode steps: 7, steps per second: 247, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [2.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1117.177002, mean_absolute_error: 2.644375, mean_q: 0.846800\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3673/10000: episode: 820, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1616.720093, mean_absolute_error: 3.760635, mean_q: 0.862014\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3677/10000: episode: 821, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [2.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1097.113281, mean_absolute_error: 2.621920, mean_q: 0.855060\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3684/10000: episode: 822, duration: 0.027s, episode steps: 7, steps per second: 260, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.857 [2.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1232.272705, mean_absolute_error: 2.934938, mean_q: 0.862207\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3688/10000: episode: 823, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1096.984863, mean_absolute_error: 2.616988, mean_q: 0.910049\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3695/10000: episode: 824, duration: 0.027s, episode steps: 7, steps per second: 263, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 3.286 [0.000, 7.000], mean observation: 0.771 [0.000, 2.000], loss: 1074.345947, mean_absolute_error: 2.570393, mean_q: 0.884455\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3701/10000: episode: 825, duration: 0.025s, episode steps: 6, steps per second: 242, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1201.311157, mean_absolute_error: 2.820064, mean_q: 0.916489\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3708/10000: episode: 826, duration: 0.032s, episode steps: 7, steps per second: 220, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.714 [2.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1097.644287, mean_absolute_error: 2.627138, mean_q: 0.841320\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3715/10000: episode: 827, duration: 0.032s, episode steps: 7, steps per second: 220, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.571 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 984.122437, mean_absolute_error: 2.352293, mean_q: 0.868803\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3718/10000: episode: 828, duration: 0.015s, episode steps: 3, steps per second: 197, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [5.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 890.636047, mean_absolute_error: 2.156733, mean_q: 0.896958\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3724/10000: episode: 829, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1122.567749, mean_absolute_error: 2.653978, mean_q: 0.904330\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3729/10000: episode: 830, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 938.962891, mean_absolute_error: 2.253236, mean_q: 0.899060\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3733/10000: episode: 831, duration: 0.025s, episode steps: 4, steps per second: 158, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1021.980896, mean_absolute_error: 2.477291, mean_q: 0.883244\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3740/10000: episode: 832, duration: 0.029s, episode steps: 7, steps per second: 238, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1052.014404, mean_absolute_error: 2.517369, mean_q: 0.855265\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3746/10000: episode: 833, duration: 0.027s, episode steps: 6, steps per second: 226, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 892.821716, mean_absolute_error: 2.195707, mean_q: 0.840168\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3750/10000: episode: 834, duration: 0.017s, episode steps: 4, steps per second: 231, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.500 [0.000, 2.000], loss: 1133.012085, mean_absolute_error: 2.669400, mean_q: 0.839804\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3752/10000: episode: 835, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 709.559937, mean_absolute_error: 1.790443, mean_q: 0.838642\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3757/10000: episode: 836, duration: 0.022s, episode steps: 5, steps per second: 231, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 907.743835, mean_absolute_error: 2.191077, mean_q: 0.845644\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3762/10000: episode: 837, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 941.603821, mean_absolute_error: 2.280813, mean_q: 0.801033\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3770/10000: episode: 838, duration: 0.040s, episode steps: 8, steps per second: 202, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 2.750 [0.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 1037.652832, mean_absolute_error: 2.480432, mean_q: 0.820914\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3776/10000: episode: 839, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1124.162476, mean_absolute_error: 2.688390, mean_q: 0.755144\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3782/10000: episode: 840, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1199.717163, mean_absolute_error: 2.825902, mean_q: 0.791067\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3787/10000: episode: 841, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1129.058228, mean_absolute_error: 2.691934, mean_q: 0.791665\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3789/10000: episode: 842, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 943.941345, mean_absolute_error: 2.304244, mean_q: 0.775598\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3791/10000: episode: 843, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 706.583069, mean_absolute_error: 1.767436, mean_q: 0.802749\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3795/10000: episode: 844, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1332.729614, mean_absolute_error: 3.148746, mean_q: 0.799320\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3801/10000: episode: 845, duration: 0.023s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1097.008301, mean_absolute_error: 2.626640, mean_q: 0.749671\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3806/10000: episode: 846, duration: 0.023s, episode steps: 5, steps per second: 222, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1158.303345, mean_absolute_error: 2.731515, mean_q: 0.707902\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3812/10000: episode: 847, duration: 0.024s, episode steps: 6, steps per second: 254, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1071.583496, mean_absolute_error: 2.565619, mean_q: 0.757849\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3818/10000: episode: 848, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1018.847168, mean_absolute_error: 2.439477, mean_q: 0.751516\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3820/10000: episode: 849, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 940.889771, mean_absolute_error: 2.283973, mean_q: 0.796393\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3826/10000: episode: 850, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 965.839783, mean_absolute_error: 2.319040, mean_q: 0.755939\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3830/10000: episode: 851, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 976.788330, mean_absolute_error: 2.314669, mean_q: 0.797764\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3832/10000: episode: 852, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 553.056091, mean_absolute_error: 1.450334, mean_q: 0.750856\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3837/10000: episode: 853, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1063.966064, mean_absolute_error: 2.527274, mean_q: 0.746099\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3840/10000: episode: 854, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 833.587341, mean_absolute_error: 2.019977, mean_q: 0.719025\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3847/10000: episode: 855, duration: 0.026s, episode steps: 7, steps per second: 273, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [2.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 895.675598, mean_absolute_error: 2.162245, mean_q: 0.731115\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3851/10000: episode: 856, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1290.833740, mean_absolute_error: 3.036064, mean_q: 0.759435\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3858/10000: episode: 857, duration: 0.027s, episode steps: 7, steps per second: 259, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.714 [0.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 943.779236, mean_absolute_error: 2.302242, mean_q: 0.783742\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3860/10000: episode: 858, duration: 0.013s, episode steps: 2, steps per second: 155, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 866.477539, mean_absolute_error: 2.139024, mean_q: 0.747561\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3863/10000: episode: 859, duration: 0.014s, episode steps: 3, steps per second: 211, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 991.878418, mean_absolute_error: 2.375106, mean_q: 0.799314\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3867/10000: episode: 860, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 665.843018, mean_absolute_error: 1.649038, mean_q: 0.760127\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3871/10000: episode: 861, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [1.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 1133.038330, mean_absolute_error: 2.675676, mean_q: 0.804959\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3877/10000: episode: 862, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1252.892090, mean_absolute_error: 2.958223, mean_q: 0.782838\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3883/10000: episode: 863, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1097.076294, mean_absolute_error: 2.619937, mean_q: 0.759048\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3888/10000: episode: 864, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.400 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 691.979126, mean_absolute_error: 1.724941, mean_q: 0.754712\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3893/10000: episode: 865, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1193.211914, mean_absolute_error: 2.842700, mean_q: 0.774987\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3901/10000: episode: 866, duration: 0.029s, episode steps: 8, steps per second: 272, episode reward: 20.000, mean reward: 2.500 [0.000, 20.000], mean action: 3.875 [0.000, 8.000], mean observation: 0.850 [0.000, 2.000], loss: 1115.762207, mean_absolute_error: 2.650635, mean_q: 0.723583\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3904/10000: episode: 867, duration: 0.013s, episode steps: 3, steps per second: 235, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 835.457336, mean_absolute_error: 2.039939, mean_q: 0.739135\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3911/10000: episode: 868, duration: 0.030s, episode steps: 7, steps per second: 236, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1006.114807, mean_absolute_error: 2.389618, mean_q: 0.734441\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3917/10000: episode: 869, duration: 0.024s, episode steps: 6, steps per second: 247, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1095.186279, mean_absolute_error: 2.591162, mean_q: 0.733751\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3924/10000: episode: 870, duration: 0.027s, episode steps: 7, steps per second: 262, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1074.314819, mean_absolute_error: 2.566945, mean_q: 0.736850\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3929/10000: episode: 871, duration: 0.021s, episode steps: 5, steps per second: 240, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.400 [2.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 880.832825, mean_absolute_error: 2.159519, mean_q: 0.771950\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3933/10000: episode: 872, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1136.263062, mean_absolute_error: 2.702325, mean_q: 0.812860\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3940/10000: episode: 873, duration: 0.028s, episode steps: 7, steps per second: 253, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 965.891907, mean_absolute_error: 2.334112, mean_q: 0.850618\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3943/10000: episode: 874, duration: 0.014s, episode steps: 3, steps per second: 219, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [1.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 679.377502, mean_absolute_error: 1.676408, mean_q: 0.866710\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3948/10000: episode: 875, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 754.588745, mean_absolute_error: 1.855982, mean_q: 0.865009\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3951/10000: episode: 876, duration: 0.015s, episode steps: 3, steps per second: 203, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 991.858704, mean_absolute_error: 2.368194, mean_q: 0.847090\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3956/10000: episode: 877, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1002.605103, mean_absolute_error: 2.404798, mean_q: 0.867762\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3961/10000: episode: 878, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 968.965027, mean_absolute_error: 2.304568, mean_q: 0.874828\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3965/10000: episode: 879, duration: 0.019s, episode steps: 4, steps per second: 205, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1054.892578, mean_absolute_error: 2.494757, mean_q: 0.852793\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3967/10000: episode: 880, duration: 0.011s, episode steps: 2, steps per second: 185, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 859.615051, mean_absolute_error: 2.068357, mean_q: 0.859924\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3970/10000: episode: 881, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [5.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1252.269653, mean_absolute_error: 2.943340, mean_q: 0.851873\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3976/10000: episode: 882, duration: 0.022s, episode steps: 6, steps per second: 271, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 992.016785, mean_absolute_error: 2.376738, mean_q: 0.835922\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3981/10000: episode: 883, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1501.453125, mean_absolute_error: 3.499521, mean_q: 0.836872\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3984/10000: episode: 884, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 733.557922, mean_absolute_error: 1.821468, mean_q: 0.806174\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3991/10000: episode: 885, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.671 [0.000, 2.000], loss: 1253.692627, mean_absolute_error: 2.967224, mean_q: 0.809195\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 3994/10000: episode: 886, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 837.692932, mean_absolute_error: 2.018309, mean_q: 0.757731\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4000/10000: episode: 887, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 6.000 [3.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1098.971313, mean_absolute_error: 2.640945, mean_q: 0.769114\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4006/10000: episode: 888, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1069.078979, mean_absolute_error: 2.551626, mean_q: 0.811089\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4011/10000: episode: 889, duration: 0.021s, episode steps: 5, steps per second: 243, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1098.982666, mean_absolute_error: 2.636885, mean_q: 0.812965\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4015/10000: episode: 890, duration: 0.018s, episode steps: 4, steps per second: 221, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1177.253052, mean_absolute_error: 2.802767, mean_q: 0.784482\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4020/10000: episode: 891, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 753.493408, mean_absolute_error: 1.850387, mean_q: 0.752563\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4024/10000: episode: 892, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 979.925293, mean_absolute_error: 2.367044, mean_q: 0.817221\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4030/10000: episode: 893, duration: 0.023s, episode steps: 6, steps per second: 260, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1148.061646, mean_absolute_error: 2.725993, mean_q: 0.865097\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4034/10000: episode: 894, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 861.193237, mean_absolute_error: 2.085218, mean_q: 0.854340\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4038/10000: episode: 895, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1017.433960, mean_absolute_error: 2.433492, mean_q: 0.830486\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4043/10000: episode: 896, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [3.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1409.380493, mean_absolute_error: 3.298457, mean_q: 0.852655\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4047/10000: episode: 897, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [2.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 939.132812, mean_absolute_error: 2.254326, mean_q: 0.811087\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4053/10000: episode: 898, duration: 0.022s, episode steps: 6, steps per second: 271, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1017.929688, mean_absolute_error: 2.434526, mean_q: 0.816912\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4060/10000: episode: 899, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.714 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1276.945923, mean_absolute_error: 3.018637, mean_q: 0.849129\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4063/10000: episode: 900, duration: 0.016s, episode steps: 3, steps per second: 190, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [3.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 990.646057, mean_absolute_error: 2.355915, mean_q: 0.818083\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4067/10000: episode: 901, duration: 0.018s, episode steps: 4, steps per second: 224, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 898.691406, mean_absolute_error: 2.161276, mean_q: 0.816284\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4071/10000: episode: 902, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 629.927917, mean_absolute_error: 1.596347, mean_q: 0.827441\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4074/10000: episode: 903, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 627.320740, mean_absolute_error: 1.566547, mean_q: 0.805299\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4076/10000: episode: 904, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 1103.330322, mean_absolute_error: 2.685610, mean_q: 0.827275\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4079/10000: episode: 905, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [0.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1408.543335, mean_absolute_error: 3.290203, mean_q: 0.816084\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4082/10000: episode: 906, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [3.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1412.752930, mean_absolute_error: 3.334575, mean_q: 0.828653\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4090/10000: episode: 907, duration: 0.030s, episode steps: 8, steps per second: 268, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 2.875 [0.000, 6.000], mean observation: 0.725 [0.000, 2.000], loss: 1058.665527, mean_absolute_error: 2.539710, mean_q: 0.833399\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4093/10000: episode: 908, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1150.200562, mean_absolute_error: 2.742599, mean_q: 0.840706\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4100/10000: episode: 909, duration: 0.026s, episode steps: 7, steps per second: 267, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [2.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1009.174316, mean_absolute_error: 2.443567, mean_q: 0.861425\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4105/10000: episode: 910, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 939.500366, mean_absolute_error: 2.260489, mean_q: 0.844910\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4110/10000: episode: 911, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.200 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1001.438660, mean_absolute_error: 2.387182, mean_q: 0.851378\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4116/10000: episode: 912, duration: 0.027s, episode steps: 6, steps per second: 221, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 860.670715, mean_absolute_error: 2.083150, mean_q: 0.835265\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4122/10000: episode: 913, duration: 0.024s, episode steps: 6, steps per second: 251, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.833 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1462.053345, mean_absolute_error: 3.416775, mean_q: 0.814859\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4125/10000: episode: 914, duration: 0.013s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [3.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1048.103027, mean_absolute_error: 2.526137, mean_q: 0.805144\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4130/10000: episode: 915, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 911.900879, mean_absolute_error: 2.217688, mean_q: 0.793838\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4137/10000: episode: 916, duration: 0.025s, episode steps: 7, steps per second: 276, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [2.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1118.966431, mean_absolute_error: 2.663929, mean_q: 0.795859\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4143/10000: episode: 917, duration: 0.023s, episode steps: 6, steps per second: 258, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 2.833 [0.000, 7.000], mean observation: 0.700 [0.000, 2.000], loss: 940.942322, mean_absolute_error: 2.267834, mean_q: 0.791517\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4147/10000: episode: 918, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [3.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 859.595093, mean_absolute_error: 2.057599, mean_q: 0.840636\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4152/10000: episode: 919, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1097.567261, mean_absolute_error: 2.620157, mean_q: 0.816017\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4155/10000: episode: 920, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1200.226807, mean_absolute_error: 2.840272, mean_q: 0.774151\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4161/10000: episode: 921, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1225.654663, mean_absolute_error: 2.890661, mean_q: 0.791903\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4165/10000: episode: 922, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 980.661499, mean_absolute_error: 2.358703, mean_q: 0.762057\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4171/10000: episode: 923, duration: 0.026s, episode steps: 6, steps per second: 231, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 941.772034, mean_absolute_error: 2.271492, mean_q: 0.776294\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4177/10000: episode: 924, duration: 0.028s, episode steps: 6, steps per second: 217, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1226.132568, mean_absolute_error: 2.891231, mean_q: 0.758421\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4180/10000: episode: 925, duration: 0.014s, episode steps: 3, steps per second: 219, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 989.809387, mean_absolute_error: 2.357860, mean_q: 0.719560\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4185/10000: episode: 926, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 937.723328, mean_absolute_error: 2.241996, mean_q: 0.710529\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4189/10000: episode: 927, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1057.975220, mean_absolute_error: 2.517221, mean_q: 0.718013\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4191/10000: episode: 928, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 1095.560303, mean_absolute_error: 2.587107, mean_q: 0.718047\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4195/10000: episode: 929, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 983.021790, mean_absolute_error: 2.392267, mean_q: 0.707203\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4197/10000: episode: 930, duration: 0.011s, episode steps: 2, steps per second: 190, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 940.514099, mean_absolute_error: 2.261353, mean_q: 0.698040\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4203/10000: episode: 931, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1278.282837, mean_absolute_error: 3.005209, mean_q: 0.689962\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4209/10000: episode: 932, duration: 0.024s, episode steps: 6, steps per second: 246, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.333 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 834.985840, mean_absolute_error: 2.010556, mean_q: 0.661781\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4212/10000: episode: 933, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1099.966797, mean_absolute_error: 2.637779, mean_q: 0.756694\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4217/10000: episode: 934, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 1.400 [0.000, 3.000], mean observation: 0.600 [0.000, 2.000], loss: 1031.733765, mean_absolute_error: 2.436624, mean_q: 0.687380\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4222/10000: episode: 935, duration: 0.023s, episode steps: 5, steps per second: 215, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 937.675415, mean_absolute_error: 2.227931, mean_q: 0.667273\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4226/10000: episode: 936, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1133.112305, mean_absolute_error: 2.665331, mean_q: 0.706237\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4231/10000: episode: 937, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.200 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1126.424927, mean_absolute_error: 2.655449, mean_q: 0.712628\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4236/10000: episode: 938, duration: 0.019s, episode steps: 5, steps per second: 264, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 752.823425, mean_absolute_error: 1.843756, mean_q: 0.682147\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4242/10000: episode: 939, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 992.674744, mean_absolute_error: 2.377090, mean_q: 0.671634\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4244/10000: episode: 940, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 1.000], loss: 1328.296143, mean_absolute_error: 3.094495, mean_q: 0.689340\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4248/10000: episode: 941, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1292.375122, mean_absolute_error: 3.049905, mean_q: 0.693850\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4255/10000: episode: 942, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.714 [0.000, 6.000], mean observation: 0.657 [0.000, 2.000], loss: 1297.014526, mean_absolute_error: 3.040697, mean_q: 0.677430\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4258/10000: episode: 943, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1146.002319, mean_absolute_error: 2.693193, mean_q: 0.661264\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4260/10000: episode: 944, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.200 [0.000, 1.000], loss: 1015.757751, mean_absolute_error: 2.392205, mean_q: 0.675817\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4263/10000: episode: 945, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.908081, mean_absolute_error: 2.579346, mean_q: 0.717419\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4269/10000: episode: 946, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 965.945007, mean_absolute_error: 2.303967, mean_q: 0.694052\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4272/10000: episode: 947, duration: 0.014s, episode steps: 3, steps per second: 219, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1045.758301, mean_absolute_error: 2.493116, mean_q: 0.685545\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4277/10000: episode: 948, duration: 0.023s, episode steps: 5, steps per second: 215, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.200 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1282.126343, mean_absolute_error: 2.992081, mean_q: 0.743041\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4280/10000: episode: 949, duration: 0.014s, episode steps: 3, steps per second: 211, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [1.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: 737.489258, mean_absolute_error: 1.865771, mean_q: 0.686757\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4282/10000: episode: 950, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 940.786865, mean_absolute_error: 2.258220, mean_q: 0.675208\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4287/10000: episode: 951, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1190.050171, mean_absolute_error: 2.812121, mean_q: 0.714790\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4296/10000: episode: 952, duration: 0.033s, episode steps: 9, steps per second: 276, episode reward: -100.000, mean reward: -11.111 [-100.000, 0.000], mean action: 4.667 [0.000, 8.000], mean observation: 0.811 [0.000, 2.000], loss: 1094.973511, mean_absolute_error: 2.584365, mean_q: 0.699637\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4301/10000: episode: 953, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.400 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1031.434448, mean_absolute_error: 2.440944, mean_q: 0.703092\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4304/10000: episode: 954, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [3.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 991.803772, mean_absolute_error: 2.351537, mean_q: 0.728840\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4309/10000: episode: 955, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1220.177734, mean_absolute_error: 2.859236, mean_q: 0.755991\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4314/10000: episode: 956, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 907.697937, mean_absolute_error: 2.183440, mean_q: 0.644450\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4316/10000: episode: 957, duration: 0.010s, episode steps: 2, steps per second: 208, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 706.459473, mean_absolute_error: 1.755754, mean_q: 0.678386\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4321/10000: episode: 958, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1001.312317, mean_absolute_error: 2.383898, mean_q: 0.676931\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4327/10000: episode: 959, duration: 0.028s, episode steps: 6, steps per second: 218, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1068.946899, mean_absolute_error: 2.532768, mean_q: 0.747226\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4332/10000: episode: 960, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1317.298584, mean_absolute_error: 3.101505, mean_q: 0.692575\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4336/10000: episode: 961, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 976.759521, mean_absolute_error: 2.319632, mean_q: 0.682759\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4342/10000: episode: 962, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 887.567566, mean_absolute_error: 2.136732, mean_q: 0.668144\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4346/10000: episode: 963, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 824.223083, mean_absolute_error: 2.007528, mean_q: 0.671404\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4352/10000: episode: 964, duration: 0.022s, episode steps: 6, steps per second: 270, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 965.304016, mean_absolute_error: 2.308782, mean_q: 0.669012\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4355/10000: episode: 965, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1257.132690, mean_absolute_error: 2.993005, mean_q: 0.674135\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4361/10000: episode: 966, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 782.871277, mean_absolute_error: 1.888622, mean_q: 0.703287\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4366/10000: episode: 967, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 939.541199, mean_absolute_error: 2.244228, mean_q: 0.728036\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4373/10000: episode: 968, duration: 0.026s, episode steps: 7, steps per second: 264, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [1.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 917.928345, mean_absolute_error: 2.208162, mean_q: 0.746862\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4376/10000: episode: 969, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [1.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 944.118896, mean_absolute_error: 2.295091, mean_q: 0.711959\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4383/10000: episode: 970, duration: 0.027s, episode steps: 7, steps per second: 257, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 3.571 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1097.658813, mean_absolute_error: 2.607506, mean_q: 0.718808\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4385/10000: episode: 971, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.300 [0.000, 2.000], loss: 1024.544312, mean_absolute_error: 2.500667, mean_q: 0.709024\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4387/10000: episode: 972, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.300 [0.000, 2.000], loss: 865.868591, mean_absolute_error: 2.113046, mean_q: 0.716043\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4390/10000: episode: 973, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1045.754028, mean_absolute_error: 2.491318, mean_q: 0.728767\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4396/10000: episode: 974, duration: 0.026s, episode steps: 6, steps per second: 235, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1408.548828, mean_absolute_error: 3.280252, mean_q: 0.675959\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4401/10000: episode: 975, duration: 0.027s, episode steps: 5, steps per second: 187, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 911.421509, mean_absolute_error: 2.214820, mean_q: 0.667472\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4404/10000: episode: 976, duration: 0.015s, episode steps: 3, steps per second: 206, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1357.791382, mean_absolute_error: 3.171587, mean_q: 0.656243\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4408/10000: episode: 977, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 668.780518, mean_absolute_error: 1.670942, mean_q: 0.683918\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4416/10000: episode: 978, duration: 0.034s, episode steps: 8, steps per second: 237, episode reward: -20.000, mean reward: -2.500 [-20.000, 0.000], mean action: 4.250 [0.000, 8.000], mean observation: 0.750 [0.000, 2.000], loss: 880.648804, mean_absolute_error: 2.120185, mean_q: 0.677607\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4421/10000: episode: 979, duration: 0.035s, episode steps: 5, steps per second: 143, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1285.581543, mean_absolute_error: 3.034650, mean_q: 0.676745\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4424/10000: episode: 980, duration: 0.018s, episode steps: 3, steps per second: 168, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [5.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 941.812439, mean_absolute_error: 2.275105, mean_q: 0.640363\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4432/10000: episode: 981, duration: 0.034s, episode steps: 8, steps per second: 237, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 2.875 [0.000, 6.000], mean observation: 0.725 [0.000, 2.000], loss: 1095.668213, mean_absolute_error: 2.594857, mean_q: 0.685028\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4439/10000: episode: 982, duration: 0.032s, episode steps: 7, steps per second: 222, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1050.091431, mean_absolute_error: 2.489336, mean_q: 0.682090\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4443/10000: episode: 983, duration: 0.020s, episode steps: 4, steps per second: 202, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1056.995361, mean_absolute_error: 2.504431, mean_q: 0.670463\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4450/10000: episode: 984, duration: 0.034s, episode steps: 7, steps per second: 208, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.714 [2.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 984.125916, mean_absolute_error: 2.351255, mean_q: 0.666738\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4455/10000: episode: 985, duration: 0.025s, episode steps: 5, steps per second: 200, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1161.673096, mean_absolute_error: 2.762836, mean_q: 0.697888\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4462/10000: episode: 986, duration: 0.031s, episode steps: 7, steps per second: 228, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.671 [0.000, 2.000], loss: 1096.379150, mean_absolute_error: 2.597054, mean_q: 0.778435\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4466/10000: episode: 987, duration: 0.019s, episode steps: 4, steps per second: 207, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1139.186523, mean_absolute_error: 2.730939, mean_q: 0.871622\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4468/10000: episode: 988, duration: 0.013s, episode steps: 2, steps per second: 154, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 859.527344, mean_absolute_error: 2.059819, mean_q: 0.868876\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4475/10000: episode: 989, duration: 0.032s, episode steps: 7, steps per second: 219, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.714 [0.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 1364.253906, mean_absolute_error: 3.189314, mean_q: 0.847966\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4482/10000: episode: 990, duration: 0.030s, episode steps: 7, steps per second: 231, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1297.458740, mean_absolute_error: 3.050921, mean_q: 0.725287\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4487/10000: episode: 991, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1188.258057, mean_absolute_error: 2.773654, mean_q: 0.763646\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4491/10000: episode: 992, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1017.342407, mean_absolute_error: 2.416893, mean_q: 0.791069\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4498/10000: episode: 993, duration: 0.027s, episode steps: 7, steps per second: 257, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1209.358643, mean_absolute_error: 2.856890, mean_q: 0.798917\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4500/10000: episode: 994, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 1410.903687, mean_absolute_error: 3.303280, mean_q: 0.769562\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4503/10000: episode: 995, duration: 0.014s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [3.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1200.157715, mean_absolute_error: 2.826265, mean_q: 0.773503\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4509/10000: episode: 996, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.167 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 1121.552368, mean_absolute_error: 2.651853, mean_q: 0.780752\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4515/10000: episode: 997, duration: 0.026s, episode steps: 6, steps per second: 231, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 890.460754, mean_absolute_error: 2.161318, mean_q: 0.767186\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4520/10000: episode: 998, duration: 0.022s, episode steps: 5, steps per second: 231, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1097.555542, mean_absolute_error: 2.614998, mean_q: 0.795802\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4525/10000: episode: 999, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1096.650635, mean_absolute_error: 2.601307, mean_q: 0.783999\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4530/10000: episode: 1000, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1004.037598, mean_absolute_error: 2.406355, mean_q: 0.767295\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4536/10000: episode: 1001, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 937.709167, mean_absolute_error: 2.227953, mean_q: 0.795355\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4540/10000: episode: 1002, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 978.511169, mean_absolute_error: 2.332900, mean_q: 0.770820\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4545/10000: episode: 1003, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1282.627563, mean_absolute_error: 2.997031, mean_q: 0.783402\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4553/10000: episode: 1004, duration: 0.034s, episode steps: 8, steps per second: 234, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.625 [1.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1059.580811, mean_absolute_error: 2.544681, mean_q: 0.786367\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4556/10000: episode: 1005, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 939.782776, mean_absolute_error: 2.250536, mean_q: 0.745522\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4561/10000: episode: 1006, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [1.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1126.428467, mean_absolute_error: 2.658704, mean_q: 0.755346\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4568/10000: episode: 1007, duration: 0.030s, episode steps: 7, steps per second: 234, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1387.505859, mean_absolute_error: 3.249020, mean_q: 0.780228\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4572/10000: episode: 1008, duration: 0.019s, episode steps: 4, steps per second: 215, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 981.697937, mean_absolute_error: 2.361082, mean_q: 0.789579\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4579/10000: episode: 1009, duration: 0.027s, episode steps: 7, steps per second: 257, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [1.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1074.343994, mean_absolute_error: 2.559759, mean_q: 0.788120\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4582/10000: episode: 1010, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1043.964233, mean_absolute_error: 2.492156, mean_q: 0.827704\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4584/10000: episode: 1011, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 703.364868, mean_absolute_error: 1.723355, mean_q: 0.807873\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4590/10000: episode: 1012, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1095.908813, mean_absolute_error: 2.594033, mean_q: 0.800666\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4594/10000: episode: 1013, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1056.309326, mean_absolute_error: 2.509366, mean_q: 0.790003\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4599/10000: episode: 1014, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.200 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1126.755981, mean_absolute_error: 2.666721, mean_q: 0.779802\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4602/10000: episode: 1015, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 993.961243, mean_absolute_error: 2.391751, mean_q: 0.828828\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4605/10000: episode: 1016, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [4.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 783.567566, mean_absolute_error: 1.915272, mean_q: 0.735678\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4610/10000: episode: 1017, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.400 [1.000, 7.000], mean observation: 0.620 [0.000, 2.000], loss: 1033.969971, mean_absolute_error: 2.471872, mean_q: 0.762452\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4615/10000: episode: 1018, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.620 [0.000, 2.000], loss: 1033.044922, mean_absolute_error: 2.449902, mean_q: 0.741283\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4622/10000: episode: 1019, duration: 0.032s, episode steps: 7, steps per second: 222, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1051.130249, mean_absolute_error: 2.504751, mean_q: 0.779213\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4628/10000: episode: 1020, duration: 0.024s, episode steps: 6, steps per second: 250, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.167 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1229.143433, mean_absolute_error: 2.916782, mean_q: 0.813586\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4634/10000: episode: 1021, duration: 0.026s, episode steps: 6, steps per second: 234, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 757.503601, mean_absolute_error: 1.851810, mean_q: 0.816848\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4639/10000: episode: 1022, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1000.338074, mean_absolute_error: 2.376748, mean_q: 0.797514\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4643/10000: episode: 1023, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [1.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 1173.933105, mean_absolute_error: 2.770514, mean_q: 0.795437\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4649/10000: episode: 1024, duration: 0.024s, episode steps: 6, steps per second: 245, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1147.941284, mean_absolute_error: 2.707745, mean_q: 0.729383\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4652/10000: episode: 1025, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [0.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.918579, mean_absolute_error: 2.571802, mean_q: 0.755409\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4657/10000: episode: 1026, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 877.452942, mean_absolute_error: 2.098049, mean_q: 0.805300\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4659/10000: episode: 1027, duration: 0.011s, episode steps: 2, steps per second: 188, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 1019.224915, mean_absolute_error: 2.429658, mean_q: 0.825523\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4667/10000: episode: 1028, duration: 0.033s, episode steps: 8, steps per second: 244, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.750 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 959.648132, mean_absolute_error: 2.289105, mean_q: 0.832841\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4671/10000: episode: 1029, duration: 0.019s, episode steps: 4, steps per second: 216, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1250.137695, mean_absolute_error: 2.919030, mean_q: 0.810354\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4676/10000: episode: 1030, duration: 0.025s, episode steps: 5, steps per second: 201, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 911.412964, mean_absolute_error: 2.214200, mean_q: 0.803580\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4680/10000: episode: 1031, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 903.480225, mean_absolute_error: 2.185680, mean_q: 0.781042\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4686/10000: episode: 1032, duration: 0.023s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1328.385376, mean_absolute_error: 3.092932, mean_q: 0.798589\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4691/10000: episode: 1033, duration: 0.021s, episode steps: 5, steps per second: 241, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.800 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 909.901184, mean_absolute_error: 2.193718, mean_q: 0.813924\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4698/10000: episode: 1034, duration: 0.026s, episode steps: 7, steps per second: 269, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1318.085205, mean_absolute_error: 3.076572, mean_q: 0.818248\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4702/10000: episode: 1035, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 670.435974, mean_absolute_error: 1.693619, mean_q: 0.836049\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4707/10000: episode: 1036, duration: 0.021s, episode steps: 5, steps per second: 244, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.600 [0.000, 2.000], loss: 1034.079712, mean_absolute_error: 2.461637, mean_q: 0.830168\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4713/10000: episode: 1037, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 913.634949, mean_absolute_error: 2.192536, mean_q: 0.839471\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4717/10000: episode: 1038, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 825.298828, mean_absolute_error: 2.020609, mean_q: 0.814610\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4721/10000: episode: 1039, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 980.480103, mean_absolute_error: 2.348913, mean_q: 0.841014\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4724/10000: episode: 1040, duration: 0.018s, episode steps: 3, steps per second: 170, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1099.961060, mean_absolute_error: 2.639047, mean_q: 0.855890\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4727/10000: episode: 1041, duration: 0.015s, episode steps: 3, steps per second: 199, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [5.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 730.182312, mean_absolute_error: 1.765453, mean_q: 0.851347\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4732/10000: episode: 1042, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 721.307251, mean_absolute_error: 1.769589, mean_q: 0.777865\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4737/10000: episode: 1043, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1066.364502, mean_absolute_error: 2.545521, mean_q: 0.765012\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4743/10000: episode: 1044, duration: 0.023s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.667 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 863.783020, mean_absolute_error: 2.098123, mean_q: 0.770045\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4745/10000: episode: 1045, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.200 [0.000, 1.000], loss: 1018.875488, mean_absolute_error: 2.424210, mean_q: 0.787638\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4750/10000: episode: 1046, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [3.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1281.393433, mean_absolute_error: 2.985085, mean_q: 0.821521\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4754/10000: episode: 1047, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [2.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 976.706421, mean_absolute_error: 2.311241, mean_q: 0.827805\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4759/10000: episode: 1048, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1161.421875, mean_absolute_error: 2.757448, mean_q: 0.826039\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4763/10000: episode: 1049, duration: 0.016s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.750 [5.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 940.781494, mean_absolute_error: 2.262420, mean_q: 0.787443\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4766/10000: episode: 1050, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1148.084595, mean_absolute_error: 2.712337, mean_q: 0.777884\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4768/10000: episode: 1051, duration: 0.010s, episode steps: 2, steps per second: 196, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 784.250793, mean_absolute_error: 1.925035, mean_q: 0.774338\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4776/10000: episode: 1052, duration: 0.033s, episode steps: 8, steps per second: 243, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.375 [0.000, 6.000], mean observation: 0.725 [0.000, 2.000], loss: 901.598145, mean_absolute_error: 2.180465, mean_q: 0.747120\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4783/10000: episode: 1053, duration: 0.028s, episode steps: 7, steps per second: 253, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.571 [0.000, 7.000], mean observation: 0.671 [0.000, 2.000], loss: 1141.236694, mean_absolute_error: 2.707085, mean_q: 0.758232\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4789/10000: episode: 1054, duration: 0.025s, episode steps: 6, steps per second: 236, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1279.297729, mean_absolute_error: 3.019763, mean_q: 0.767717\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4794/10000: episode: 1055, duration: 0.022s, episode steps: 5, steps per second: 231, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1065.259766, mean_absolute_error: 2.535365, mean_q: 0.836618\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4800/10000: episode: 1056, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1174.262451, mean_absolute_error: 2.774510, mean_q: 0.844492\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4805/10000: episode: 1057, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 753.843872, mean_absolute_error: 1.861508, mean_q: 0.865170\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4807/10000: episode: 1058, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 1175.158691, mean_absolute_error: 2.783349, mean_q: 0.846975\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4811/10000: episode: 1059, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 862.702332, mean_absolute_error: 2.097948, mean_q: 0.848315\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4815/10000: episode: 1060, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 707.310181, mean_absolute_error: 1.749581, mean_q: 0.800234\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4821/10000: episode: 1061, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1020.396545, mean_absolute_error: 2.445198, mean_q: 0.797476\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4824/10000: episode: 1062, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 885.596680, mean_absolute_error: 2.111475, mean_q: 0.777999\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4832/10000: episode: 1063, duration: 0.033s, episode steps: 8, steps per second: 240, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.625 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1056.430664, mean_absolute_error: 2.507880, mean_q: 0.778453\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4836/10000: episode: 1064, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1056.278809, mean_absolute_error: 2.508576, mean_q: 0.762990\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4840/10000: episode: 1065, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.400 [0.000, 2.000], loss: 781.473755, mean_absolute_error: 1.894651, mean_q: 0.780183\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4843/10000: episode: 1066, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1146.036377, mean_absolute_error: 2.696284, mean_q: 0.787272\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4848/10000: episode: 1067, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1160.740356, mean_absolute_error: 2.759907, mean_q: 0.797605\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4855/10000: episode: 1068, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.286 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1140.270752, mean_absolute_error: 2.696035, mean_q: 0.764277\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4860/10000: episode: 1069, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 941.473450, mean_absolute_error: 2.265935, mean_q: 0.776941\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4866/10000: episode: 1070, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 3.667 [1.000, 7.000], mean observation: 0.700 [0.000, 2.000], loss: 756.409973, mean_absolute_error: 1.828240, mean_q: 0.761401\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4869/10000: episode: 1071, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 681.398132, mean_absolute_error: 1.686374, mean_q: 0.806869\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4874/10000: episode: 1072, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 845.181519, mean_absolute_error: 2.039154, mean_q: 0.760855\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4879/10000: episode: 1073, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1253.840942, mean_absolute_error: 2.954108, mean_q: 0.786586\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4882/10000: episode: 1074, duration: 0.015s, episode steps: 3, steps per second: 205, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 939.773132, mean_absolute_error: 2.259631, mean_q: 0.772525\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4887/10000: episode: 1075, duration: 0.023s, episode steps: 5, steps per second: 221, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 753.676636, mean_absolute_error: 1.845242, mean_q: 0.743310\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4890/10000: episode: 1076, duration: 0.014s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [1.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 989.760925, mean_absolute_error: 2.346404, mean_q: 0.785801\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4895/10000: episode: 1077, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 971.538452, mean_absolute_error: 2.322277, mean_q: 0.752881\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4900/10000: episode: 1078, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 937.657227, mean_absolute_error: 2.222882, mean_q: 0.781730\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4905/10000: episode: 1079, duration: 0.021s, episode steps: 5, steps per second: 235, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1064.921997, mean_absolute_error: 2.521733, mean_q: 0.789402\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4908/10000: episode: 1080, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [3.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 943.914307, mean_absolute_error: 2.288607, mean_q: 0.796792\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4912/10000: episode: 1081, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1134.549316, mean_absolute_error: 2.671992, mean_q: 0.844526\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4914/10000: episode: 1082, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 941.158447, mean_absolute_error: 2.265278, mean_q: 0.838811\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4917/10000: episode: 1083, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 627.099365, mean_absolute_error: 1.565743, mean_q: 0.761615\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4923/10000: episode: 1084, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1045.903076, mean_absolute_error: 2.501743, mean_q: 0.799322\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4927/10000: episode: 1085, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [3.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1172.068481, mean_absolute_error: 2.752666, mean_q: 0.832421\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4932/10000: episode: 1086, duration: 0.022s, episode steps: 5, steps per second: 232, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [3.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 942.695923, mean_absolute_error: 2.285231, mean_q: 0.829612\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4938/10000: episode: 1087, duration: 0.027s, episode steps: 6, steps per second: 226, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1124.465698, mean_absolute_error: 2.678667, mean_q: 0.811742\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4943/10000: episode: 1088, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 877.588379, mean_absolute_error: 2.123073, mean_q: 0.812182\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4947/10000: episode: 1089, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 898.660339, mean_absolute_error: 2.146884, mean_q: 0.837315\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4952/10000: episode: 1090, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1001.482605, mean_absolute_error: 2.393655, mean_q: 0.834998\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4959/10000: episode: 1091, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 940.482117, mean_absolute_error: 2.267790, mean_q: 0.824754\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4963/10000: episode: 1092, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.500 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1409.436157, mean_absolute_error: 3.304167, mean_q: 0.835429\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4967/10000: episode: 1093, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 898.683472, mean_absolute_error: 2.158540, mean_q: 0.844053\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4971/10000: episode: 1094, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 939.153442, mean_absolute_error: 2.262736, mean_q: 0.788995\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4976/10000: episode: 1095, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1190.099243, mean_absolute_error: 2.819995, mean_q: 0.813819\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4981/10000: episode: 1096, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 878.749146, mean_absolute_error: 2.145067, mean_q: 0.791199\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4987/10000: episode: 1097, duration: 0.026s, episode steps: 6, steps per second: 230, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.667 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1017.881531, mean_absolute_error: 2.432948, mean_q: 0.766725\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4991/10000: episode: 1098, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [3.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 781.482056, mean_absolute_error: 1.888892, mean_q: 0.783667\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 4995/10000: episode: 1099, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.000 [0.000, 2.000], mean observation: 0.400 [0.000, 2.000], loss: 981.572815, mean_absolute_error: 2.376276, mean_q: 0.777812\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5002/10000: episode: 1100, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.286 [0.000, 5.000], mean observation: 0.757 [0.000, 2.000], loss: 920.662476, mean_absolute_error: 2.243225, mean_q: 0.802766\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5009/10000: episode: 1101, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 3.714 [0.000, 7.000], mean observation: 0.771 [0.000, 2.000], loss: 939.985291, mean_absolute_error: 2.250443, mean_q: 0.796955\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5016/10000: episode: 1102, duration: 0.026s, episode steps: 7, steps per second: 274, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1139.481689, mean_absolute_error: 2.688551, mean_q: 0.813140\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5020/10000: episode: 1103, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1022.115723, mean_absolute_error: 2.462539, mean_q: 0.800213\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5024/10000: episode: 1104, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 7.000 [5.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1058.010132, mean_absolute_error: 2.524089, mean_q: 0.761635\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5028/10000: episode: 1105, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1253.314453, mean_absolute_error: 2.958620, mean_q: 0.754188\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5032/10000: episode: 1106, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 1214.242554, mean_absolute_error: 2.865197, mean_q: 0.743261\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5035/10000: episode: 1107, duration: 0.013s, episode steps: 3, steps per second: 237, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 787.500061, mean_absolute_error: 1.955961, mean_q: 0.717964\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5041/10000: episode: 1108, duration: 0.024s, episode steps: 6, steps per second: 247, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [3.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 678.342224, mean_absolute_error: 1.668082, mean_q: 0.687934\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5043/10000: episode: 1109, duration: 0.012s, episode steps: 2, steps per second: 170, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 1331.419922, mean_absolute_error: 3.130710, mean_q: 0.695686\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5049/10000: episode: 1110, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [3.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1175.207642, mean_absolute_error: 2.793420, mean_q: 0.698016\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5053/10000: episode: 1111, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1095.353027, mean_absolute_error: 2.595425, mean_q: 0.690079\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5058/10000: episode: 1112, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 783.969360, mean_absolute_error: 1.921493, mean_q: 0.687195\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5064/10000: episode: 1113, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 939.790344, mean_absolute_error: 2.257757, mean_q: 0.682855\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5066/10000: episode: 1114, duration: 0.010s, episode steps: 2, steps per second: 205, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 1484.566162, mean_absolute_error: 3.452472, mean_q: 0.711969\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5069/10000: episode: 1115, duration: 0.013s, episode steps: 3, steps per second: 235, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 886.605774, mean_absolute_error: 2.117084, mean_q: 0.711462\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5076/10000: episode: 1116, duration: 0.025s, episode steps: 7, steps per second: 279, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.286 [1.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1098.332886, mean_absolute_error: 2.626760, mean_q: 0.729830\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5081/10000: episode: 1117, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1093.927124, mean_absolute_error: 2.576561, mean_q: 0.707406\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5085/10000: episode: 1118, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [3.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1093.969360, mean_absolute_error: 2.588784, mean_q: 0.724393\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5090/10000: episode: 1119, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1157.654663, mean_absolute_error: 2.722225, mean_q: 0.708518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5092/10000: episode: 1120, duration: 0.011s, episode steps: 2, steps per second: 175, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 1565.780273, mean_absolute_error: 3.651359, mean_q: 0.785163\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5094/10000: episode: 1121, duration: 0.013s, episode steps: 2, steps per second: 158, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 1097.062500, mean_absolute_error: 2.615589, mean_q: 0.674878\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5097/10000: episode: 1122, duration: 0.014s, episode steps: 3, steps per second: 214, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1250.194946, mean_absolute_error: 2.934172, mean_q: 0.715837\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5100/10000: episode: 1123, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 937.830261, mean_absolute_error: 2.234254, mean_q: 0.709187\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5104/10000: episode: 1124, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1022.058411, mean_absolute_error: 2.467304, mean_q: 0.787644\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5108/10000: episode: 1125, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1093.923828, mean_absolute_error: 2.578575, mean_q: 0.749788\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5112/10000: episode: 1126, duration: 0.016s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1095.502075, mean_absolute_error: 2.601700, mean_q: 0.738991\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5119/10000: episode: 1127, duration: 0.028s, episode steps: 7, steps per second: 249, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.571 [0.000, 6.000], mean observation: 0.657 [0.000, 2.000], loss: 1028.674438, mean_absolute_error: 2.450936, mean_q: 0.765779\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5124/10000: episode: 1128, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1034.974854, mean_absolute_error: 2.480324, mean_q: 0.760931\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5129/10000: episode: 1129, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1188.308350, mean_absolute_error: 2.786037, mean_q: 0.767459\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5134/10000: episode: 1130, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [2.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 782.830444, mean_absolute_error: 1.901949, mean_q: 0.815271\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5140/10000: episode: 1131, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.667 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1356.327271, mean_absolute_error: 3.178605, mean_q: 0.788258\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5142/10000: episode: 1132, duration: 0.011s, episode steps: 2, steps per second: 188, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 1331.128052, mean_absolute_error: 3.134321, mean_q: 0.819672\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5145/10000: episode: 1133, duration: 0.016s, episode steps: 3, steps per second: 187, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1046.009644, mean_absolute_error: 2.507287, mean_q: 0.816520\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5151/10000: episode: 1134, duration: 0.025s, episode steps: 6, steps per second: 240, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 965.716125, mean_absolute_error: 2.310092, mean_q: 0.781670\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5158/10000: episode: 1135, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.429 [1.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 787.707397, mean_absolute_error: 1.957214, mean_q: 0.759115\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5165/10000: episode: 1136, duration: 0.026s, episode steps: 7, steps per second: 266, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.143 [0.000, 5.000], mean observation: 0.757 [0.000, 2.000], loss: 893.044250, mean_absolute_error: 2.135221, mean_q: 0.721549\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5167/10000: episode: 1137, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 1253.329834, mean_absolute_error: 2.967412, mean_q: 0.751893\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5174/10000: episode: 1138, duration: 0.026s, episode steps: 7, steps per second: 269, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 870.719177, mean_absolute_error: 2.079426, mean_q: 0.742455\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5181/10000: episode: 1139, duration: 0.026s, episode steps: 7, steps per second: 266, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1028.619507, mean_absolute_error: 2.450187, mean_q: 0.765763\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5186/10000: episode: 1140, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1440.341187, mean_absolute_error: 3.369350, mean_q: 0.734329\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5192/10000: episode: 1141, duration: 0.023s, episode steps: 6, steps per second: 258, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.667 [3.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 913.753235, mean_absolute_error: 2.201949, mean_q: 0.712348\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5195/10000: episode: 1142, duration: 0.017s, episode steps: 3, steps per second: 174, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.929688, mean_absolute_error: 2.577838, mean_q: 0.733697\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5200/10000: episode: 1143, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 722.591187, mean_absolute_error: 1.793375, mean_q: 0.695141\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5204/10000: episode: 1144, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.400 [0.000, 2.000], loss: 940.831543, mean_absolute_error: 2.271529, mean_q: 0.688498\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5207/10000: episode: 1145, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.924683, mean_absolute_error: 2.577838, mean_q: 0.696374\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5212/10000: episode: 1146, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 971.462585, mean_absolute_error: 2.332331, mean_q: 0.722201\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5218/10000: episode: 1147, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1253.687500, mean_absolute_error: 2.957103, mean_q: 0.684864\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5224/10000: episode: 1148, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1333.497314, mean_absolute_error: 3.150882, mean_q: 0.722312\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5231/10000: episode: 1149, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 987.781311, mean_absolute_error: 2.391647, mean_q: 0.702840\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5236/10000: episode: 1150, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1001.464844, mean_absolute_error: 2.395301, mean_q: 0.667720\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5241/10000: episode: 1151, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1158.942871, mean_absolute_error: 2.746567, mean_q: 0.720942\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5244/10000: episode: 1152, duration: 0.016s, episode steps: 3, steps per second: 192, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [0.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1093.982788, mean_absolute_error: 2.591938, mean_q: 0.745913\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5247/10000: episode: 1153, duration: 0.014s, episode steps: 3, steps per second: 208, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [2.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1095.844604, mean_absolute_error: 2.606148, mean_q: 0.764101\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5255/10000: episode: 1154, duration: 0.029s, episode steps: 8, steps per second: 275, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.125 [1.000, 7.000], mean observation: 0.825 [0.000, 2.000], loss: 1075.184082, mean_absolute_error: 2.541847, mean_q: 0.744068\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5259/10000: episode: 1155, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1250.183594, mean_absolute_error: 2.921558, mean_q: 0.729349\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5265/10000: episode: 1156, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 916.740784, mean_absolute_error: 2.223947, mean_q: 0.732570\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5270/10000: episode: 1157, duration: 0.021s, episode steps: 5, steps per second: 243, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1190.073120, mean_absolute_error: 2.817873, mean_q: 0.731966\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5273/10000: episode: 1158, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1202.425049, mean_absolute_error: 2.859216, mean_q: 0.669828\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5279/10000: episode: 1159, duration: 0.023s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1070.071899, mean_absolute_error: 2.549393, mean_q: 0.724194\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5285/10000: episode: 1160, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1148.134644, mean_absolute_error: 2.712020, mean_q: 0.744906\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5288/10000: episode: 1161, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [4.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1408.304321, mean_absolute_error: 3.286906, mean_q: 0.756331\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5291/10000: episode: 1162, duration: 0.015s, episode steps: 3, steps per second: 204, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 887.749268, mean_absolute_error: 2.143534, mean_q: 0.692940\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5299/10000: episode: 1163, duration: 0.034s, episode steps: 8, steps per second: 235, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.375 [0.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 1001.936340, mean_absolute_error: 2.425852, mean_q: 0.723708\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5304/10000: episode: 1164, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 848.841980, mean_absolute_error: 2.080894, mean_q: 0.721684\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5311/10000: episode: 1165, duration: 0.026s, episode steps: 7, steps per second: 271, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.286 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1073.828491, mean_absolute_error: 2.547128, mean_q: 0.724831\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5316/10000: episode: 1166, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 843.949585, mean_absolute_error: 2.026328, mean_q: 0.719861\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5320/10000: episode: 1167, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1254.867432, mean_absolute_error: 2.976469, mean_q: 0.725352\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5325/10000: episode: 1168, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1099.062378, mean_absolute_error: 2.633598, mean_q: 0.721563\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5331/10000: episode: 1169, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1200.218140, mean_absolute_error: 2.840356, mean_q: 0.727470\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5335/10000: episode: 1170, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 941.753418, mean_absolute_error: 2.269205, mean_q: 0.707493\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5338/10000: episode: 1171, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.433 [0.000, 2.000], loss: 1358.492065, mean_absolute_error: 3.193051, mean_q: 0.700320\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5340/10000: episode: 1172, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 703.342407, mean_absolute_error: 1.716217, mean_q: 0.722286\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5346/10000: episode: 1173, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1069.961914, mean_absolute_error: 2.542927, mean_q: 0.731565\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5348/10000: episode: 1174, duration: 0.010s, episode steps: 2, steps per second: 205, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 1250.200073, mean_absolute_error: 2.936454, mean_q: 0.686031\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5352/10000: episode: 1175, duration: 0.021s, episode steps: 4, steps per second: 192, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1369.071655, mean_absolute_error: 3.205459, mean_q: 0.723216\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5358/10000: episode: 1176, duration: 0.026s, episode steps: 6, steps per second: 231, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1098.189331, mean_absolute_error: 2.621867, mean_q: 0.725268\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5361/10000: episode: 1177, duration: 0.014s, episode steps: 3, steps per second: 217, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [0.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 838.020813, mean_absolute_error: 2.048979, mean_q: 0.753495\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5366/10000: episode: 1178, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.800 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 970.323730, mean_absolute_error: 2.317159, mean_q: 0.732158\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5371/10000: episode: 1179, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 911.303894, mean_absolute_error: 2.210650, mean_q: 0.711256\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5373/10000: episode: 1180, duration: 0.010s, episode steps: 2, steps per second: 209, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 942.113770, mean_absolute_error: 2.277909, mean_q: 0.742613\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5378/10000: episode: 1181, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.400 [2.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 909.938843, mean_absolute_error: 2.199420, mean_q: 0.737723\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5381/10000: episode: 1182, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.000 [5.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1041.867065, mean_absolute_error: 2.465574, mean_q: 0.738160\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5384/10000: episode: 1183, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1458.580688, mean_absolute_error: 3.396258, mean_q: 0.781269\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5389/10000: episode: 1184, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1192.431396, mean_absolute_error: 2.831856, mean_q: 0.792570\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5393/10000: episode: 1185, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1096.912109, mean_absolute_error: 2.615150, mean_q: 0.742784\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5395/10000: episode: 1186, duration: 0.010s, episode steps: 2, steps per second: 206, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 937.822632, mean_absolute_error: 2.227447, mean_q: 0.751002\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5400/10000: episode: 1187, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 910.187805, mean_absolute_error: 2.205715, mean_q: 0.729721\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5404/10000: episode: 1188, duration: 0.019s, episode steps: 4, steps per second: 205, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [4.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 820.545349, mean_absolute_error: 1.983132, mean_q: 0.780562\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5409/10000: episode: 1189, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1191.295288, mean_absolute_error: 2.827132, mean_q: 0.784638\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5414/10000: episode: 1190, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1192.968018, mean_absolute_error: 2.833471, mean_q: 0.779217\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5417/10000: episode: 1191, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 939.766846, mean_absolute_error: 2.253239, mean_q: 0.782163\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5420/10000: episode: 1192, duration: 0.014s, episode steps: 3, steps per second: 218, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [4.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1104.682861, mean_absolute_error: 2.684376, mean_q: 0.864813\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5426/10000: episode: 1193, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1305.312134, mean_absolute_error: 3.076919, mean_q: 0.839218\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5430/10000: episode: 1194, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1173.463013, mean_absolute_error: 2.764463, mean_q: 0.787530\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5434/10000: episode: 1195, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1448.478027, mean_absolute_error: 3.391925, mean_q: 0.794351\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5438/10000: episode: 1196, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 862.876770, mean_absolute_error: 2.094157, mean_q: 0.842517\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5442/10000: episode: 1197, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1526.634399, mean_absolute_error: 3.565570, mean_q: 0.817373\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5446/10000: episode: 1198, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1370.377686, mean_absolute_error: 3.219703, mean_q: 0.829215\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5448/10000: episode: 1199, duration: 0.011s, episode steps: 2, steps per second: 189, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 1015.843262, mean_absolute_error: 2.413426, mean_q: 0.783717\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5455/10000: episode: 1200, duration: 0.032s, episode steps: 7, steps per second: 222, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.571 [0.000, 5.000], mean observation: 0.757 [0.000, 2.000], loss: 941.282532, mean_absolute_error: 2.274933, mean_q: 0.806489\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5462/10000: episode: 1201, duration: 0.027s, episode steps: 7, steps per second: 264, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.429 [1.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1143.108276, mean_absolute_error: 2.721299, mean_q: 0.812114\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5466/10000: episode: 1202, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1328.323975, mean_absolute_error: 3.100633, mean_q: 0.843443\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5469/10000: episode: 1203, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 887.701721, mean_absolute_error: 2.137136, mean_q: 0.816601\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5474/10000: episode: 1204, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.400 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1160.687500, mean_absolute_error: 2.762084, mean_q: 0.812950\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5479/10000: episode: 1205, duration: 0.024s, episode steps: 5, steps per second: 212, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1159.711182, mean_absolute_error: 2.751670, mean_q: 0.780218\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5481/10000: episode: 1206, duration: 0.011s, episode steps: 2, steps per second: 188, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.200 [0.000, 1.000], loss: 937.679810, mean_absolute_error: 2.224367, mean_q: 0.762590\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5487/10000: episode: 1207, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1176.541626, mean_absolute_error: 2.793243, mean_q: 0.787149\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5493/10000: episode: 1208, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.700 [0.000, 2.000], loss: 1043.852417, mean_absolute_error: 2.487060, mean_q: 0.754835\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5496/10000: episode: 1209, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1304.159790, mean_absolute_error: 3.063538, mean_q: 0.763766\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5499/10000: episode: 1210, duration: 0.014s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.971802, mean_absolute_error: 2.586701, mean_q: 0.751647\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5502/10000: episode: 1211, duration: 0.014s, episode steps: 3, steps per second: 212, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 838.061584, mean_absolute_error: 2.048328, mean_q: 0.747532\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5506/10000: episode: 1212, duration: 0.019s, episode steps: 4, steps per second: 206, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 822.125977, mean_absolute_error: 2.003298, mean_q: 0.788162\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5511/10000: episode: 1213, duration: 0.021s, episode steps: 5, steps per second: 235, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [3.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1157.792725, mean_absolute_error: 2.726560, mean_q: 0.841685\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5519/10000: episode: 1214, duration: 0.029s, episode steps: 8, steps per second: 275, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.250 [1.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 1057.911865, mean_absolute_error: 2.529114, mean_q: 0.807118\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5521/10000: episode: 1215, duration: 0.010s, episode steps: 2, steps per second: 199, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 1172.071899, mean_absolute_error: 2.743550, mean_q: 0.811369\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5524/10000: episode: 1216, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 989.931702, mean_absolute_error: 2.354500, mean_q: 0.757447\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5529/10000: episode: 1217, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.200 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 846.359375, mean_absolute_error: 2.056417, mean_q: 0.775678\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5536/10000: episode: 1218, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.714 [0.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 984.056824, mean_absolute_error: 2.355954, mean_q: 0.769856\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5538/10000: episode: 1219, duration: 0.010s, episode steps: 2, steps per second: 205, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 862.714294, mean_absolute_error: 2.098874, mean_q: 0.794838\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5540/10000: episode: 1220, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 937.734131, mean_absolute_error: 2.238860, mean_q: 0.795156\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5544/10000: episode: 1221, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1017.237671, mean_absolute_error: 2.422429, mean_q: 0.812351\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5548/10000: episode: 1222, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1025.512695, mean_absolute_error: 2.509552, mean_q: 0.800931\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5554/10000: episode: 1223, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.500 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 1200.628784, mean_absolute_error: 2.835022, mean_q: 0.810882\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5558/10000: episode: 1224, duration: 0.020s, episode steps: 4, steps per second: 200, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1253.326172, mean_absolute_error: 2.960489, mean_q: 0.825598\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5564/10000: episode: 1225, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1204.203247, mean_absolute_error: 2.876093, mean_q: 0.814902\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5571/10000: episode: 1226, duration: 0.026s, episode steps: 7, steps per second: 266, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 3.286 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1141.176147, mean_absolute_error: 2.706161, mean_q: 0.776393\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5576/10000: episode: 1227, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1221.447021, mean_absolute_error: 2.886400, mean_q: 0.777486\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5580/10000: episode: 1228, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 939.291870, mean_absolute_error: 2.239061, mean_q: 0.741709\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5585/10000: episode: 1229, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1095.190674, mean_absolute_error: 2.587962, mean_q: 0.775715\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5588/10000: episode: 1230, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [3.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 946.049011, mean_absolute_error: 2.324057, mean_q: 0.746285\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5590/10000: episode: 1231, duration: 0.010s, episode steps: 2, steps per second: 194, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 937.717651, mean_absolute_error: 2.240442, mean_q: 0.751670\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5594/10000: episode: 1232, duration: 0.019s, episode steps: 4, steps per second: 210, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1095.532715, mean_absolute_error: 2.602345, mean_q: 0.774197\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5600/10000: episode: 1233, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1095.118774, mean_absolute_error: 2.598964, mean_q: 0.767010\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5605/10000: episode: 1234, duration: 0.022s, episode steps: 5, steps per second: 225, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1190.066162, mean_absolute_error: 2.814322, mean_q: 0.764931\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5608/10000: episode: 1235, duration: 0.016s, episode steps: 3, steps per second: 189, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1097.900024, mean_absolute_error: 2.615969, mean_q: 0.781583\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5612/10000: episode: 1236, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1251.608765, mean_absolute_error: 2.944859, mean_q: 0.789193\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5616/10000: episode: 1237, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 823.672241, mean_absolute_error: 2.015521, mean_q: 0.781442\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5618/10000: episode: 1238, duration: 0.010s, episode steps: 2, steps per second: 193, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 937.758545, mean_absolute_error: 2.242838, mean_q: 0.811285\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5622/10000: episode: 1239, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1211.145020, mean_absolute_error: 2.843733, mean_q: 0.796360\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5627/10000: episode: 1240, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1063.155518, mean_absolute_error: 2.515522, mean_q: 0.820837\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5632/10000: episode: 1241, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1063.920654, mean_absolute_error: 2.529896, mean_q: 0.817998\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5637/10000: episode: 1242, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [3.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1128.832520, mean_absolute_error: 2.692020, mean_q: 0.834057\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5640/10000: episode: 1243, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: 840.075378, mean_absolute_error: 2.064625, mean_q: 0.773490\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5644/10000: episode: 1244, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [4.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1256.418945, mean_absolute_error: 2.981185, mean_q: 0.821332\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5648/10000: episode: 1245, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [3.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1139.422363, mean_absolute_error: 2.725728, mean_q: 0.812840\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5654/10000: episode: 1246, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 968.945801, mean_absolute_error: 2.349816, mean_q: 0.800276\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5663/10000: episode: 1247, duration: 0.037s, episode steps: 9, steps per second: 245, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.822 [0.000, 2.000], loss: 957.158630, mean_absolute_error: 2.295026, mean_q: 0.751877\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5667/10000: episode: 1248, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1136.280273, mean_absolute_error: 2.700428, mean_q: 0.771833\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5671/10000: episode: 1249, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1057.868774, mean_absolute_error: 2.524976, mean_q: 0.791126\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5675/10000: episode: 1250, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 822.903748, mean_absolute_error: 1.998017, mean_q: 0.774971\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5680/10000: episode: 1251, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 878.818848, mean_absolute_error: 2.129412, mean_q: 0.757389\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5687/10000: episode: 1252, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [1.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1005.569641, mean_absolute_error: 2.400263, mean_q: 0.732998\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5694/10000: episode: 1253, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 871.620728, mean_absolute_error: 2.094021, mean_q: 0.726466\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5699/10000: episode: 1254, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1131.204956, mean_absolute_error: 2.708285, mean_q: 0.745993\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5701/10000: episode: 1255, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 1335.410767, mean_absolute_error: 3.158579, mean_q: 0.805686\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5707/10000: episode: 1256, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1043.035156, mean_absolute_error: 2.482833, mean_q: 0.787376\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5710/10000: episode: 1257, duration: 0.015s, episode steps: 3, steps per second: 200, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1304.584839, mean_absolute_error: 3.068505, mean_q: 0.792649\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5717/10000: episode: 1258, duration: 0.026s, episode steps: 7, steps per second: 266, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1186.029419, mean_absolute_error: 2.806405, mean_q: 0.831561\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5723/10000: episode: 1259, duration: 0.022s, episode steps: 6, steps per second: 270, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.167 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 966.183777, mean_absolute_error: 2.318551, mean_q: 0.899237\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5730/10000: episode: 1260, duration: 0.026s, episode steps: 7, steps per second: 274, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.143 [0.000, 7.000], mean observation: 0.671 [0.000, 2.000], loss: 1053.182983, mean_absolute_error: 2.526079, mean_q: 0.881886\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5737/10000: episode: 1261, duration: 0.026s, episode steps: 7, steps per second: 273, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 939.614258, mean_absolute_error: 2.263894, mean_q: 0.858926\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5743/10000: episode: 1262, duration: 0.022s, episode steps: 6, steps per second: 272, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 941.999817, mean_absolute_error: 2.283052, mean_q: 0.788673\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5749/10000: episode: 1263, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.167 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1228.502075, mean_absolute_error: 2.897475, mean_q: 0.798812\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5753/10000: episode: 1264, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1214.285278, mean_absolute_error: 2.868414, mean_q: 0.815161\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5760/10000: episode: 1265, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.286 [2.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1072.457764, mean_absolute_error: 2.543957, mean_q: 0.772860\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5766/10000: episode: 1266, duration: 0.027s, episode steps: 6, steps per second: 220, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 815.402344, mean_absolute_error: 2.029121, mean_q: 0.752846\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5771/10000: episode: 1267, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [2.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1065.191528, mean_absolute_error: 2.535944, mean_q: 0.766952\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5775/10000: episode: 1268, duration: 0.016s, episode steps: 4, steps per second: 252, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 665.814758, mean_absolute_error: 1.639161, mean_q: 0.754468\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5778/10000: episode: 1269, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1147.074097, mean_absolute_error: 2.695610, mean_q: 0.788603\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5784/10000: episode: 1270, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1073.486084, mean_absolute_error: 2.575414, mean_q: 0.770438\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5788/10000: episode: 1271, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1253.286133, mean_absolute_error: 2.952600, mean_q: 0.761449\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5791/10000: episode: 1272, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 939.590637, mean_absolute_error: 2.258005, mean_q: 0.760321\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5796/10000: episode: 1273, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 972.683228, mean_absolute_error: 2.337639, mean_q: 0.756690\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5801/10000: episode: 1274, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 971.819641, mean_absolute_error: 2.326364, mean_q: 0.732094\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5806/10000: episode: 1275, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 815.322632, mean_absolute_error: 1.982993, mean_q: 0.766669\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5811/10000: episode: 1276, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 876.569702, mean_absolute_error: 2.107953, mean_q: 0.717255\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5818/10000: episode: 1277, duration: 0.027s, episode steps: 7, steps per second: 259, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.857 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 785.693787, mean_absolute_error: 1.937309, mean_q: 0.767300\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5823/10000: episode: 1278, duration: 0.023s, episode steps: 5, steps per second: 215, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 942.451843, mean_absolute_error: 2.283308, mean_q: 0.757421\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5825/10000: episode: 1279, duration: 0.010s, episode steps: 2, steps per second: 195, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 1.000], loss: 1018.949158, mean_absolute_error: 2.442088, mean_q: 0.777411\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5830/10000: episode: 1280, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1130.090698, mean_absolute_error: 2.712145, mean_q: 0.763255\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5833/10000: episode: 1281, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1200.199341, mean_absolute_error: 2.836928, mean_q: 0.775023\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5839/10000: episode: 1282, duration: 0.026s, episode steps: 6, steps per second: 234, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1147.063354, mean_absolute_error: 2.706840, mean_q: 0.746569\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5841/10000: episode: 1283, duration: 0.010s, episode steps: 2, steps per second: 207, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 869.256104, mean_absolute_error: 2.151451, mean_q: 0.771411\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5846/10000: episode: 1284, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1159.945068, mean_absolute_error: 2.755462, mean_q: 0.739593\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5855/10000: episode: 1285, duration: 0.032s, episode steps: 9, steps per second: 279, episode reward: -100.000, mean reward: -11.111 [-100.000, 0.000], mean action: 3.556 [0.000, 8.000], mean observation: 0.911 [0.000, 2.000], loss: 939.815552, mean_absolute_error: 2.264904, mean_q: 0.769781\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5859/10000: episode: 1286, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1059.573730, mean_absolute_error: 2.534844, mean_q: 0.757238\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5867/10000: episode: 1287, duration: 0.029s, episode steps: 8, steps per second: 275, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 1251.742188, mean_absolute_error: 2.930684, mean_q: 0.774494\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5869/10000: episode: 1288, duration: 0.011s, episode steps: 2, steps per second: 186, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 782.258362, mean_absolute_error: 1.880805, mean_q: 0.830765\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5871/10000: episode: 1289, duration: 0.012s, episode steps: 2, steps per second: 161, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1175.928711, mean_absolute_error: 2.774183, mean_q: 0.809195\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5874/10000: episode: 1290, duration: 0.014s, episode steps: 3, steps per second: 209, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 737.498230, mean_absolute_error: 1.863998, mean_q: 0.724728\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5878/10000: episode: 1291, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 865.220520, mean_absolute_error: 2.117249, mean_q: 0.707188\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5884/10000: episode: 1292, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.500 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 1174.122681, mean_absolute_error: 2.772376, mean_q: 0.730704\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5888/10000: episode: 1293, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 781.446777, mean_absolute_error: 1.885312, mean_q: 0.756759\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5893/10000: episode: 1294, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1064.439697, mean_absolute_error: 2.517044, mean_q: 0.760635\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5896/10000: episode: 1295, duration: 0.013s, episode steps: 3, steps per second: 239, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 683.345215, mean_absolute_error: 1.724445, mean_q: 0.800896\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5899/10000: episode: 1296, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 943.949402, mean_absolute_error: 2.299444, mean_q: 0.768060\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5901/10000: episode: 1297, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 946.771179, mean_absolute_error: 2.334999, mean_q: 0.797765\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5904/10000: episode: 1298, duration: 0.013s, episode steps: 3, steps per second: 235, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [2.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1253.924927, mean_absolute_error: 2.953626, mean_q: 0.802531\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5910/10000: episode: 1299, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1124.145142, mean_absolute_error: 2.681199, mean_q: 0.732993\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5912/10000: episode: 1300, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 862.723511, mean_absolute_error: 2.096447, mean_q: 0.777661\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5918/10000: episode: 1301, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 965.744873, mean_absolute_error: 2.317026, mean_q: 0.783354\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5922/10000: episode: 1302, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1137.085449, mean_absolute_error: 2.701624, mean_q: 0.773514\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5928/10000: episode: 1303, duration: 0.024s, episode steps: 6, steps per second: 246, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 1072.912964, mean_absolute_error: 2.567299, mean_q: 0.801932\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5935/10000: episode: 1304, duration: 0.026s, episode steps: 7, steps per second: 271, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.571 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1118.967651, mean_absolute_error: 2.662126, mean_q: 0.819896\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5938/10000: episode: 1305, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [3.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 994.067383, mean_absolute_error: 2.395844, mean_q: 0.820637\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5945/10000: episode: 1306, duration: 0.026s, episode steps: 7, steps per second: 273, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1142.946411, mean_absolute_error: 2.720446, mean_q: 0.803436\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5950/10000: episode: 1307, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1126.953857, mean_absolute_error: 2.660814, mean_q: 0.798582\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5953/10000: episode: 1308, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1096.056396, mean_absolute_error: 2.605839, mean_q: 0.809322\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5959/10000: episode: 1309, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 940.749329, mean_absolute_error: 2.271648, mean_q: 0.787397\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5965/10000: episode: 1310, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 915.738464, mean_absolute_error: 2.220817, mean_q: 0.751623\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5972/10000: episode: 1311, duration: 0.029s, episode steps: 7, steps per second: 242, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.671 [0.000, 2.000], loss: 1142.085205, mean_absolute_error: 2.718178, mean_q: 0.751317\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5974/10000: episode: 1312, duration: 0.012s, episode steps: 2, steps per second: 169, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 1328.351074, mean_absolute_error: 3.106754, mean_q: 0.810226\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5980/10000: episode: 1313, duration: 0.025s, episode steps: 6, steps per second: 244, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1175.643799, mean_absolute_error: 2.783525, mean_q: 0.772350\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5985/10000: episode: 1314, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 1.800 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 941.485046, mean_absolute_error: 2.283814, mean_q: 0.722098\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5990/10000: episode: 1315, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1098.702026, mean_absolute_error: 2.628368, mean_q: 0.715515\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5996/10000: episode: 1316, duration: 0.023s, episode steps: 6, steps per second: 258, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1147.220947, mean_absolute_error: 2.710970, mean_q: 0.735081\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 5998/10000: episode: 1317, duration: 0.010s, episode steps: 2, steps per second: 193, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 940.833740, mean_absolute_error: 2.269039, mean_q: 0.745900\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6003/10000: episode: 1318, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1127.586548, mean_absolute_error: 2.679638, mean_q: 0.746325\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6011/10000: episode: 1319, duration: 0.030s, episode steps: 8, steps per second: 268, episode reward: 20.000, mean reward: 2.500 [0.000, 20.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.850 [0.000, 2.000], loss: 962.465454, mean_absolute_error: 2.332109, mean_q: 0.816466\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6014/10000: episode: 1320, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.333 [0.000, 2.000], loss: 1458.473755, mean_absolute_error: 3.378363, mean_q: 0.823034\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6018/10000: episode: 1321, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1020.480713, mean_absolute_error: 2.448551, mean_q: 0.840302\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6022/10000: episode: 1322, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 743.797607, mean_absolute_error: 1.809882, mean_q: 0.832014\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6027/10000: episode: 1323, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1220.311646, mean_absolute_error: 2.864486, mean_q: 0.837028\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6032/10000: episode: 1324, duration: 0.020s, episode steps: 5, steps per second: 244, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1252.550781, mean_absolute_error: 2.948682, mean_q: 0.849537\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6035/10000: episode: 1325, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 937.731750, mean_absolute_error: 2.241289, mean_q: 0.813355\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6040/10000: episode: 1326, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1188.321777, mean_absolute_error: 2.785972, mean_q: 0.839938\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6044/10000: episode: 1327, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 978.272949, mean_absolute_error: 2.322202, mean_q: 0.828006\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6048/10000: episode: 1328, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 824.963989, mean_absolute_error: 2.021261, mean_q: 0.816924\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6053/10000: episode: 1329, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 5.400 [2.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 848.700073, mean_absolute_error: 2.075701, mean_q: 0.798907\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6057/10000: episode: 1330, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1015.861938, mean_absolute_error: 2.419337, mean_q: 0.775881\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6064/10000: episode: 1331, duration: 0.027s, episode steps: 7, steps per second: 264, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.857 [0.000, 7.000], mean observation: 0.671 [0.000, 2.000], loss: 963.673950, mean_absolute_error: 2.319450, mean_q: 0.729976\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6066/10000: episode: 1332, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 937.693970, mean_absolute_error: 2.235837, mean_q: 0.719740\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6070/10000: episode: 1333, duration: 0.017s, episode steps: 4, steps per second: 229, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 979.092651, mean_absolute_error: 2.342315, mean_q: 0.713076\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6075/10000: episode: 1334, duration: 0.024s, episode steps: 5, steps per second: 207, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 720.219482, mean_absolute_error: 1.760229, mean_q: 0.738667\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6077/10000: episode: 1335, duration: 0.011s, episode steps: 2, steps per second: 175, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 869.231018, mean_absolute_error: 2.161629, mean_q: 0.736871\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6081/10000: episode: 1336, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1056.442139, mean_absolute_error: 2.520826, mean_q: 0.752306\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6085/10000: episode: 1337, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 937.708923, mean_absolute_error: 2.233560, mean_q: 0.754918\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6088/10000: episode: 1338, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 885.647156, mean_absolute_error: 2.125769, mean_q: 0.771770\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6092/10000: episode: 1339, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 824.162537, mean_absolute_error: 2.023781, mean_q: 0.743973\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6101/10000: episode: 1340, duration: 0.035s, episode steps: 9, steps per second: 258, episode reward: -100.000, mean reward: -11.111 [-100.000, 0.000], mean action: 4.444 [1.000, 8.000], mean observation: 0.811 [0.000, 2.000], loss: 1271.374390, mean_absolute_error: 3.001393, mean_q: 0.844589\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6104/10000: episode: 1341, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 835.616150, mean_absolute_error: 2.024778, mean_q: 0.872279\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6108/10000: episode: 1342, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1097.069092, mean_absolute_error: 2.613370, mean_q: 0.877057\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6113/10000: episode: 1343, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1220.728149, mean_absolute_error: 2.865628, mean_q: 0.864505\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6119/10000: episode: 1344, duration: 0.025s, episode steps: 6, steps per second: 237, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 1149.054810, mean_absolute_error: 2.721313, mean_q: 0.895108\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6124/10000: episode: 1345, duration: 0.024s, episode steps: 5, steps per second: 208, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.800 [1.000, 7.000], mean observation: 0.520 [0.000, 2.000], loss: 938.947876, mean_absolute_error: 2.247616, mean_q: 0.912789\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6129/10000: episode: 1346, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 971.319946, mean_absolute_error: 2.331394, mean_q: 0.926364\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6133/10000: episode: 1347, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [3.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 898.638489, mean_absolute_error: 2.142988, mean_q: 0.903002\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6140/10000: episode: 1348, duration: 0.027s, episode steps: 7, steps per second: 263, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.714 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1074.879883, mean_absolute_error: 2.565516, mean_q: 0.918711\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6146/10000: episode: 1349, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [1.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 939.292480, mean_absolute_error: 2.245922, mean_q: 0.900271\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6148/10000: episode: 1350, duration: 0.010s, episode steps: 2, steps per second: 194, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.300 [0.000, 2.000], loss: 784.327148, mean_absolute_error: 1.931313, mean_q: 0.897973\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6155/10000: episode: 1351, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 962.294739, mean_absolute_error: 2.314381, mean_q: 0.885019\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6159/10000: episode: 1352, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 976.770874, mean_absolute_error: 2.317750, mean_q: 0.856916\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6165/10000: episode: 1353, duration: 0.023s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 913.752136, mean_absolute_error: 2.192802, mean_q: 0.839400\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6170/10000: episode: 1354, duration: 0.021s, episode steps: 5, steps per second: 239, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1065.095703, mean_absolute_error: 2.540352, mean_q: 0.809880\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6173/10000: episode: 1355, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 995.023376, mean_absolute_error: 2.408061, mean_q: 0.792992\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6181/10000: episode: 1356, duration: 0.031s, episode steps: 8, steps per second: 262, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.875 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 824.379150, mean_absolute_error: 2.023344, mean_q: 0.733137\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6187/10000: episode: 1357, duration: 0.024s, episode steps: 6, steps per second: 252, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1020.839417, mean_absolute_error: 2.459214, mean_q: 0.742346\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6193/10000: episode: 1358, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 994.737122, mean_absolute_error: 2.395066, mean_q: 0.797136\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6197/10000: episode: 1359, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 939.916382, mean_absolute_error: 2.253489, mean_q: 0.820355\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6203/10000: episode: 1360, duration: 0.023s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 992.957031, mean_absolute_error: 2.388837, mean_q: 0.812534\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6206/10000: episode: 1361, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [4.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 945.663391, mean_absolute_error: 2.325915, mean_q: 0.823097\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6213/10000: episode: 1362, duration: 0.027s, episode steps: 7, steps per second: 263, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.857 [1.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1028.887817, mean_absolute_error: 2.458050, mean_q: 0.793667\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6218/10000: episode: 1363, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1096.486572, mean_absolute_error: 2.604976, mean_q: 0.820285\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6222/10000: episode: 1364, duration: 0.021s, episode steps: 4, steps per second: 190, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 820.538330, mean_absolute_error: 1.979065, mean_q: 0.764056\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6225/10000: episode: 1365, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.984009, mean_absolute_error: 2.588312, mean_q: 0.782599\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6228/10000: episode: 1366, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [3.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1096.083374, mean_absolute_error: 2.615087, mean_q: 0.816165\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6234/10000: episode: 1367, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 628.367737, mean_absolute_error: 1.578263, mean_q: 0.769527\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6239/10000: episode: 1368, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1096.565918, mean_absolute_error: 2.602520, mean_q: 0.775050\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6245/10000: episode: 1369, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1227.386108, mean_absolute_error: 2.903827, mean_q: 0.771353\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6248/10000: episode: 1370, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1252.060913, mean_absolute_error: 2.945863, mean_q: 0.785091\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6251/10000: episode: 1371, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [1.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 1044.815796, mean_absolute_error: 2.496843, mean_q: 0.747946\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6257/10000: episode: 1372, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 940.280090, mean_absolute_error: 2.248604, mean_q: 0.747982\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6264/10000: episode: 1373, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.286 [1.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 986.287964, mean_absolute_error: 2.365702, mean_q: 0.801951\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6270/10000: episode: 1374, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 862.919250, mean_absolute_error: 2.094486, mean_q: 0.737808\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6277/10000: episode: 1375, duration: 0.031s, episode steps: 7, steps per second: 227, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.286 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1208.588989, mean_absolute_error: 2.855523, mean_q: 0.768393\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6283/10000: episode: 1376, duration: 0.025s, episode steps: 6, steps per second: 243, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1099.587891, mean_absolute_error: 2.639632, mean_q: 0.782953\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6287/10000: episode: 1377, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [3.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 668.905579, mean_absolute_error: 1.685902, mean_q: 0.729865\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6295/10000: episode: 1378, duration: 0.031s, episode steps: 8, steps per second: 258, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.625 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 1074.822388, mean_absolute_error: 2.534420, mean_q: 0.750781\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6299/10000: episode: 1379, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 746.957214, mean_absolute_error: 1.860668, mean_q: 0.760461\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6303/10000: episode: 1380, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 1254.574829, mean_absolute_error: 2.975611, mean_q: 0.760352\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6306/10000: episode: 1381, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 993.727051, mean_absolute_error: 2.388851, mean_q: 0.737267\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6311/10000: episode: 1382, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.400 [4.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1064.708008, mean_absolute_error: 2.526868, mean_q: 0.753467\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6315/10000: episode: 1383, duration: 0.018s, episode steps: 4, steps per second: 222, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 901.030640, mean_absolute_error: 2.159273, mean_q: 0.771422\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6318/10000: episode: 1384, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1154.658813, mean_absolute_error: 2.783007, mean_q: 0.793175\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6323/10000: episode: 1385, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1282.716064, mean_absolute_error: 3.004265, mean_q: 0.799051\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6328/10000: episode: 1386, duration: 0.021s, episode steps: 5, steps per second: 240, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1002.611450, mean_absolute_error: 2.393652, mean_q: 0.764484\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6331/10000: episode: 1387, duration: 0.017s, episode steps: 3, steps per second: 178, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [4.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1042.908325, mean_absolute_error: 2.461837, mean_q: 0.806987\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6333/10000: episode: 1388, duration: 0.011s, episode steps: 2, steps per second: 179, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 937.756165, mean_absolute_error: 2.248800, mean_q: 0.841928\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6337/10000: episode: 1389, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.250 [5.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1017.258850, mean_absolute_error: 2.420708, mean_q: 0.863304\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6342/10000: episode: 1390, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1158.943115, mean_absolute_error: 2.748648, mean_q: 0.837544\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6346/10000: episode: 1391, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1019.678711, mean_absolute_error: 2.428263, mean_q: 0.805393\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6351/10000: episode: 1392, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1408.834595, mean_absolute_error: 3.292767, mean_q: 0.846657\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6357/10000: episode: 1393, duration: 0.022s, episode steps: 6, steps per second: 271, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1042.821899, mean_absolute_error: 2.476059, mean_q: 0.824291\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6361/10000: episode: 1394, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 901.818848, mean_absolute_error: 2.176571, mean_q: 0.835936\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6364/10000: episode: 1395, duration: 0.012s, episode steps: 3, steps per second: 241, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1566.472778, mean_absolute_error: 3.657633, mean_q: 0.839343\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6368/10000: episode: 1396, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1097.102051, mean_absolute_error: 2.624158, mean_q: 0.772979\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6372/10000: episode: 1397, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1018.152405, mean_absolute_error: 2.425212, mean_q: 0.790018\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6377/10000: episode: 1398, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 940.198120, mean_absolute_error: 2.259059, mean_q: 0.772820\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6382/10000: episode: 1399, duration: 0.024s, episode steps: 5, steps per second: 211, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 938.933594, mean_absolute_error: 2.240640, mean_q: 0.779365\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6387/10000: episode: 1400, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1127.564453, mean_absolute_error: 2.673341, mean_q: 0.791381\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6395/10000: episode: 1401, duration: 0.029s, episode steps: 8, steps per second: 278, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 922.065918, mean_absolute_error: 2.231802, mean_q: 0.772856\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6398/10000: episode: 1402, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1202.064331, mean_absolute_error: 2.852965, mean_q: 0.775579\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6403/10000: episode: 1403, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1032.078613, mean_absolute_error: 2.444772, mean_q: 0.772535\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6411/10000: episode: 1404, duration: 0.029s, episode steps: 8, steps per second: 278, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.875 [2.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1058.029907, mean_absolute_error: 2.530699, mean_q: 0.766032\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6416/10000: episode: 1405, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 942.820618, mean_absolute_error: 2.289587, mean_q: 0.797858\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6419/10000: episode: 1406, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 781.463196, mean_absolute_error: 1.884414, mean_q: 0.775869\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6426/10000: episode: 1407, duration: 0.026s, episode steps: 7, steps per second: 269, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.429 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1185.044922, mean_absolute_error: 2.801533, mean_q: 0.789464\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6430/10000: episode: 1408, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1018.967712, mean_absolute_error: 2.439767, mean_q: 0.776338\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6434/10000: episode: 1409, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 943.646423, mean_absolute_error: 2.298471, mean_q: 0.745720\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6440/10000: episode: 1410, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.833 [3.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 942.008301, mean_absolute_error: 2.271348, mean_q: 0.764187\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6444/10000: episode: 1411, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [3.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 946.211731, mean_absolute_error: 2.320637, mean_q: 0.726221\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6449/10000: episode: 1412, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 1.800 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1192.615112, mean_absolute_error: 2.841088, mean_q: 0.733745\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6454/10000: episode: 1413, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 879.623352, mean_absolute_error: 2.144121, mean_q: 0.782718\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6457/10000: episode: 1414, duration: 0.014s, episode steps: 3, steps per second: 219, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [3.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 1202.487183, mean_absolute_error: 2.857103, mean_q: 0.772655\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6461/10000: episode: 1415, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 703.929565, mean_absolute_error: 1.719232, mean_q: 0.759381\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6467/10000: episode: 1416, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 6.167 [4.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1123.019165, mean_absolute_error: 2.672350, mean_q: 0.807161\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6469/10000: episode: 1417, duration: 0.010s, episode steps: 2, steps per second: 205, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 1172.041260, mean_absolute_error: 2.739939, mean_q: 0.794510\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6475/10000: episode: 1418, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.500 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1226.148804, mean_absolute_error: 2.894977, mean_q: 0.822863\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6480/10000: episode: 1419, duration: 0.021s, episode steps: 5, steps per second: 240, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1096.317139, mean_absolute_error: 2.601699, mean_q: 0.817603\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6483/10000: episode: 1420, duration: 0.015s, episode steps: 3, steps per second: 195, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [2.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 885.603516, mean_absolute_error: 2.109799, mean_q: 0.822008\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6485/10000: episode: 1421, duration: 0.012s, episode steps: 2, steps per second: 173, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1096.898682, mean_absolute_error: 2.609360, mean_q: 0.817061\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6491/10000: episode: 1422, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1019.980774, mean_absolute_error: 2.451051, mean_q: 0.839019\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6496/10000: episode: 1423, duration: 0.019s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1192.563354, mean_absolute_error: 2.831465, mean_q: 0.850155\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6502/10000: episode: 1424, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1074.249390, mean_absolute_error: 2.585797, mean_q: 0.863099\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6507/10000: episode: 1425, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.520 [0.000, 2.000], loss: 1250.669556, mean_absolute_error: 2.927528, mean_q: 0.855482\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6513/10000: episode: 1426, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 3.333 [0.000, 8.000], mean observation: 0.700 [0.000, 2.000], loss: 990.801331, mean_absolute_error: 2.355210, mean_q: 0.834076\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6517/10000: episode: 1427, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1172.058960, mean_absolute_error: 2.752812, mean_q: 0.815055\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6520/10000: episode: 1428, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 939.773682, mean_absolute_error: 2.253965, mean_q: 0.815319\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6527/10000: episode: 1429, duration: 0.027s, episode steps: 7, steps per second: 260, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [1.000, 6.000], mean observation: 0.657 [0.000, 2.000], loss: 967.105347, mean_absolute_error: 2.365745, mean_q: 0.790421\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6530/10000: episode: 1430, duration: 0.014s, episode steps: 3, steps per second: 207, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 843.842773, mean_absolute_error: 2.111756, mean_q: 0.800082\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6534/10000: episode: 1431, duration: 0.020s, episode steps: 4, steps per second: 201, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 900.231079, mean_absolute_error: 2.156874, mean_q: 0.776201\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6536/10000: episode: 1432, duration: 0.010s, episode steps: 2, steps per second: 194, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 940.820679, mean_absolute_error: 2.274615, mean_q: 0.784896\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6541/10000: episode: 1433, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1157.988525, mean_absolute_error: 2.736539, mean_q: 0.795442\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6547/10000: episode: 1434, duration: 0.025s, episode steps: 6, steps per second: 241, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 862.723145, mean_absolute_error: 2.099492, mean_q: 0.830874\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6549/10000: episode: 1435, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 625.236328, mean_absolute_error: 1.544723, mean_q: 0.836170\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6557/10000: episode: 1436, duration: 0.030s, episode steps: 8, steps per second: 263, episode reward: -20.000, mean reward: -2.500 [-20.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.750 [0.000, 2.000], loss: 1194.666016, mean_absolute_error: 2.826656, mean_q: 0.854617\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6559/10000: episode: 1437, duration: 0.010s, episode steps: 2, steps per second: 206, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 782.064636, mean_absolute_error: 1.897687, mean_q: 0.810977\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6563/10000: episode: 1438, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 904.441162, mean_absolute_error: 2.209939, mean_q: 0.787672\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6568/10000: episode: 1439, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 940.347656, mean_absolute_error: 2.253741, mean_q: 0.790250\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6571/10000: episode: 1440, duration: 0.014s, episode steps: 3, steps per second: 219, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1357.661743, mean_absolute_error: 3.175557, mean_q: 0.806347\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6574/10000: episode: 1441, duration: 0.014s, episode steps: 3, steps per second: 215, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1568.244507, mean_absolute_error: 3.655577, mean_q: 0.847842\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6578/10000: episode: 1442, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 982.996033, mean_absolute_error: 2.372230, mean_q: 0.818140\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6583/10000: episode: 1443, duration: 0.023s, episode steps: 5, steps per second: 213, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1314.396606, mean_absolute_error: 3.069200, mean_q: 0.822585\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6590/10000: episode: 1444, duration: 0.027s, episode steps: 7, steps per second: 255, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.571 [1.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 851.873535, mean_absolute_error: 2.067789, mean_q: 0.818294\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6596/10000: episode: 1445, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 966.865173, mean_absolute_error: 2.325176, mean_q: 0.791908\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6602/10000: episode: 1446, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1121.977173, mean_absolute_error: 2.657468, mean_q: 0.797308\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6609/10000: episode: 1447, duration: 0.026s, episode steps: 7, steps per second: 271, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1051.520996, mean_absolute_error: 2.495754, mean_q: 0.813020\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6615/10000: episode: 1448, duration: 0.023s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.833 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 966.456055, mean_absolute_error: 2.321720, mean_q: 0.787660\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6621/10000: episode: 1449, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1173.012085, mean_absolute_error: 2.763487, mean_q: 0.823264\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6623/10000: episode: 1450, duration: 0.010s, episode steps: 2, steps per second: 205, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 547.141968, mean_absolute_error: 1.375016, mean_q: 0.831858\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6625/10000: episode: 1451, duration: 0.010s, episode steps: 2, steps per second: 209, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 863.063477, mean_absolute_error: 2.109602, mean_q: 0.831756\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6630/10000: episode: 1452, duration: 0.021s, episode steps: 5, steps per second: 235, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [1.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 976.360535, mean_absolute_error: 2.392319, mean_q: 0.848084\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6634/10000: episode: 1453, duration: 0.021s, episode steps: 4, steps per second: 194, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.500 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 901.507568, mean_absolute_error: 2.193163, mean_q: 0.823559\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6640/10000: episode: 1454, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 914.451477, mean_absolute_error: 2.197338, mean_q: 0.826203\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6646/10000: episode: 1455, duration: 0.022s, episode steps: 6, steps per second: 271, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1017.000549, mean_absolute_error: 2.427532, mean_q: 0.811674\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6651/10000: episode: 1456, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1065.820923, mean_absolute_error: 2.537743, mean_q: 0.765689\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6659/10000: episode: 1457, duration: 0.029s, episode steps: 8, steps per second: 280, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.250 [1.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 978.343201, mean_absolute_error: 2.343862, mean_q: 0.783985\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6663/10000: episode: 1458, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [2.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1173.670654, mean_absolute_error: 2.781760, mean_q: 0.815950\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6669/10000: episode: 1459, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [2.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 1173.027100, mean_absolute_error: 2.769275, mean_q: 0.788428\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6673/10000: episode: 1460, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1096.322510, mean_absolute_error: 2.600428, mean_q: 0.823552\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6676/10000: episode: 1461, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 887.916748, mean_absolute_error: 2.140961, mean_q: 0.797714\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6683/10000: episode: 1462, duration: 0.026s, episode steps: 7, steps per second: 271, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1118.062500, mean_absolute_error: 2.651439, mean_q: 0.815509\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6688/10000: episode: 1463, duration: 0.023s, episode steps: 5, steps per second: 216, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1066.458252, mean_absolute_error: 2.537122, mean_q: 0.829363\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6691/10000: episode: 1464, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [3.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 943.742981, mean_absolute_error: 2.300153, mean_q: 0.843574\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6695/10000: episode: 1465, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.500 [0.000, 2.000], loss: 903.381226, mean_absolute_error: 2.201575, mean_q: 0.845906\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6701/10000: episode: 1466, duration: 0.022s, episode steps: 6, steps per second: 270, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1279.417114, mean_absolute_error: 2.993797, mean_q: 0.802897\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6706/10000: episode: 1467, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1065.685303, mean_absolute_error: 2.527805, mean_q: 0.781650\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6710/10000: episode: 1468, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1213.462891, mean_absolute_error: 2.858494, mean_q: 0.781940\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6713/10000: episode: 1469, duration: 0.013s, episode steps: 3, steps per second: 237, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [1.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 837.978516, mean_absolute_error: 2.036806, mean_q: 0.839373\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6718/10000: episode: 1470, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1126.416748, mean_absolute_error: 2.651415, mean_q: 0.871875\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6721/10000: episode: 1471, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.000 [5.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 785.407532, mean_absolute_error: 1.927902, mean_q: 0.830243\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6729/10000: episode: 1472, duration: 0.029s, episode steps: 8, steps per second: 281, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 900.181030, mean_absolute_error: 2.157897, mean_q: 0.837457\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6734/10000: episode: 1473, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1000.216431, mean_absolute_error: 2.372052, mean_q: 0.824768\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6738/10000: episode: 1474, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 547.811035, mean_absolute_error: 1.367244, mean_q: 0.759488\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6742/10000: episode: 1475, duration: 0.020s, episode steps: 4, steps per second: 204, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [4.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1290.791626, mean_absolute_error: 3.021993, mean_q: 0.740070\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6748/10000: episode: 1476, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1174.079712, mean_absolute_error: 2.774855, mean_q: 0.811851\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6752/10000: episode: 1477, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [5.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 862.732666, mean_absolute_error: 2.102844, mean_q: 0.836025\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6754/10000: episode: 1478, duration: 0.010s, episode steps: 2, steps per second: 202, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 862.720276, mean_absolute_error: 2.095524, mean_q: 0.845275\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6758/10000: episode: 1479, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1134.549927, mean_absolute_error: 2.679952, mean_q: 0.837774\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6762/10000: episode: 1480, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [5.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1290.804077, mean_absolute_error: 3.022145, mean_q: 0.855449\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6767/10000: episode: 1481, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1003.308716, mean_absolute_error: 2.396556, mean_q: 0.856233\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6772/10000: episode: 1482, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 694.056824, mean_absolute_error: 1.740981, mean_q: 0.890838\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6776/10000: episode: 1483, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 979.037720, mean_absolute_error: 2.315266, mean_q: 0.899278\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6782/10000: episode: 1484, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.500 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1071.474731, mean_absolute_error: 2.547554, mean_q: 0.925339\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6788/10000: episode: 1485, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1045.411255, mean_absolute_error: 2.484936, mean_q: 0.941920\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6792/10000: episode: 1486, duration: 0.020s, episode steps: 4, steps per second: 200, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1214.086548, mean_absolute_error: 2.867445, mean_q: 0.933444\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6797/10000: episode: 1487, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 751.454102, mean_absolute_error: 1.832045, mean_q: 0.864744\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6803/10000: episode: 1488, duration: 0.024s, episode steps: 6, steps per second: 247, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 939.282959, mean_absolute_error: 2.251385, mean_q: 0.857182\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6809/10000: episode: 1489, duration: 0.023s, episode steps: 6, steps per second: 257, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.500 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 860.630371, mean_absolute_error: 2.071400, mean_q: 0.836574\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6811/10000: episode: 1490, duration: 0.010s, episode steps: 2, steps per second: 195, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 1331.469482, mean_absolute_error: 3.132940, mean_q: 0.850499\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6816/10000: episode: 1491, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 938.994812, mean_absolute_error: 2.251973, mean_q: 0.866575\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6822/10000: episode: 1492, duration: 0.026s, episode steps: 6, steps per second: 231, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.667 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 965.753845, mean_absolute_error: 2.316019, mean_q: 0.855317\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6827/10000: episode: 1493, duration: 0.021s, episode steps: 5, steps per second: 234, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1252.592041, mean_absolute_error: 2.952931, mean_q: 0.848621\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6832/10000: episode: 1494, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1033.842163, mean_absolute_error: 2.469346, mean_q: 0.836467\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6837/10000: episode: 1495, duration: 0.022s, episode steps: 5, steps per second: 227, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 752.615417, mean_absolute_error: 1.848432, mean_q: 0.820922\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6839/10000: episode: 1496, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1331.472168, mean_absolute_error: 3.145853, mean_q: 0.810035\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6846/10000: episode: 1497, duration: 0.027s, episode steps: 7, steps per second: 257, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1165.051025, mean_absolute_error: 2.781666, mean_q: 0.828516\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6849/10000: episode: 1498, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 937.732483, mean_absolute_error: 2.241910, mean_q: 0.851193\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6853/10000: episode: 1499, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 866.495728, mean_absolute_error: 2.135325, mean_q: 0.812230\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6856/10000: episode: 1500, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1198.146118, mean_absolute_error: 2.815822, mean_q: 0.841976\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6862/10000: episode: 1501, duration: 0.023s, episode steps: 6, steps per second: 257, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1151.774292, mean_absolute_error: 2.757457, mean_q: 0.817666\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6868/10000: episode: 1502, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1046.983521, mean_absolute_error: 2.520685, mean_q: 0.803748\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6872/10000: episode: 1503, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 940.847473, mean_absolute_error: 2.272249, mean_q: 0.802961\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6876/10000: episode: 1504, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 742.706482, mean_absolute_error: 1.803499, mean_q: 0.784656\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6881/10000: episode: 1505, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 969.580444, mean_absolute_error: 2.298615, mean_q: 0.835006\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6886/10000: episode: 1506, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1191.645752, mean_absolute_error: 2.830492, mean_q: 0.861574\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6891/10000: episode: 1507, duration: 0.025s, episode steps: 5, steps per second: 203, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1002.845337, mean_absolute_error: 2.404439, mean_q: 0.856607\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6896/10000: episode: 1508, duration: 0.021s, episode steps: 5, steps per second: 235, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1100.755615, mean_absolute_error: 2.629515, mean_q: 0.859837\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6899/10000: episode: 1509, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 993.764160, mean_absolute_error: 2.394552, mean_q: 0.830683\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6903/10000: episode: 1510, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.250 [5.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 901.907959, mean_absolute_error: 2.174743, mean_q: 0.837811\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6907/10000: episode: 1511, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1176.778564, mean_absolute_error: 2.795891, mean_q: 0.827655\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6913/10000: episode: 1512, duration: 0.023s, episode steps: 6, steps per second: 260, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 915.593079, mean_absolute_error: 2.206005, mean_q: 0.829203\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6919/10000: episode: 1513, duration: 0.023s, episode steps: 6, steps per second: 257, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1096.563477, mean_absolute_error: 2.598664, mean_q: 0.809694\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6923/10000: episode: 1514, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1137.560913, mean_absolute_error: 2.725039, mean_q: 0.816402\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6931/10000: episode: 1515, duration: 0.030s, episode steps: 8, steps per second: 269, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 2.875 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 961.060913, mean_absolute_error: 2.317310, mean_q: 0.815496\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6935/10000: episode: 1516, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 901.781494, mean_absolute_error: 2.184281, mean_q: 0.787751\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6940/10000: episode: 1517, duration: 0.021s, episode steps: 5, steps per second: 244, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1063.948975, mean_absolute_error: 2.522417, mean_q: 0.819194\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6943/10000: episode: 1518, duration: 0.016s, episode steps: 3, steps per second: 193, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 887.723694, mean_absolute_error: 2.142317, mean_q: 0.815792\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6948/10000: episode: 1519, duration: 0.022s, episode steps: 5, steps per second: 224, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1001.464478, mean_absolute_error: 2.391707, mean_q: 0.788844\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6953/10000: episode: 1520, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1064.071045, mean_absolute_error: 2.521300, mean_q: 0.812582\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6958/10000: episode: 1521, duration: 0.032s, episode steps: 5, steps per second: 157, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 940.083801, mean_absolute_error: 2.258098, mean_q: 0.821607\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6964/10000: episode: 1522, duration: 0.034s, episode steps: 6, steps per second: 174, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1043.442261, mean_absolute_error: 2.474964, mean_q: 0.806098\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6968/10000: episode: 1523, duration: 0.027s, episode steps: 4, steps per second: 147, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1173.581543, mean_absolute_error: 2.765837, mean_q: 0.821646\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6974/10000: episode: 1524, duration: 0.036s, episode steps: 6, steps per second: 168, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 940.543945, mean_absolute_error: 2.262571, mean_q: 0.824978\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6978/10000: episode: 1525, duration: 0.026s, episode steps: 4, steps per second: 155, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1054.885010, mean_absolute_error: 2.485631, mean_q: 0.818291\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6982/10000: episode: 1526, duration: 0.024s, episode steps: 4, steps per second: 170, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 706.327759, mean_absolute_error: 1.750295, mean_q: 0.795737\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6988/10000: episode: 1527, duration: 0.025s, episode steps: 6, steps per second: 243, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1097.196411, mean_absolute_error: 2.615426, mean_q: 0.822221\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6993/10000: episode: 1528, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1252.836792, mean_absolute_error: 2.956575, mean_q: 0.803061\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 6999/10000: episode: 1529, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 964.273621, mean_absolute_error: 2.290426, mean_q: 0.798759\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7004/10000: episode: 1530, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [3.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 938.967773, mean_absolute_error: 2.247455, mean_q: 0.820169\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7009/10000: episode: 1531, duration: 0.021s, episode steps: 5, steps per second: 235, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1062.724121, mean_absolute_error: 2.507926, mean_q: 0.801184\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7014/10000: episode: 1532, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1129.584839, mean_absolute_error: 2.688184, mean_q: 0.816191\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7020/10000: episode: 1533, duration: 0.024s, episode steps: 6, steps per second: 250, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 809.610535, mean_absolute_error: 1.973658, mean_q: 0.784843\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7024/10000: episode: 1534, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.250 [0.000, 3.000], mean observation: 0.500 [0.000, 2.000], loss: 821.974609, mean_absolute_error: 1.999687, mean_q: 0.807283\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7026/10000: episode: 1535, duration: 0.010s, episode steps: 2, steps per second: 194, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 1096.895142, mean_absolute_error: 2.623541, mean_q: 0.848509\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7029/10000: episode: 1536, duration: 0.017s, episode steps: 3, steps per second: 175, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1041.895142, mean_absolute_error: 2.469266, mean_q: 0.814550\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7034/10000: episode: 1537, duration: 0.022s, episode steps: 5, steps per second: 222, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 692.632507, mean_absolute_error: 1.737235, mean_q: 0.837896\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7040/10000: episode: 1538, duration: 0.023s, episode steps: 6, steps per second: 260, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 1096.571167, mean_absolute_error: 2.604571, mean_q: 0.839135\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7042/10000: episode: 1539, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 1406.470215, mean_absolute_error: 3.280999, mean_q: 0.806226\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7046/10000: episode: 1540, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1101.624756, mean_absolute_error: 2.661442, mean_q: 0.839111\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7052/10000: episode: 1541, duration: 0.023s, episode steps: 6, steps per second: 257, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.167 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 967.005920, mean_absolute_error: 2.330151, mean_q: 0.791164\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7054/10000: episode: 1542, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 547.095642, mean_absolute_error: 1.358774, mean_q: 0.796315\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7059/10000: episode: 1543, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1224.502319, mean_absolute_error: 2.915089, mean_q: 0.815586\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7064/10000: episode: 1544, duration: 0.022s, episode steps: 5, steps per second: 224, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 944.486938, mean_absolute_error: 2.307349, mean_q: 0.803089\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7070/10000: episode: 1545, duration: 0.025s, episode steps: 6, steps per second: 237, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1124.592407, mean_absolute_error: 2.685458, mean_q: 0.800684\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7072/10000: episode: 1546, duration: 0.011s, episode steps: 2, steps per second: 188, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1640.806396, mean_absolute_error: 3.785556, mean_q: 0.812092\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7077/10000: episode: 1547, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 974.168579, mean_absolute_error: 2.358528, mean_q: 0.803323\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7083/10000: episode: 1548, duration: 0.028s, episode steps: 6, steps per second: 215, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [2.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 916.274475, mean_absolute_error: 2.222559, mean_q: 0.786367\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7087/10000: episode: 1549, duration: 0.018s, episode steps: 4, steps per second: 219, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1023.664917, mean_absolute_error: 2.503637, mean_q: 0.799773\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7089/10000: episode: 1550, duration: 0.010s, episode steps: 2, steps per second: 199, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 392.471497, mean_absolute_error: 1.038247, mean_q: 0.730384\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7093/10000: episode: 1551, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1214.104492, mean_absolute_error: 2.870054, mean_q: 0.820424\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7095/10000: episode: 1552, duration: 0.010s, episode steps: 2, steps per second: 193, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 1097.079590, mean_absolute_error: 2.616321, mean_q: 0.804899\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7098/10000: episode: 1553, duration: 0.014s, episode steps: 3, steps per second: 210, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1045.995850, mean_absolute_error: 2.505677, mean_q: 0.758993\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7102/10000: episode: 1554, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1137.383179, mean_absolute_error: 2.717214, mean_q: 0.812669\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7105/10000: episode: 1555, duration: 0.016s, episode steps: 3, steps per second: 184, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [4.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 994.994141, mean_absolute_error: 2.391800, mean_q: 0.789603\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7110/10000: episode: 1556, duration: 0.027s, episode steps: 5, steps per second: 186, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 788.227905, mean_absolute_error: 1.961656, mean_q: 0.806395\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7114/10000: episode: 1557, duration: 0.035s, episode steps: 4, steps per second: 115, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.500 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1255.048340, mean_absolute_error: 2.979198, mean_q: 0.785382\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7121/10000: episode: 1558, duration: 0.052s, episode steps: 7, steps per second: 133, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.143 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1187.352295, mean_absolute_error: 2.819642, mean_q: 0.794063\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7126/10000: episode: 1559, duration: 0.030s, episode steps: 5, steps per second: 166, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1065.188232, mean_absolute_error: 2.535784, mean_q: 0.774447\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7129/10000: episode: 1560, duration: 0.020s, episode steps: 3, steps per second: 151, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [5.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 994.588135, mean_absolute_error: 2.392081, mean_q: 0.747437\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7134/10000: episode: 1561, duration: 0.031s, episode steps: 5, steps per second: 163, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [3.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 752.879517, mean_absolute_error: 1.841660, mean_q: 0.775317\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7138/10000: episode: 1562, duration: 0.022s, episode steps: 4, steps per second: 183, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1135.190674, mean_absolute_error: 2.676786, mean_q: 0.787512\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7140/10000: episode: 1563, duration: 0.011s, episode steps: 2, steps per second: 180, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 393.679291, mean_absolute_error: 1.049482, mean_q: 0.749701\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7146/10000: episode: 1564, duration: 0.022s, episode steps: 6, steps per second: 270, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 912.590332, mean_absolute_error: 2.181652, mean_q: 0.747773\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7149/10000: episode: 1565, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1308.524414, mean_absolute_error: 3.109982, mean_q: 0.779470\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7152/10000: episode: 1566, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [3.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1151.551025, mean_absolute_error: 2.740738, mean_q: 0.749409\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7155/10000: episode: 1567, duration: 0.013s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1252.308960, mean_absolute_error: 2.958764, mean_q: 0.768150\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7160/10000: episode: 1568, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 721.364075, mean_absolute_error: 1.780756, mean_q: 0.746226\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7164/10000: episode: 1569, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 706.301147, mean_absolute_error: 1.746078, mean_q: 0.733573\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7168/10000: episode: 1570, duration: 0.018s, episode steps: 4, steps per second: 228, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [4.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1329.158813, mean_absolute_error: 3.109572, mean_q: 0.753746\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7174/10000: episode: 1571, duration: 0.024s, episode steps: 6, steps per second: 252, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 967.441101, mean_absolute_error: 2.336202, mean_q: 0.762639\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7180/10000: episode: 1572, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 4.667 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1045.834595, mean_absolute_error: 2.509853, mean_q: 0.766607\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7183/10000: episode: 1573, duration: 0.013s, episode steps: 3, steps per second: 236, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [5.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1041.869995, mean_absolute_error: 2.461719, mean_q: 0.752587\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7186/10000: episode: 1574, duration: 0.014s, episode steps: 3, steps per second: 218, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [1.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: 939.835938, mean_absolute_error: 2.262160, mean_q: 0.786261\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7188/10000: episode: 1575, duration: 0.010s, episode steps: 2, steps per second: 199, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.300 [0.000, 2.000], loss: 937.734375, mean_absolute_error: 2.242534, mean_q: 0.752855\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7190/10000: episode: 1576, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 1020.095093, mean_absolute_error: 2.451792, mean_q: 0.722910\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7192/10000: episode: 1577, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 1096.842651, mean_absolute_error: 2.601497, mean_q: 0.752562\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7197/10000: episode: 1578, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.200 [1.000, 4.000], mean observation: 0.600 [0.000, 2.000], loss: 1191.986816, mean_absolute_error: 2.834096, mean_q: 0.722400\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7202/10000: episode: 1579, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1035.824463, mean_absolute_error: 2.478119, mean_q: 0.778389\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7206/10000: episode: 1580, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [3.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 900.080933, mean_absolute_error: 2.169099, mean_q: 0.746655\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7211/10000: episode: 1581, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 814.598267, mean_absolute_error: 1.965211, mean_q: 0.769332\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7214/10000: episode: 1582, duration: 0.015s, episode steps: 3, steps per second: 199, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1043.954590, mean_absolute_error: 2.484226, mean_q: 0.742113\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7220/10000: episode: 1583, duration: 0.026s, episode steps: 6, steps per second: 231, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.167 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1177.295532, mean_absolute_error: 2.810929, mean_q: 0.762091\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7229/10000: episode: 1584, duration: 0.032s, episode steps: 9, steps per second: 280, episode reward: -100.000, mean reward: -11.111 [-100.000, 0.000], mean action: 3.333 [0.000, 8.000], mean observation: 0.911 [0.000, 2.000], loss: 1114.074341, mean_absolute_error: 2.644887, mean_q: 0.747116\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7234/10000: episode: 1585, duration: 0.021s, episode steps: 5, steps per second: 240, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1316.435547, mean_absolute_error: 3.102394, mean_q: 0.756906\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7239/10000: episode: 1586, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 906.473145, mean_absolute_error: 2.167790, mean_q: 0.751301\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7246/10000: episode: 1587, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.714 [0.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1026.988403, mean_absolute_error: 2.429635, mean_q: 0.786157\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7251/10000: episode: 1588, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 940.967285, mean_absolute_error: 2.265102, mean_q: 0.778344\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7255/10000: episode: 1589, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 902.406677, mean_absolute_error: 2.177147, mean_q: 0.797392\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7257/10000: episode: 1590, duration: 0.010s, episode steps: 2, steps per second: 207, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 548.678772, mean_absolute_error: 1.373858, mean_q: 0.735985\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7260/10000: episode: 1591, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [1.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 1201.470215, mean_absolute_error: 2.833107, mean_q: 0.808825\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7263/10000: episode: 1592, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [1.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 1617.889038, mean_absolute_error: 3.758862, mean_q: 0.772536\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7265/10000: episode: 1593, duration: 0.011s, episode steps: 2, steps per second: 186, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 787.135559, mean_absolute_error: 1.954064, mean_q: 0.849358\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7272/10000: episode: 1594, duration: 0.030s, episode steps: 7, steps per second: 234, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1030.498901, mean_absolute_error: 2.472314, mean_q: 0.784495\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7279/10000: episode: 1595, duration: 0.027s, episode steps: 7, steps per second: 255, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [1.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1162.167114, mean_absolute_error: 2.742884, mean_q: 0.833510\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7281/10000: episode: 1596, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 862.473389, mean_absolute_error: 2.098731, mean_q: 0.858133\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7286/10000: episode: 1597, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1002.840027, mean_absolute_error: 2.399742, mean_q: 0.802008\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7289/10000: episode: 1598, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 887.516418, mean_absolute_error: 2.143615, mean_q: 0.836529\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7296/10000: episode: 1599, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.571 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1052.526245, mean_absolute_error: 2.512408, mean_q: 0.809560\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7302/10000: episode: 1600, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 965.932678, mean_absolute_error: 2.313370, mean_q: 0.826059\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7307/10000: episode: 1601, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1283.932861, mean_absolute_error: 3.011857, mean_q: 0.826232\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7309/10000: episode: 1602, duration: 0.010s, episode steps: 2, steps per second: 195, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 626.819946, mean_absolute_error: 1.552000, mean_q: 0.840946\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7314/10000: episode: 1603, duration: 0.021s, episode steps: 5, steps per second: 242, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1158.926514, mean_absolute_error: 2.738352, mean_q: 0.823553\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7318/10000: episode: 1604, duration: 0.019s, episode steps: 4, steps per second: 213, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 941.589539, mean_absolute_error: 2.264664, mean_q: 0.812178\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7320/10000: episode: 1605, duration: 0.011s, episode steps: 2, steps per second: 189, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 781.499329, mean_absolute_error: 1.900051, mean_q: 0.801244\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7323/10000: episode: 1606, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [1.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1409.317505, mean_absolute_error: 3.301159, mean_q: 0.830257\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7326/10000: episode: 1607, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.000 [5.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1202.113159, mean_absolute_error: 2.859980, mean_q: 0.757025\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7328/10000: episode: 1608, duration: 0.010s, episode steps: 2, steps per second: 197, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 1328.308594, mean_absolute_error: 3.099951, mean_q: 0.795523\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7331/10000: episode: 1609, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 1000.236633, mean_absolute_error: 2.469459, mean_q: 0.794636\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7336/10000: episode: 1610, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1096.352295, mean_absolute_error: 2.603281, mean_q: 0.794404\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7341/10000: episode: 1611, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 941.458618, mean_absolute_error: 2.265158, mean_q: 0.868488\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7346/10000: episode: 1612, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 2.600 [0.000, 5.000], mean observation: 0.520 [0.000, 2.000], loss: 1196.307617, mean_absolute_error: 2.875394, mean_q: 0.882479\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7349/10000: episode: 1613, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1303.187622, mean_absolute_error: 3.027508, mean_q: 0.880346\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7356/10000: episode: 1614, duration: 0.028s, episode steps: 7, steps per second: 251, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [1.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1186.034912, mean_absolute_error: 2.809343, mean_q: 0.873836\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7363/10000: episode: 1615, duration: 0.031s, episode steps: 7, steps per second: 229, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 3.714 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 874.145935, mean_absolute_error: 2.120041, mean_q: 0.883819\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7365/10000: episode: 1616, duration: 0.013s, episode steps: 2, steps per second: 151, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 1178.319580, mean_absolute_error: 2.822145, mean_q: 0.824713\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7368/10000: episode: 1617, duration: 0.015s, episode steps: 3, steps per second: 206, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 889.907288, mean_absolute_error: 2.167743, mean_q: 0.863405\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7373/10000: episode: 1618, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 815.195984, mean_absolute_error: 1.979706, mean_q: 0.790817\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7376/10000: episode: 1619, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [2.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 729.400452, mean_absolute_error: 1.772407, mean_q: 0.795096\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7379/10000: episode: 1620, duration: 0.014s, episode steps: 3, steps per second: 215, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [0.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 889.808411, mean_absolute_error: 2.163170, mean_q: 0.769215\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7385/10000: episode: 1621, duration: 0.024s, episode steps: 6, steps per second: 254, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1016.879211, mean_absolute_error: 2.419858, mean_q: 0.792965\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7390/10000: episode: 1622, duration: 0.019s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 817.598083, mean_absolute_error: 2.010175, mean_q: 0.806450\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7396/10000: episode: 1623, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 915.546875, mean_absolute_error: 2.229551, mean_q: 0.792275\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7400/10000: episode: 1624, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 900.252136, mean_absolute_error: 2.169033, mean_q: 0.797610\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7404/10000: episode: 1625, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [3.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 859.609314, mean_absolute_error: 2.068914, mean_q: 0.753969\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7412/10000: episode: 1626, duration: 0.029s, episode steps: 8, steps per second: 276, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.625 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 958.046204, mean_absolute_error: 2.296151, mean_q: 0.774986\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7416/10000: episode: 1627, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 980.061768, mean_absolute_error: 2.357497, mean_q: 0.752604\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7419/10000: episode: 1628, duration: 0.014s, episode steps: 3, steps per second: 211, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 939.992737, mean_absolute_error: 2.254781, mean_q: 0.755328\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7425/10000: episode: 1629, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1121.051392, mean_absolute_error: 2.658260, mean_q: 0.776458\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7429/10000: episode: 1630, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1251.030762, mean_absolute_error: 2.927668, mean_q: 0.775412\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7436/10000: episode: 1631, duration: 0.026s, episode steps: 7, steps per second: 269, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.571 [2.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 852.437500, mean_absolute_error: 2.073169, mean_q: 0.784032\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7440/10000: episode: 1632, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1018.816895, mean_absolute_error: 2.445784, mean_q: 0.795564\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7446/10000: episode: 1633, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1303.850464, mean_absolute_error: 3.054394, mean_q: 0.806254\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7451/10000: episode: 1634, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1096.498535, mean_absolute_error: 2.614551, mean_q: 0.783064\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7454/10000: episode: 1635, duration: 0.015s, episode steps: 3, steps per second: 206, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [3.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 783.576355, mean_absolute_error: 1.920113, mean_q: 0.775418\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7460/10000: episode: 1636, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1097.001099, mean_absolute_error: 2.617079, mean_q: 0.773767\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7465/10000: episode: 1637, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [3.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1314.656250, mean_absolute_error: 3.086112, mean_q: 0.804191\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7473/10000: episode: 1638, duration: 0.036s, episode steps: 8, steps per second: 223, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1019.674072, mean_absolute_error: 2.449806, mean_q: 0.813285\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7476/10000: episode: 1639, duration: 0.015s, episode steps: 3, steps per second: 205, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1044.206421, mean_absolute_error: 2.485301, mean_q: 0.857176\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7482/10000: episode: 1640, duration: 0.023s, episode steps: 6, steps per second: 260, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1019.194397, mean_absolute_error: 2.438454, mean_q: 0.820824\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7489/10000: episode: 1641, duration: 0.027s, episode steps: 7, steps per second: 256, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.286 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1183.558716, mean_absolute_error: 2.778825, mean_q: 0.773525\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7491/10000: episode: 1642, duration: 0.010s, episode steps: 2, steps per second: 196, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 547.840637, mean_absolute_error: 1.378714, mean_q: 0.755077\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7496/10000: episode: 1643, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 938.866821, mean_absolute_error: 2.255811, mean_q: 0.778485\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7499/10000: episode: 1644, duration: 0.014s, episode steps: 3, steps per second: 217, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 733.366211, mean_absolute_error: 1.822707, mean_q: 0.757418\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7503/10000: episode: 1645, duration: 0.018s, episode steps: 4, steps per second: 217, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [3.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 864.333069, mean_absolute_error: 2.115998, mean_q: 0.792514\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7506/10000: episode: 1646, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 783.333191, mean_absolute_error: 1.908018, mean_q: 0.760489\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7510/10000: episode: 1647, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 865.203125, mean_absolute_error: 2.109045, mean_q: 0.743275\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7513/10000: episode: 1648, duration: 0.014s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 787.541687, mean_absolute_error: 1.961179, mean_q: 0.803594\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7516/10000: episode: 1649, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1406.665405, mean_absolute_error: 3.273462, mean_q: 0.834509\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7520/10000: episode: 1650, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1175.181396, mean_absolute_error: 2.778690, mean_q: 0.869977\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7526/10000: episode: 1651, duration: 0.027s, episode steps: 6, steps per second: 222, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1148.731201, mean_absolute_error: 2.711474, mean_q: 0.846545\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7533/10000: episode: 1652, duration: 0.026s, episode steps: 7, steps per second: 267, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.857 [0.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 1076.603271, mean_absolute_error: 2.587738, mean_q: 0.819786\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7536/10000: episode: 1653, duration: 0.014s, episode steps: 3, steps per second: 217, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.333 [0.000, 2.000], loss: 891.711975, mean_absolute_error: 2.193607, mean_q: 0.866027\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7538/10000: episode: 1654, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 1093.947998, mean_absolute_error: 2.580286, mean_q: 0.898871\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7545/10000: episode: 1655, duration: 0.026s, episode steps: 7, steps per second: 266, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.286 [3.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 851.045837, mean_absolute_error: 2.055991, mean_q: 0.892748\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7550/10000: episode: 1656, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1252.207397, mean_absolute_error: 2.935693, mean_q: 0.901088\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7552/10000: episode: 1657, duration: 0.010s, episode steps: 2, steps per second: 205, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 862.993774, mean_absolute_error: 2.079905, mean_q: 0.903100\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7556/10000: episode: 1658, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 746.789673, mean_absolute_error: 1.833459, mean_q: 0.899175\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7561/10000: episode: 1659, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1002.785461, mean_absolute_error: 2.385496, mean_q: 0.905187\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7564/10000: episode: 1660, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [3.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1098.073120, mean_absolute_error: 2.614365, mean_q: 0.927964\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7569/10000: episode: 1661, duration: 0.021s, episode steps: 5, steps per second: 235, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.800 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1130.005127, mean_absolute_error: 2.696126, mean_q: 0.907530\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7571/10000: episode: 1662, duration: 0.012s, episode steps: 2, steps per second: 165, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 859.566528, mean_absolute_error: 2.055008, mean_q: 0.895958\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7576/10000: episode: 1663, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 1.200 [0.000, 3.000], mean observation: 0.600 [0.000, 2.000], loss: 1161.586670, mean_absolute_error: 2.760307, mean_q: 0.910644\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7579/10000: episode: 1664, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [5.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1204.996704, mean_absolute_error: 2.877362, mean_q: 0.865503\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7585/10000: episode: 1665, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1121.135986, mean_absolute_error: 2.645324, mean_q: 0.865984\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7592/10000: episode: 1666, duration: 0.027s, episode steps: 7, steps per second: 258, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1072.512695, mean_absolute_error: 2.539074, mean_q: 0.861000\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7597/10000: episode: 1667, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 940.207031, mean_absolute_error: 2.264811, mean_q: 0.864445\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7600/10000: episode: 1668, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 992.405457, mean_absolute_error: 2.378384, mean_q: 0.882915\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7602/10000: episode: 1669, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 1019.176208, mean_absolute_error: 2.436949, mean_q: 0.905012\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7607/10000: episode: 1670, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1128.938843, mean_absolute_error: 2.683568, mean_q: 0.896307\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7613/10000: episode: 1671, duration: 0.024s, episode steps: 6, steps per second: 251, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1122.113159, mean_absolute_error: 2.661363, mean_q: 0.915894\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7619/10000: episode: 1672, duration: 0.025s, episode steps: 6, steps per second: 236, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 941.236084, mean_absolute_error: 2.262838, mean_q: 0.919550\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7624/10000: episode: 1673, duration: 0.024s, episode steps: 5, steps per second: 209, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1191.262939, mean_absolute_error: 2.822989, mean_q: 0.912389\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7628/10000: episode: 1674, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 901.934448, mean_absolute_error: 2.186132, mean_q: 0.890582\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7634/10000: episode: 1675, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 1198.594360, mean_absolute_error: 2.817261, mean_q: 0.921103\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7637/10000: episode: 1676, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [5.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1094.028320, mean_absolute_error: 2.594641, mean_q: 0.903579\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7641/10000: episode: 1677, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1095.002808, mean_absolute_error: 2.593264, mean_q: 0.893015\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7649/10000: episode: 1678, duration: 0.029s, episode steps: 8, steps per second: 275, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 5.250 [1.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 882.248474, mean_absolute_error: 2.134196, mean_q: 0.907549\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7653/10000: episode: 1679, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 865.843506, mean_absolute_error: 2.127482, mean_q: 0.911220\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7659/10000: episode: 1680, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1277.297119, mean_absolute_error: 2.997674, mean_q: 0.912839\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7661/10000: episode: 1681, duration: 0.010s, episode steps: 2, steps per second: 207, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 867.711548, mean_absolute_error: 2.124042, mean_q: 0.885990\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7665/10000: episode: 1682, duration: 0.017s, episode steps: 4, steps per second: 236, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 939.791931, mean_absolute_error: 2.234253, mean_q: 0.909375\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7671/10000: episode: 1683, duration: 0.024s, episode steps: 6, steps per second: 248, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1176.957275, mean_absolute_error: 2.799127, mean_q: 0.888404\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7677/10000: episode: 1684, duration: 0.026s, episode steps: 6, steps per second: 230, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 1042.281128, mean_absolute_error: 2.461068, mean_q: 0.854285\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7679/10000: episode: 1685, duration: 0.011s, episode steps: 2, steps per second: 176, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.300 [0.000, 2.000], loss: 937.699158, mean_absolute_error: 2.230887, mean_q: 0.854049\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7686/10000: episode: 1686, duration: 0.027s, episode steps: 7, steps per second: 258, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1255.120117, mean_absolute_error: 2.963429, mean_q: 0.819300\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7691/10000: episode: 1687, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 877.340027, mean_absolute_error: 2.091634, mean_q: 0.738422\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7695/10000: episode: 1688, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 901.659790, mean_absolute_error: 2.175298, mean_q: 0.798400\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7700/10000: episode: 1689, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1034.115845, mean_absolute_error: 2.468005, mean_q: 0.761436\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7705/10000: episode: 1690, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1067.721191, mean_absolute_error: 2.567039, mean_q: 0.802900\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7710/10000: episode: 1691, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 846.336121, mean_absolute_error: 2.050356, mean_q: 0.802537\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7717/10000: episode: 1692, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.714 [0.000, 5.000], mean observation: 0.657 [0.000, 2.000], loss: 1117.630737, mean_absolute_error: 2.636643, mean_q: 0.831244\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7722/10000: episode: 1693, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 972.733215, mean_absolute_error: 2.348406, mean_q: 0.848193\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7726/10000: episode: 1694, duration: 0.020s, episode steps: 4, steps per second: 198, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 822.852905, mean_absolute_error: 1.981967, mean_q: 0.773803\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7730/10000: episode: 1695, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 0.400 [0.000, 2.000], loss: 1015.865662, mean_absolute_error: 2.410125, mean_q: 0.769509\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7732/10000: episode: 1696, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 788.041626, mean_absolute_error: 1.961205, mean_q: 0.759420\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7735/10000: episode: 1697, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1102.062744, mean_absolute_error: 2.663566, mean_q: 0.770956\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7738/10000: episode: 1698, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 941.114929, mean_absolute_error: 2.264304, mean_q: 0.794965\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7745/10000: episode: 1699, duration: 0.026s, episode steps: 7, steps per second: 273, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 963.671997, mean_absolute_error: 2.312750, mean_q: 0.791688\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7750/10000: episode: 1700, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1002.619812, mean_absolute_error: 2.390170, mean_q: 0.780560\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7757/10000: episode: 1701, duration: 0.026s, episode steps: 7, steps per second: 270, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1005.903687, mean_absolute_error: 2.394881, mean_q: 0.760640\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7762/10000: episode: 1702, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1253.370850, mean_absolute_error: 2.952304, mean_q: 0.784892\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7765/10000: episode: 1703, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1252.401978, mean_absolute_error: 2.953571, mean_q: 0.797500\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7770/10000: episode: 1704, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1226.712158, mean_absolute_error: 2.924400, mean_q: 0.804353\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7778/10000: episode: 1705, duration: 0.031s, episode steps: 8, steps per second: 258, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.125 [1.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 1078.240112, mean_absolute_error: 2.579325, mean_q: 0.787785\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7781/10000: episode: 1706, duration: 0.018s, episode steps: 3, steps per second: 166, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.333 [0.000, 2.000], loss: 996.052490, mean_absolute_error: 2.419091, mean_q: 0.827124\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7787/10000: episode: 1707, duration: 0.024s, episode steps: 6, steps per second: 250, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1204.493286, mean_absolute_error: 2.875907, mean_q: 0.820919\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7793/10000: episode: 1708, duration: 0.024s, episode steps: 6, steps per second: 250, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.167 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1228.229492, mean_absolute_error: 2.914728, mean_q: 0.815835\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7798/10000: episode: 1709, duration: 0.021s, episode steps: 5, steps per second: 244, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1252.698608, mean_absolute_error: 2.952975, mean_q: 0.807238\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7805/10000: episode: 1710, duration: 0.027s, episode steps: 7, steps per second: 262, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.671 [0.000, 2.000], loss: 1011.275452, mean_absolute_error: 2.439152, mean_q: 0.822307\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7808/10000: episode: 1711, duration: 0.015s, episode steps: 3, steps per second: 196, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 1043.766113, mean_absolute_error: 2.487285, mean_q: 0.857116\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7813/10000: episode: 1712, duration: 0.021s, episode steps: 5, steps per second: 238, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1126.511475, mean_absolute_error: 2.657593, mean_q: 0.781113\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7818/10000: episode: 1713, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1036.423706, mean_absolute_error: 2.502069, mean_q: 0.827539\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7824/10000: episode: 1714, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.833 [3.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1071.366089, mean_absolute_error: 2.542519, mean_q: 0.842905\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7827/10000: episode: 1715, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1306.442749, mean_absolute_error: 3.086267, mean_q: 0.887889\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7830/10000: episode: 1716, duration: 0.015s, episode steps: 3, steps per second: 194, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [5.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1302.286987, mean_absolute_error: 3.033301, mean_q: 0.910699\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7835/10000: episode: 1717, duration: 0.026s, episode steps: 5, steps per second: 194, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1095.827759, mean_absolute_error: 2.596999, mean_q: 0.905066\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7840/10000: episode: 1718, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1068.188721, mean_absolute_error: 2.545931, mean_q: 0.890499\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7846/10000: episode: 1719, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1043.418823, mean_absolute_error: 2.473364, mean_q: 0.912204\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7850/10000: episode: 1720, duration: 0.017s, episode steps: 4, steps per second: 233, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 900.093323, mean_absolute_error: 2.139173, mean_q: 0.875093\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7852/10000: episode: 1721, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 547.084534, mean_absolute_error: 1.360550, mean_q: 0.899007\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7856/10000: episode: 1722, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [5.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 900.116089, mean_absolute_error: 2.157763, mean_q: 0.919543\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7860/10000: episode: 1723, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1135.187256, mean_absolute_error: 2.674833, mean_q: 0.925131\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7862/10000: episode: 1724, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 939.289246, mean_absolute_error: 2.236200, mean_q: 0.903709\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7867/10000: episode: 1725, duration: 0.021s, episode steps: 5, steps per second: 243, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1093.957642, mean_absolute_error: 2.578767, mean_q: 0.874332\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7869/10000: episode: 1726, duration: 0.010s, episode steps: 2, steps per second: 197, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 1025.194824, mean_absolute_error: 2.505141, mean_q: 0.882924\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7875/10000: episode: 1727, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 994.096375, mean_absolute_error: 2.395594, mean_q: 0.881024\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7878/10000: episode: 1728, duration: 0.014s, episode steps: 3, steps per second: 214, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 940.790833, mean_absolute_error: 2.252534, mean_q: 0.877267\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7883/10000: episode: 1729, duration: 0.023s, episode steps: 5, steps per second: 219, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1098.235840, mean_absolute_error: 2.623150, mean_q: 0.896444\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7887/10000: episode: 1730, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.750 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 939.319092, mean_absolute_error: 2.260681, mean_q: 0.866451\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7891/10000: episode: 1731, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1251.391602, mean_absolute_error: 2.924632, mean_q: 0.878749\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7896/10000: episode: 1732, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1126.457153, mean_absolute_error: 2.662213, mean_q: 0.890942\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7898/10000: episode: 1733, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.300 [0.000, 2.000], loss: 1178.338623, mean_absolute_error: 2.831513, mean_q: 0.895566\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7902/10000: episode: 1734, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 982.248108, mean_absolute_error: 2.381419, mean_q: 0.897704\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7905/10000: episode: 1735, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [0.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1095.060913, mean_absolute_error: 2.596220, mean_q: 0.909123\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7908/10000: episode: 1736, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [5.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 989.818787, mean_absolute_error: 2.353240, mean_q: 0.882213\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7914/10000: episode: 1737, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.667 [3.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1071.200317, mean_absolute_error: 2.566471, mean_q: 0.903063\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7918/10000: episode: 1738, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 861.945923, mean_absolute_error: 2.073898, mean_q: 0.923402\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7921/10000: episode: 1739, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1198.136475, mean_absolute_error: 2.813497, mean_q: 0.863198\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7928/10000: episode: 1740, duration: 0.028s, episode steps: 7, steps per second: 249, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.571 [0.000, 6.000], mean observation: 0.657 [0.000, 2.000], loss: 988.687195, mean_absolute_error: 2.393833, mean_q: 0.904335\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7934/10000: episode: 1741, duration: 0.026s, episode steps: 6, steps per second: 234, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 785.551819, mean_absolute_error: 1.940831, mean_q: 0.887740\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7937/10000: episode: 1742, duration: 0.014s, episode steps: 3, steps per second: 219, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [2.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 887.992615, mean_absolute_error: 2.155580, mean_q: 0.906327\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7941/10000: episode: 1743, duration: 0.018s, episode steps: 4, steps per second: 223, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [2.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 748.546021, mean_absolute_error: 1.880012, mean_q: 0.864538\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7944/10000: episode: 1744, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 785.869934, mean_absolute_error: 1.934110, mean_q: 0.851640\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7946/10000: episode: 1745, duration: 0.010s, episode steps: 2, steps per second: 197, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 1017.480225, mean_absolute_error: 2.428397, mean_q: 0.922527\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7950/10000: episode: 1746, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 7.000 [6.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 939.322021, mean_absolute_error: 2.264875, mean_q: 0.850161\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7955/10000: episode: 1747, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 785.994751, mean_absolute_error: 1.934083, mean_q: 0.843210\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7958/10000: episode: 1748, duration: 0.014s, episode steps: 3, steps per second: 218, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [2.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 734.631592, mean_absolute_error: 1.828286, mean_q: 0.870035\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7963/10000: episode: 1749, duration: 0.021s, episode steps: 5, steps per second: 241, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 785.733215, mean_absolute_error: 1.928894, mean_q: 0.840100\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7968/10000: episode: 1750, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1253.963623, mean_absolute_error: 2.967576, mean_q: 0.850776\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7971/10000: episode: 1751, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.000 [5.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1200.322144, mean_absolute_error: 2.843521, mean_q: 0.865017\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7978/10000: episode: 1752, duration: 0.027s, episode steps: 7, steps per second: 256, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.714 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1230.919189, mean_absolute_error: 2.906847, mean_q: 0.830507\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7983/10000: episode: 1753, duration: 0.023s, episode steps: 5, steps per second: 219, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1129.610962, mean_absolute_error: 2.682791, mean_q: 0.804932\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7987/10000: episode: 1754, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 978.199524, mean_absolute_error: 2.338203, mean_q: 0.865477\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7993/10000: episode: 1755, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1200.077515, mean_absolute_error: 2.828523, mean_q: 0.891405\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 7998/10000: episode: 1756, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 879.852722, mean_absolute_error: 2.124941, mean_q: 0.860467\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8000/10000: episode: 1757, duration: 0.010s, episode steps: 2, steps per second: 196, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 862.652954, mean_absolute_error: 2.084547, mean_q: 0.861319\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8006/10000: episode: 1758, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 3.833 [0.000, 7.000], mean observation: 0.700 [0.000, 2.000], loss: 967.318420, mean_absolute_error: 2.319443, mean_q: 0.883284\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8009/10000: episode: 1759, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [5.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 993.984192, mean_absolute_error: 2.397548, mean_q: 0.881697\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8015/10000: episode: 1760, duration: 0.023s, episode steps: 6, steps per second: 258, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 4.167 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 966.740051, mean_absolute_error: 2.313459, mean_q: 0.846336\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8018/10000: episode: 1761, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1253.492188, mean_absolute_error: 2.941545, mean_q: 0.847604\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8021/10000: episode: 1762, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 1041.869263, mean_absolute_error: 2.464794, mean_q: 0.792004\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8025/10000: episode: 1763, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1018.933411, mean_absolute_error: 2.434419, mean_q: 0.788430\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8028/10000: episode: 1764, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [2.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1355.484131, mean_absolute_error: 3.159951, mean_q: 0.828103\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8031/10000: episode: 1765, duration: 0.015s, episode steps: 3, steps per second: 200, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [3.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1250.185913, mean_absolute_error: 2.923387, mean_q: 0.787191\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8036/10000: episode: 1766, duration: 0.022s, episode steps: 5, steps per second: 228, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1252.679199, mean_absolute_error: 2.949994, mean_q: 0.813053\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8042/10000: episode: 1767, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 964.898438, mean_absolute_error: 2.296699, mean_q: 0.773137\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8048/10000: episode: 1768, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1304.600952, mean_absolute_error: 3.054246, mean_q: 0.804669\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8053/10000: episode: 1769, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [4.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 973.778137, mean_absolute_error: 2.332426, mean_q: 0.833535\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8059/10000: episode: 1770, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 967.997559, mean_absolute_error: 2.327343, mean_q: 0.845181\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8068/10000: episode: 1771, duration: 0.033s, episode steps: 9, steps per second: 273, episode reward: -100.000, mean reward: -11.111 [-100.000, 0.000], mean action: 4.222 [0.000, 8.000], mean observation: 0.811 [0.000, 2.000], loss: 923.121948, mean_absolute_error: 2.224470, mean_q: 0.878818\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8073/10000: episode: 1772, duration: 0.019s, episode steps: 5, steps per second: 262, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1003.777344, mean_absolute_error: 2.402700, mean_q: 0.894635\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8081/10000: episode: 1773, duration: 0.029s, episode steps: 8, steps per second: 275, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.500 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 1154.887451, mean_absolute_error: 2.738083, mean_q: 0.894773\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8086/10000: episode: 1774, duration: 0.021s, episode steps: 5, steps per second: 241, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1314.553467, mean_absolute_error: 3.071035, mean_q: 0.896213\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8091/10000: episode: 1775, duration: 0.022s, episode steps: 5, steps per second: 223, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.400 [0.000, 7.000], mean observation: 0.520 [0.000, 2.000], loss: 1002.594543, mean_absolute_error: 2.402035, mean_q: 0.877482\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8097/10000: episode: 1776, duration: 0.023s, episode steps: 6, steps per second: 257, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 862.861023, mean_absolute_error: 2.085118, mean_q: 0.893937\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8101/10000: episode: 1777, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 904.733704, mean_absolute_error: 2.211517, mean_q: 0.879032\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8106/10000: episode: 1778, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.800 [1.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 1131.821899, mean_absolute_error: 2.718665, mean_q: 0.885892\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8110/10000: episode: 1779, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 939.300171, mean_absolute_error: 2.252265, mean_q: 0.863971\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8117/10000: episode: 1780, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.714 [1.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1139.464844, mean_absolute_error: 2.674241, mean_q: 0.885258\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8124/10000: episode: 1781, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.714 [2.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 962.201843, mean_absolute_error: 2.305290, mean_q: 0.899756\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8126/10000: episode: 1782, duration: 0.010s, episode steps: 2, steps per second: 208, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 1487.676270, mean_absolute_error: 3.471155, mean_q: 0.887751\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8132/10000: episode: 1783, duration: 0.022s, episode steps: 6, steps per second: 268, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1128.689331, mean_absolute_error: 2.721702, mean_q: 0.877387\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8137/10000: episode: 1784, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1283.965576, mean_absolute_error: 3.025673, mean_q: 0.864923\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8142/10000: episode: 1785, duration: 0.021s, episode steps: 5, steps per second: 241, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 846.579407, mean_absolute_error: 2.050670, mean_q: 0.805376\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8145/10000: episode: 1786, duration: 0.014s, episode steps: 3, steps per second: 211, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [5.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 836.658691, mean_absolute_error: 2.024666, mean_q: 0.823552\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8151/10000: episode: 1787, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 3.000 [0.000, 6.000], mean observation: 0.700 [0.000, 2.000], loss: 911.688293, mean_absolute_error: 2.180434, mean_q: 0.803801\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8159/10000: episode: 1788, duration: 0.030s, episode steps: 8, steps per second: 270, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.125 [0.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 999.338135, mean_absolute_error: 2.392644, mean_q: 0.812818\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8165/10000: episode: 1789, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1020.826416, mean_absolute_error: 2.457678, mean_q: 0.806362\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8170/10000: episode: 1790, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 5.000], mean observation: 0.600 [0.000, 2.000], loss: 845.230469, mean_absolute_error: 2.046293, mean_q: 0.804012\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8176/10000: episode: 1791, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1072.086670, mean_absolute_error: 2.571705, mean_q: 0.867883\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8179/10000: episode: 1792, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 833.590332, mean_absolute_error: 2.016977, mean_q: 0.867427\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8183/10000: episode: 1793, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 863.363159, mean_absolute_error: 2.095560, mean_q: 0.868877\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8187/10000: episode: 1794, duration: 0.018s, episode steps: 4, steps per second: 222, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1098.516846, mean_absolute_error: 2.632464, mean_q: 0.886398\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8194/10000: episode: 1795, duration: 0.031s, episode steps: 7, steps per second: 227, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 5.286 [2.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 961.715454, mean_absolute_error: 2.291891, mean_q: 0.863669\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8199/10000: episode: 1796, duration: 0.020s, episode steps: 5, steps per second: 244, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1220.337280, mean_absolute_error: 2.866948, mean_q: 0.896675\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8204/10000: episode: 1797, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 970.232605, mean_absolute_error: 2.321106, mean_q: 0.882669\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8208/10000: episode: 1798, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1447.854492, mean_absolute_error: 3.376942, mean_q: 0.851017\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8212/10000: episode: 1799, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.500 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 551.525085, mean_absolute_error: 1.429338, mean_q: 0.870404\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8220/10000: episode: 1800, duration: 0.029s, episode steps: 8, steps per second: 275, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.625 [1.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 1157.821045, mean_absolute_error: 2.760023, mean_q: 0.885995\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8224/10000: episode: 1801, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 942.573730, mean_absolute_error: 2.286791, mean_q: 0.874846\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8228/10000: episode: 1802, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 903.374207, mean_absolute_error: 2.205374, mean_q: 0.869162\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8234/10000: episode: 1803, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1173.672729, mean_absolute_error: 2.769103, mean_q: 0.866149\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8240/10000: episode: 1804, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1069.919434, mean_absolute_error: 2.548205, mean_q: 0.850564\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8246/10000: episode: 1805, duration: 0.023s, episode steps: 6, steps per second: 256, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 810.678650, mean_absolute_error: 1.987070, mean_q: 0.827473\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8251/10000: episode: 1806, duration: 0.023s, episode steps: 5, steps per second: 216, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 877.526184, mean_absolute_error: 2.136608, mean_q: 0.856585\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8257/10000: episode: 1807, duration: 0.024s, episode steps: 6, steps per second: 254, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 938.890869, mean_absolute_error: 2.254398, mean_q: 0.818904\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8260/10000: episode: 1808, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 731.509827, mean_absolute_error: 1.804904, mean_q: 0.807299\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8267/10000: episode: 1809, duration: 0.026s, episode steps: 7, steps per second: 266, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.857 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1139.612061, mean_absolute_error: 2.700548, mean_q: 0.805592\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8271/10000: episode: 1810, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 824.928101, mean_absolute_error: 2.018276, mean_q: 0.820800\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8273/10000: episode: 1811, duration: 0.010s, episode steps: 2, steps per second: 204, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.300 [0.000, 2.000], loss: 1337.096924, mean_absolute_error: 3.206422, mean_q: 0.802922\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8278/10000: episode: 1812, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 880.986145, mean_absolute_error: 2.149393, mean_q: 0.811559\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8282/10000: episode: 1813, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 939.944458, mean_absolute_error: 2.260124, mean_q: 0.806911\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8290/10000: episode: 1814, duration: 0.031s, episode steps: 8, steps per second: 255, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.625 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1076.334229, mean_absolute_error: 2.560604, mean_q: 0.842750\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8296/10000: episode: 1815, duration: 0.024s, episode steps: 6, steps per second: 247, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 731.474854, mean_absolute_error: 1.806659, mean_q: 0.804052\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8298/10000: episode: 1816, duration: 0.012s, episode steps: 2, steps per second: 174, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 781.464233, mean_absolute_error: 1.881577, mean_q: 0.838597\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8303/10000: episode: 1817, duration: 0.022s, episode steps: 5, steps per second: 227, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1065.368286, mean_absolute_error: 2.548270, mean_q: 0.793340\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8309/10000: episode: 1818, duration: 0.025s, episode steps: 6, steps per second: 244, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1095.540405, mean_absolute_error: 2.593464, mean_q: 0.822309\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8317/10000: episode: 1819, duration: 0.030s, episode steps: 8, steps per second: 264, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.875 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 1115.708740, mean_absolute_error: 2.657115, mean_q: 0.819044\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8321/10000: episode: 1820, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1370.684448, mean_absolute_error: 3.223585, mean_q: 0.822968\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8325/10000: episode: 1821, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1290.733765, mean_absolute_error: 3.032475, mean_q: 0.796925\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8329/10000: episode: 1822, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1138.843262, mean_absolute_error: 2.737719, mean_q: 0.807979\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8336/10000: episode: 1823, duration: 0.026s, episode steps: 7, steps per second: 267, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 6.000], mean observation: 0.657 [0.000, 2.000], loss: 1119.757935, mean_absolute_error: 2.664015, mean_q: 0.795726\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8342/10000: episode: 1824, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1303.830933, mean_absolute_error: 3.045896, mean_q: 0.786857\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8344/10000: episode: 1825, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 940.796509, mean_absolute_error: 2.259857, mean_q: 0.800190\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8350/10000: episode: 1826, duration: 0.027s, episode steps: 6, steps per second: 224, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 861.495117, mean_absolute_error: 2.089603, mean_q: 0.786786\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8354/10000: episode: 1827, duration: 0.017s, episode steps: 4, steps per second: 237, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 939.912354, mean_absolute_error: 2.251738, mean_q: 0.801122\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8359/10000: episode: 1828, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1035.200317, mean_absolute_error: 2.484750, mean_q: 0.771639\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8364/10000: episode: 1829, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1379.816040, mean_absolute_error: 3.245129, mean_q: 0.810203\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8367/10000: episode: 1830, duration: 0.014s, episode steps: 3, steps per second: 219, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 989.745422, mean_absolute_error: 2.335830, mean_q: 0.803871\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8369/10000: episode: 1831, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.300 [0.000, 2.000], loss: 863.008667, mean_absolute_error: 2.084293, mean_q: 0.789439\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8373/10000: episode: 1832, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1447.054199, mean_absolute_error: 3.373352, mean_q: 0.805270\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8378/10000: episode: 1833, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1006.309387, mean_absolute_error: 2.435466, mean_q: 0.775048\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8382/10000: episode: 1834, duration: 0.016s, episode steps: 4, steps per second: 243, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [2.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 785.744995, mean_absolute_error: 1.934297, mean_q: 0.782346\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8385/10000: episode: 1835, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1045.401855, mean_absolute_error: 2.482273, mean_q: 0.781451\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8391/10000: episode: 1836, duration: 0.024s, episode steps: 6, steps per second: 255, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.833 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 966.684082, mean_absolute_error: 2.327344, mean_q: 0.812889\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8394/10000: episode: 1837, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 939.995361, mean_absolute_error: 2.247102, mean_q: 0.836574\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8399/10000: episode: 1838, duration: 0.022s, episode steps: 5, steps per second: 224, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1316.187256, mean_absolute_error: 3.102747, mean_q: 0.875417\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8404/10000: episode: 1839, duration: 0.023s, episode steps: 5, steps per second: 220, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.200 [4.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1282.710693, mean_absolute_error: 3.001982, mean_q: 0.875708\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8410/10000: episode: 1840, duration: 0.024s, episode steps: 6, steps per second: 253, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1303.282104, mean_absolute_error: 3.043030, mean_q: 0.858690\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8413/10000: episode: 1841, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [1.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 941.662903, mean_absolute_error: 2.278659, mean_q: 0.867706\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8417/10000: episode: 1842, duration: 0.017s, episode steps: 4, steps per second: 242, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 0.750 [0.000, 2.000], mean observation: 0.400 [0.000, 2.000], loss: 1017.565430, mean_absolute_error: 2.429177, mean_q: 0.852642\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8421/10000: episode: 1843, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.250 [5.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1021.285278, mean_absolute_error: 2.448239, mean_q: 0.851117\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8427/10000: episode: 1844, duration: 0.025s, episode steps: 6, steps per second: 237, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 913.673584, mean_absolute_error: 2.205549, mean_q: 0.868853\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8433/10000: episode: 1845, duration: 0.024s, episode steps: 6, steps per second: 254, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1017.310242, mean_absolute_error: 2.425207, mean_q: 0.908347\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8437/10000: episode: 1846, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 978.345520, mean_absolute_error: 2.341672, mean_q: 0.916021\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8439/10000: episode: 1847, duration: 0.010s, episode steps: 2, steps per second: 205, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 859.655334, mean_absolute_error: 2.060633, mean_q: 0.919818\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8442/10000: episode: 1848, duration: 0.014s, episode steps: 3, steps per second: 218, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [1.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: 1146.035522, mean_absolute_error: 2.696059, mean_q: 0.908230\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8444/10000: episode: 1849, duration: 0.011s, episode steps: 2, steps per second: 186, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 1490.510254, mean_absolute_error: 3.508321, mean_q: 0.928354\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8452/10000: episode: 1850, duration: 0.035s, episode steps: 8, steps per second: 227, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.250 [0.000, 8.000], mean observation: 0.825 [0.000, 2.000], loss: 999.418091, mean_absolute_error: 2.396233, mean_q: 0.896281\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8456/10000: episode: 1851, duration: 0.017s, episode steps: 4, steps per second: 234, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 784.757385, mean_absolute_error: 1.897534, mean_q: 0.897871\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8460/10000: episode: 1852, duration: 0.017s, episode steps: 4, steps per second: 230, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1291.542236, mean_absolute_error: 3.024484, mean_q: 0.905847\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8464/10000: episode: 1853, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1371.258179, mean_absolute_error: 3.213619, mean_q: 0.902791\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8471/10000: episode: 1854, duration: 0.027s, episode steps: 7, steps per second: 262, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.143 [1.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 964.695984, mean_absolute_error: 2.334703, mean_q: 0.910678\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8476/10000: episode: 1855, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.600 [1.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 909.615417, mean_absolute_error: 2.202074, mean_q: 0.908996\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8484/10000: episode: 1856, duration: 0.030s, episode steps: 8, steps per second: 263, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.875 [1.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 1056.790405, mean_absolute_error: 2.517750, mean_q: 0.902015\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8488/10000: episode: 1857, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1020.940674, mean_absolute_error: 2.458062, mean_q: 0.927455\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8495/10000: episode: 1858, duration: 0.026s, episode steps: 7, steps per second: 270, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1027.922974, mean_absolute_error: 2.450722, mean_q: 0.870939\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8498/10000: episode: 1859, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [2.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 733.568298, mean_absolute_error: 1.821018, mean_q: 0.820398\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8503/10000: episode: 1860, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.620 [0.000, 2.000], loss: 876.598633, mean_absolute_error: 2.106795, mean_q: 0.806826\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8506/10000: episode: 1861, duration: 0.015s, episode steps: 3, steps per second: 195, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 989.805481, mean_absolute_error: 2.354645, mean_q: 0.792059\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8512/10000: episode: 1862, duration: 0.025s, episode steps: 6, steps per second: 236, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.500 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 993.107239, mean_absolute_error: 2.387148, mean_q: 0.799195\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8516/10000: episode: 1863, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1058.925781, mean_absolute_error: 2.529573, mean_q: 0.850160\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8521/10000: episode: 1864, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 849.210571, mean_absolute_error: 2.078530, mean_q: 0.779027\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8526/10000: episode: 1865, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1128.960815, mean_absolute_error: 2.689993, mean_q: 0.801114\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8529/10000: episode: 1866, duration: 0.013s, episode steps: 3, steps per second: 223, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 838.806641, mean_absolute_error: 2.058735, mean_q: 0.788765\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8532/10000: episode: 1867, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: 989.797180, mean_absolute_error: 2.348458, mean_q: 0.850817\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8535/10000: episode: 1868, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1102.292480, mean_absolute_error: 2.652741, mean_q: 0.856017\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8543/10000: episode: 1869, duration: 0.030s, episode steps: 8, steps per second: 265, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 883.316528, mean_absolute_error: 2.148834, mean_q: 0.853613\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8546/10000: episode: 1870, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [0.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1356.470215, mean_absolute_error: 3.189706, mean_q: 0.848437\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8551/10000: episode: 1871, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 972.148743, mean_absolute_error: 2.345192, mean_q: 0.850014\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8558/10000: episode: 1872, duration: 0.027s, episode steps: 7, steps per second: 261, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.429 [0.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 1033.172974, mean_absolute_error: 2.490889, mean_q: 0.838683\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8562/10000: episode: 1873, duration: 0.020s, episode steps: 4, steps per second: 200, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 826.330933, mean_absolute_error: 2.025089, mean_q: 0.826991\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8568/10000: episode: 1874, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1149.818359, mean_absolute_error: 2.738927, mean_q: 0.826406\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8571/10000: episode: 1875, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: 1304.202271, mean_absolute_error: 3.075805, mean_q: 0.872799\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8574/10000: episode: 1876, duration: 0.013s, episode steps: 3, steps per second: 233, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 625.235229, mean_absolute_error: 1.542396, mean_q: 0.784658\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8578/10000: episode: 1877, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 940.888916, mean_absolute_error: 2.285441, mean_q: 0.818998\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8584/10000: episode: 1878, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [1.000, 6.000], mean observation: 0.567 [0.000, 2.000], loss: 891.480225, mean_absolute_error: 2.181477, mean_q: 0.815944\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8588/10000: episode: 1879, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1057.427490, mean_absolute_error: 2.520564, mean_q: 0.817111\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8594/10000: episode: 1880, duration: 0.023s, episode steps: 6, steps per second: 264, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 967.966980, mean_absolute_error: 2.341459, mean_q: 0.840068\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8598/10000: episode: 1881, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 901.020630, mean_absolute_error: 2.169683, mean_q: 0.812062\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8605/10000: episode: 1882, duration: 0.027s, episode steps: 7, steps per second: 261, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.857 [0.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 873.446106, mean_absolute_error: 2.116769, mean_q: 0.833483\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8608/10000: episode: 1883, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 991.865906, mean_absolute_error: 2.367595, mean_q: 0.813486\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8614/10000: episode: 1884, duration: 0.027s, episode steps: 6, steps per second: 226, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.833 [3.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 990.346008, mean_absolute_error: 2.356295, mean_q: 0.835214\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8618/10000: episode: 1885, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1211.165283, mean_absolute_error: 2.845499, mean_q: 0.848436\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8623/10000: episode: 1886, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1002.736145, mean_absolute_error: 2.408610, mean_q: 0.838725\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8629/10000: episode: 1887, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [1.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 942.947510, mean_absolute_error: 2.298594, mean_q: 0.845220\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8637/10000: episode: 1888, duration: 0.031s, episode steps: 8, steps per second: 260, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 2.750 [0.000, 7.000], mean observation: 0.825 [0.000, 2.000], loss: 1076.778687, mean_absolute_error: 2.565763, mean_q: 0.803710\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8642/10000: episode: 1889, duration: 0.021s, episode steps: 5, steps per second: 239, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1005.102539, mean_absolute_error: 2.427056, mean_q: 0.852866\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8651/10000: episode: 1890, duration: 0.033s, episode steps: 9, steps per second: 274, episode reward: -20.000, mean reward: -2.222 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.922 [0.000, 2.000], loss: 764.829346, mean_absolute_error: 1.866860, mean_q: 0.816207\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8656/10000: episode: 1891, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1002.829895, mean_absolute_error: 2.396679, mean_q: 0.799136\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8659/10000: episode: 1892, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [0.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1356.474609, mean_absolute_error: 3.189473, mean_q: 0.835512\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8663/10000: episode: 1893, duration: 0.022s, episode steps: 4, steps per second: 180, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1018.808960, mean_absolute_error: 2.442424, mean_q: 0.783804\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8668/10000: episode: 1894, duration: 0.021s, episode steps: 5, steps per second: 244, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.800 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1065.222412, mean_absolute_error: 2.543067, mean_q: 0.808083\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8671/10000: episode: 1895, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [0.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 678.238831, mean_absolute_error: 1.670305, mean_q: 0.767680\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8676/10000: episode: 1896, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.800 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 977.229614, mean_absolute_error: 2.398847, mean_q: 0.818473\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8680/10000: episode: 1897, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [2.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 784.425049, mean_absolute_error: 1.913099, mean_q: 0.831686\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8688/10000: episode: 1898, duration: 0.031s, episode steps: 8, steps per second: 260, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.875 [1.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 921.226440, mean_absolute_error: 2.224217, mean_q: 0.841367\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8693/10000: episode: 1899, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1002.709961, mean_absolute_error: 2.395627, mean_q: 0.848794\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8700/10000: episode: 1900, duration: 0.026s, episode steps: 7, steps per second: 271, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.771 [0.000, 2.000], loss: 1005.954529, mean_absolute_error: 2.387904, mean_q: 0.836710\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8706/10000: episode: 1901, duration: 0.023s, episode steps: 6, steps per second: 260, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.333 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 1226.198730, mean_absolute_error: 2.886194, mean_q: 0.866052\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8709/10000: episode: 1902, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [7.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1096.048462, mean_absolute_error: 2.607286, mean_q: 0.846578\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8714/10000: episode: 1903, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 971.671082, mean_absolute_error: 2.323453, mean_q: 0.794594\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8722/10000: episode: 1904, duration: 0.032s, episode steps: 8, steps per second: 248, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.125 [1.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 1113.758301, mean_absolute_error: 2.620909, mean_q: 0.832898\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8724/10000: episode: 1905, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 553.339539, mean_absolute_error: 1.437480, mean_q: 0.827334\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8727/10000: episode: 1906, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 993.086182, mean_absolute_error: 2.376561, mean_q: 0.835322\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8729/10000: episode: 1907, duration: 0.010s, episode steps: 2, steps per second: 206, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 862.385254, mean_absolute_error: 2.091622, mean_q: 0.789509\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8735/10000: episode: 1908, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1019.283691, mean_absolute_error: 2.437316, mean_q: 0.743422\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8737/10000: episode: 1909, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 781.493408, mean_absolute_error: 1.897314, mean_q: 0.758892\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8743/10000: episode: 1910, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.500 [0.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 1045.223999, mean_absolute_error: 2.505744, mean_q: 0.781620\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8747/10000: episode: 1911, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.750 [5.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1095.359619, mean_absolute_error: 2.592081, mean_q: 0.788182\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8751/10000: episode: 1912, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [5.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 822.095459, mean_absolute_error: 1.997917, mean_q: 0.751817\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8755/10000: episode: 1913, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1173.480469, mean_absolute_error: 2.769875, mean_q: 0.803089\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8760/10000: episode: 1914, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 849.330078, mean_absolute_error: 2.081678, mean_q: 0.754166\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8765/10000: episode: 1915, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1252.687256, mean_absolute_error: 2.946620, mean_q: 0.744898\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8768/10000: episode: 1916, duration: 0.014s, episode steps: 3, steps per second: 209, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [2.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1146.066040, mean_absolute_error: 2.705172, mean_q: 0.798070\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8772/10000: episode: 1917, duration: 0.019s, episode steps: 4, steps per second: 212, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 980.484558, mean_absolute_error: 2.361314, mean_q: 0.774320\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8775/10000: episode: 1918, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.667 [3.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1409.573364, mean_absolute_error: 3.293520, mean_q: 0.828280\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8782/10000: episode: 1919, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.286 [0.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1117.184326, mean_absolute_error: 2.643644, mean_q: 0.798824\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8787/10000: episode: 1920, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 845.254272, mean_absolute_error: 2.043410, mean_q: 0.802467\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8789/10000: episode: 1921, duration: 0.011s, episode steps: 2, steps per second: 185, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.300 [0.000, 2.000], loss: 706.501282, mean_absolute_error: 1.758254, mean_q: 0.754957\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8796/10000: episode: 1922, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1119.245483, mean_absolute_error: 2.657176, mean_q: 0.805265\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8799/10000: episode: 1923, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [3.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 733.765930, mean_absolute_error: 1.823714, mean_q: 0.709730\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8804/10000: episode: 1924, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 783.959961, mean_absolute_error: 1.920686, mean_q: 0.761972\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8811/10000: episode: 1925, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1186.236938, mean_absolute_error: 2.796731, mean_q: 0.794095\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8814/10000: episode: 1926, duration: 0.014s, episode steps: 3, steps per second: 220, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1044.970825, mean_absolute_error: 2.478445, mean_q: 0.797794\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8821/10000: episode: 1927, duration: 0.030s, episode steps: 7, steps per second: 231, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.571 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1364.813477, mean_absolute_error: 3.178141, mean_q: 0.785724\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8826/10000: episode: 1928, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 941.932251, mean_absolute_error: 2.270880, mean_q: 0.776984\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8829/10000: episode: 1929, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 1093.975586, mean_absolute_error: 2.582464, mean_q: 0.770917\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8833/10000: episode: 1930, duration: 0.016s, episode steps: 4, steps per second: 246, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1060.558960, mean_absolute_error: 2.545919, mean_q: 0.746550\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8838/10000: episode: 1931, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1036.351074, mean_absolute_error: 2.496799, mean_q: 0.784553\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8843/10000: episode: 1932, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1221.593506, mean_absolute_error: 2.876365, mean_q: 0.743726\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8848/10000: episode: 1933, duration: 0.020s, episode steps: 5, steps per second: 255, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1189.444580, mean_absolute_error: 2.798796, mean_q: 0.807540\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8851/10000: episode: 1934, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1099.997925, mean_absolute_error: 2.649668, mean_q: 0.755834\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8854/10000: episode: 1935, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [1.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.958374, mean_absolute_error: 2.584538, mean_q: 0.766975\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8858/10000: episode: 1936, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.750 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1059.762817, mean_absolute_error: 2.552879, mean_q: 0.791774\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8864/10000: episode: 1937, duration: 0.024s, episode steps: 6, steps per second: 249, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 913.751648, mean_absolute_error: 2.201392, mean_q: 0.735781\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8867/10000: episode: 1938, duration: 0.017s, episode steps: 3, steps per second: 179, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 940.653320, mean_absolute_error: 2.260139, mean_q: 0.757060\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8874/10000: episode: 1939, duration: 0.029s, episode steps: 7, steps per second: 245, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.429 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 917.257507, mean_absolute_error: 2.201406, mean_q: 0.753796\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8878/10000: episode: 1940, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 942.942993, mean_absolute_error: 2.272960, mean_q: 0.729254\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8885/10000: episode: 1941, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 961.856873, mean_absolute_error: 2.303394, mean_q: 0.777520\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8891/10000: episode: 1942, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1253.932617, mean_absolute_error: 2.960500, mean_q: 0.757864\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8893/10000: episode: 1943, duration: 0.010s, episode steps: 2, steps per second: 201, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.300 [0.000, 2.000], loss: 937.688232, mean_absolute_error: 2.224930, mean_q: 0.799419\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8896/10000: episode: 1944, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 792.225281, mean_absolute_error: 1.986169, mean_q: 0.731149\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8902/10000: episode: 1945, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.167 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1070.489624, mean_absolute_error: 2.542831, mean_q: 0.726173\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8907/10000: episode: 1946, duration: 0.021s, episode steps: 5, steps per second: 239, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1251.546753, mean_absolute_error: 2.931765, mean_q: 0.762800\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8912/10000: episode: 1947, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 972.752563, mean_absolute_error: 2.341124, mean_q: 0.746874\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8916/10000: episode: 1948, duration: 0.018s, episode steps: 4, steps per second: 225, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 905.848328, mean_absolute_error: 2.225455, mean_q: 0.694235\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8922/10000: episode: 1949, duration: 0.027s, episode steps: 6, steps per second: 224, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 4.500 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 887.822754, mean_absolute_error: 2.144888, mean_q: 0.749501\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8928/10000: episode: 1950, duration: 0.022s, episode steps: 6, steps per second: 269, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 887.927734, mean_absolute_error: 2.144775, mean_q: 0.754108\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8931/10000: episode: 1951, duration: 0.013s, episode steps: 3, steps per second: 236, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.667 [7.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1304.541504, mean_absolute_error: 3.060268, mean_q: 0.759564\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8936/10000: episode: 1952, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1162.827148, mean_absolute_error: 2.786020, mean_q: 0.774375\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8939/10000: episode: 1953, duration: 0.013s, episode steps: 3, steps per second: 237, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 783.549133, mean_absolute_error: 1.911022, mean_q: 0.767648\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8943/10000: episode: 1954, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [4.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1094.724976, mean_absolute_error: 2.578322, mean_q: 0.793748\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8947/10000: episode: 1955, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1057.387573, mean_absolute_error: 2.511759, mean_q: 0.757922\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8951/10000: episode: 1956, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1172.835205, mean_absolute_error: 2.753122, mean_q: 0.726839\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8954/10000: episode: 1957, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [0.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1207.987549, mean_absolute_error: 2.914699, mean_q: 0.791815\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8961/10000: episode: 1958, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.571 [2.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1050.366943, mean_absolute_error: 2.495723, mean_q: 0.789133\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8966/10000: episode: 1959, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 876.592285, mean_absolute_error: 2.112469, mean_q: 0.779090\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8971/10000: episode: 1960, duration: 0.024s, episode steps: 5, steps per second: 211, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1187.684204, mean_absolute_error: 2.781656, mean_q: 0.765942\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8977/10000: episode: 1961, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.167 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1174.166138, mean_absolute_error: 2.784220, mean_q: 0.777197\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8980/10000: episode: 1962, duration: 0.013s, episode steps: 3, steps per second: 236, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [1.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1199.981934, mean_absolute_error: 2.829262, mean_q: 0.780151\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8987/10000: episode: 1963, duration: 0.026s, episode steps: 7, steps per second: 273, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.857 [0.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 693.959839, mean_absolute_error: 1.709846, mean_q: 0.771206\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 8995/10000: episode: 1964, duration: 0.029s, episode steps: 8, steps per second: 278, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.375 [1.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 1076.065430, mean_absolute_error: 2.556068, mean_q: 0.771932\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9002/10000: episode: 1965, duration: 0.026s, episode steps: 7, steps per second: 270, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.714 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1120.747192, mean_absolute_error: 2.681170, mean_q: 0.771860\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9009/10000: episode: 1966, duration: 0.025s, episode steps: 7, steps per second: 276, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1140.730713, mean_absolute_error: 2.699990, mean_q: 0.775167\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9013/10000: episode: 1967, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [2.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 901.648071, mean_absolute_error: 2.179830, mean_q: 0.717542\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9016/10000: episode: 1968, duration: 0.013s, episode steps: 3, steps per second: 236, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 991.862000, mean_absolute_error: 2.373210, mean_q: 0.743546\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9019/10000: episode: 1969, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [2.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 629.661560, mean_absolute_error: 1.600559, mean_q: 0.831366\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9027/10000: episode: 1970, duration: 0.035s, episode steps: 8, steps per second: 225, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 4.375 [1.000, 7.000], mean observation: 0.725 [0.000, 2.000], loss: 1194.906006, mean_absolute_error: 2.827089, mean_q: 0.766409\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9032/10000: episode: 1971, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1095.185303, mean_absolute_error: 2.593202, mean_q: 0.738362\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9038/10000: episode: 1972, duration: 0.023s, episode steps: 6, steps per second: 261, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.167 [3.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1044.886841, mean_absolute_error: 2.499061, mean_q: 0.751307\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9045/10000: episode: 1973, duration: 0.026s, episode steps: 7, steps per second: 274, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 3.143 [0.000, 7.000], mean observation: 0.771 [0.000, 2.000], loss: 1096.650757, mean_absolute_error: 2.616989, mean_q: 0.791125\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9047/10000: episode: 1974, duration: 0.010s, episode steps: 2, steps per second: 196, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 553.395996, mean_absolute_error: 1.439528, mean_q: 0.794305\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9050/10000: episode: 1975, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.333 [6.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1093.947144, mean_absolute_error: 2.580711, mean_q: 0.754588\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9054/10000: episode: 1976, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.000 [2.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 976.800903, mean_absolute_error: 2.331223, mean_q: 0.779183\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9062/10000: episode: 1977, duration: 0.029s, episode steps: 8, steps per second: 280, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.875 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 804.126831, mean_absolute_error: 1.969081, mean_q: 0.738576\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9066/10000: episode: 1978, duration: 0.016s, episode steps: 4, steps per second: 253, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1056.302734, mean_absolute_error: 2.509704, mean_q: 0.750563\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9069/10000: episode: 1979, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 834.607849, mean_absolute_error: 2.009982, mean_q: 0.739564\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9073/10000: episode: 1980, duration: 0.018s, episode steps: 4, steps per second: 220, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 979.744690, mean_absolute_error: 2.359090, mean_q: 0.766079\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9079/10000: episode: 1981, duration: 0.026s, episode steps: 6, steps per second: 235, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1095.944092, mean_absolute_error: 2.608484, mean_q: 0.771757\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9083/10000: episode: 1982, duration: 0.017s, episode steps: 4, steps per second: 241, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [3.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1176.952637, mean_absolute_error: 2.808884, mean_q: 0.770430\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9090/10000: episode: 1983, duration: 0.026s, episode steps: 7, steps per second: 273, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 940.307007, mean_absolute_error: 2.262723, mean_q: 0.755513\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9095/10000: episode: 1984, duration: 0.019s, episode steps: 5, steps per second: 261, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 845.360657, mean_absolute_error: 2.041234, mean_q: 0.759793\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9098/10000: episode: 1985, duration: 0.013s, episode steps: 3, steps per second: 231, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [4.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1148.946777, mean_absolute_error: 2.718121, mean_q: 0.808200\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9101/10000: episode: 1986, duration: 0.013s, episode steps: 3, steps per second: 239, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [1.000, 3.000], mean observation: 0.433 [0.000, 2.000], loss: 989.814941, mean_absolute_error: 2.354347, mean_q: 0.763331\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9105/10000: episode: 1987, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1290.834717, mean_absolute_error: 3.030602, mean_q: 0.785574\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9109/10000: episode: 1988, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 1134.577026, mean_absolute_error: 2.665878, mean_q: 0.739336\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9111/10000: episode: 1989, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 1562.680908, mean_absolute_error: 3.620656, mean_q: 0.758085\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9115/10000: episode: 1990, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 1134.435547, mean_absolute_error: 2.688733, mean_q: 0.765321\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9118/10000: episode: 1991, duration: 0.014s, episode steps: 3, steps per second: 215, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1202.474609, mean_absolute_error: 2.858566, mean_q: 0.706545\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9125/10000: episode: 1992, duration: 0.026s, episode steps: 7, steps per second: 265, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 938.166016, mean_absolute_error: 2.237314, mean_q: 0.720811\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9130/10000: episode: 1993, duration: 0.022s, episode steps: 5, steps per second: 225, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.800 [1.000, 6.000], mean observation: 0.620 [0.000, 2.000], loss: 1066.369873, mean_absolute_error: 2.558026, mean_q: 0.762605\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9133/10000: episode: 1994, duration: 0.014s, episode steps: 3, steps per second: 216, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [1.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1096.074463, mean_absolute_error: 2.613394, mean_q: 0.763008\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9136/10000: episode: 1995, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [4.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 939.614685, mean_absolute_error: 2.266682, mean_q: 0.697416\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9142/10000: episode: 1996, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.833 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 862.910645, mean_absolute_error: 2.093438, mean_q: 0.755123\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9149/10000: episode: 1997, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.143 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1097.286499, mean_absolute_error: 2.614946, mean_q: 0.738016\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9153/10000: episode: 1998, duration: 0.018s, episode steps: 4, steps per second: 218, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1251.789307, mean_absolute_error: 2.949519, mean_q: 0.794228\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9157/10000: episode: 1999, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1021.928467, mean_absolute_error: 2.467176, mean_q: 0.758495\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9162/10000: episode: 2000, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1220.215088, mean_absolute_error: 2.879194, mean_q: 0.759036\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9167/10000: episode: 2001, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.520 [0.000, 2.000], loss: 782.721558, mean_absolute_error: 1.903134, mean_q: 0.740584\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9170/10000: episode: 2002, duration: 0.014s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 1148.928345, mean_absolute_error: 2.720424, mean_q: 0.755222\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9174/10000: episode: 2003, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1217.458984, mean_absolute_error: 2.911918, mean_q: 0.714528\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9180/10000: episode: 2004, duration: 0.028s, episode steps: 6, steps per second: 217, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 971.316589, mean_absolute_error: 2.373646, mean_q: 0.761945\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9182/10000: episode: 2005, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 1172.106323, mean_absolute_error: 2.762983, mean_q: 0.750095\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9186/10000: episode: 2006, duration: 0.016s, episode steps: 4, steps per second: 249, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.250 [0.000, 2.000], mean observation: 0.400 [0.000, 2.000], loss: 745.517944, mean_absolute_error: 1.828792, mean_q: 0.769951\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9189/10000: episode: 2007, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 2.000], mean observation: 0.333 [0.000, 2.000], loss: 789.788574, mean_absolute_error: 1.984588, mean_q: 0.716598\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9197/10000: episode: 2008, duration: 0.030s, episode steps: 8, steps per second: 267, episode reward: -100.000, mean reward: -12.500 [-100.000, 0.000], mean action: 3.500 [0.000, 8.000], mean observation: 0.725 [0.000, 2.000], loss: 826.380554, mean_absolute_error: 2.029698, mean_q: 0.721203\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9200/10000: episode: 2009, duration: 0.013s, episode steps: 3, steps per second: 235, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1198.118652, mean_absolute_error: 2.816031, mean_q: 0.740470\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9203/10000: episode: 2010, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 833.531006, mean_absolute_error: 1.998934, mean_q: 0.704349\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9206/10000: episode: 2011, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.667 [6.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1145.989502, mean_absolute_error: 2.682368, mean_q: 0.711995\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9212/10000: episode: 2012, duration: 0.024s, episode steps: 6, steps per second: 252, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [1.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1200.157349, mean_absolute_error: 2.839191, mean_q: 0.767401\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9217/10000: episode: 2013, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.800 [2.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 814.021423, mean_absolute_error: 1.972084, mean_q: 0.741019\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9221/10000: episode: 2014, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1016.532715, mean_absolute_error: 2.408051, mean_q: 0.776242\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9226/10000: episode: 2015, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 907.086060, mean_absolute_error: 2.166393, mean_q: 0.777844\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9233/10000: episode: 2016, duration: 0.028s, episode steps: 7, steps per second: 253, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1096.510986, mean_absolute_error: 2.601102, mean_q: 0.789712\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9239/10000: episode: 2017, duration: 0.026s, episode steps: 6, steps per second: 227, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.567 [0.000, 2.000], loss: 860.626404, mean_absolute_error: 2.071077, mean_q: 0.755548\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9244/10000: episode: 2018, duration: 0.020s, episode steps: 5, steps per second: 254, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1162.568359, mean_absolute_error: 2.780343, mean_q: 0.778563\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9249/10000: episode: 2019, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 1126.361084, mean_absolute_error: 2.668479, mean_q: 0.745270\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9252/10000: episode: 2020, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.000 [0.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 993.983215, mean_absolute_error: 2.404494, mean_q: 0.761769\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9256/10000: episode: 2021, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [3.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1019.890442, mean_absolute_error: 2.447278, mean_q: 0.758739\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9260/10000: episode: 2022, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 1021.119751, mean_absolute_error: 2.453833, mean_q: 0.741334\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9263/10000: episode: 2023, duration: 0.013s, episode steps: 3, steps per second: 225, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [2.000, 3.000], mean observation: 0.333 [0.000, 2.000], loss: 1096.078491, mean_absolute_error: 2.605796, mean_q: 0.761174\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9268/10000: episode: 2024, duration: 0.020s, episode steps: 5, steps per second: 248, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.400 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1157.715332, mean_absolute_error: 2.739605, mean_q: 0.761245\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9271/10000: episode: 2025, duration: 0.014s, episode steps: 3, steps per second: 217, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 993.733826, mean_absolute_error: 2.390980, mean_q: 0.726324\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9278/10000: episode: 2026, duration: 0.027s, episode steps: 7, steps per second: 260, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [1.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 851.506775, mean_absolute_error: 2.061563, mean_q: 0.746629\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9282/10000: episode: 2027, duration: 0.018s, episode steps: 4, steps per second: 227, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 747.102661, mean_absolute_error: 1.851965, mean_q: 0.750453\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9289/10000: episode: 2028, duration: 0.032s, episode steps: 7, steps per second: 222, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.857 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1186.454102, mean_absolute_error: 2.816289, mean_q: 0.758345\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9294/10000: episode: 2029, duration: 0.021s, episode steps: 5, steps per second: 243, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 753.847473, mean_absolute_error: 1.853333, mean_q: 0.756770\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9298/10000: episode: 2030, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1054.887451, mean_absolute_error: 2.491687, mean_q: 0.773088\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9301/10000: episode: 2031, duration: 0.013s, episode steps: 3, steps per second: 230, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [4.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 991.874512, mean_absolute_error: 2.373425, mean_q: 0.778484\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9305/10000: episode: 2032, duration: 0.017s, episode steps: 4, steps per second: 239, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.000 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1137.606079, mean_absolute_error: 2.712277, mean_q: 0.759043\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9312/10000: episode: 2033, duration: 0.027s, episode steps: 7, steps per second: 259, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.857 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 986.232727, mean_absolute_error: 2.368345, mean_q: 0.754882\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9314/10000: episode: 2034, duration: 0.010s, episode steps: 2, steps per second: 195, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 472.128601, mean_absolute_error: 1.237206, mean_q: 0.796065\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9320/10000: episode: 2035, duration: 0.023s, episode steps: 6, steps per second: 257, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1094.023682, mean_absolute_error: 2.574988, mean_q: 0.766196\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9323/10000: episode: 2036, duration: 0.014s, episode steps: 3, steps per second: 215, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1096.010620, mean_absolute_error: 2.603398, mean_q: 0.772020\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9330/10000: episode: 2037, duration: 0.028s, episode steps: 7, steps per second: 252, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.429 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 942.179749, mean_absolute_error: 2.287499, mean_q: 0.783576\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9335/10000: episode: 2038, duration: 0.023s, episode steps: 5, steps per second: 214, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.200 [0.000, 4.000], mean observation: 0.600 [0.000, 2.000], loss: 1097.710205, mean_absolute_error: 2.624555, mean_q: 0.798722\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9337/10000: episode: 2039, duration: 0.011s, episode steps: 2, steps per second: 180, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.300 [0.000, 2.000], loss: 1253.465088, mean_absolute_error: 2.958044, mean_q: 0.772545\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9341/10000: episode: 2040, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [0.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 979.575806, mean_absolute_error: 2.347726, mean_q: 0.763393\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9346/10000: episode: 2041, duration: 0.020s, episode steps: 5, steps per second: 253, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 687.721985, mean_absolute_error: 1.683641, mean_q: 0.729888\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9350/10000: episode: 2042, duration: 0.017s, episode steps: 4, steps per second: 232, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [1.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 942.254883, mean_absolute_error: 2.290194, mean_q: 0.774918\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9353/10000: episode: 2043, duration: 0.015s, episode steps: 3, steps per second: 206, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 991.892273, mean_absolute_error: 2.376582, mean_q: 0.800260\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9358/10000: episode: 2044, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.400 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1282.692505, mean_absolute_error: 3.010515, mean_q: 0.752268\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9360/10000: episode: 2045, duration: 0.010s, episode steps: 2, steps per second: 194, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 1.000 [1.000, 1.000], mean observation: 0.200 [0.000, 1.000], loss: 862.402710, mean_absolute_error: 2.094398, mean_q: 0.731352\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9367/10000: episode: 2046, duration: 0.027s, episode steps: 7, steps per second: 263, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.429 [0.000, 6.000], mean observation: 0.757 [0.000, 2.000], loss: 1140.393799, mean_absolute_error: 2.701321, mean_q: 0.783603\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9370/10000: episode: 2047, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 7.000 [5.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 1093.973511, mean_absolute_error: 2.582821, mean_q: 0.786997\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9374/10000: episode: 2048, duration: 0.016s, episode steps: 4, steps per second: 244, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1289.258545, mean_absolute_error: 3.011078, mean_q: 0.771110\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9379/10000: episode: 2049, duration: 0.021s, episode steps: 5, steps per second: 233, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 6.400 [4.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1100.191406, mean_absolute_error: 2.632335, mean_q: 0.745287\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9385/10000: episode: 2050, duration: 0.027s, episode steps: 6, steps per second: 226, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.000 [1.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1073.551758, mean_absolute_error: 2.581702, mean_q: 0.765930\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9390/10000: episode: 2051, duration: 0.023s, episode steps: 5, steps per second: 221, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1284.026855, mean_absolute_error: 3.022820, mean_q: 0.770813\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9392/10000: episode: 2052, duration: 0.010s, episode steps: 2, steps per second: 193, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.200 [0.000, 1.000], loss: 553.229736, mean_absolute_error: 1.439140, mean_q: 0.692895\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9399/10000: episode: 2053, duration: 0.027s, episode steps: 7, steps per second: 262, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.571 [0.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1097.518433, mean_absolute_error: 2.620731, mean_q: 0.757762\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9404/10000: episode: 2054, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.800 [0.000, 7.000], mean observation: 0.520 [0.000, 2.000], loss: 1320.343018, mean_absolute_error: 3.146086, mean_q: 0.739087\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9406/10000: episode: 2055, duration: 0.010s, episode steps: 2, steps per second: 193, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.200 [0.000, 1.000], loss: 1097.109253, mean_absolute_error: 2.618048, mean_q: 0.743779\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9411/10000: episode: 2056, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 5.200 [1.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1127.828735, mean_absolute_error: 2.676764, mean_q: 0.758903\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9418/10000: episode: 2057, duration: 0.027s, episode steps: 7, steps per second: 257, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 5.000 [2.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 963.071960, mean_absolute_error: 2.311004, mean_q: 0.749634\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9427/10000: episode: 2058, duration: 0.033s, episode steps: 9, steps per second: 273, episode reward: -100.000, mean reward: -11.111 [-100.000, 0.000], mean action: 3.222 [0.000, 8.000], mean observation: 0.811 [0.000, 2.000], loss: 886.491211, mean_absolute_error: 2.127996, mean_q: 0.727423\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9429/10000: episode: 2059, duration: 0.014s, episode steps: 2, steps per second: 144, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.200 [0.000, 1.000], loss: 1250.142822, mean_absolute_error: 2.910992, mean_q: 0.715904\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9432/10000: episode: 2060, duration: 0.016s, episode steps: 3, steps per second: 187, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 937.723389, mean_absolute_error: 2.241802, mean_q: 0.710693\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9437/10000: episode: 2061, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 875.191589, mean_absolute_error: 2.090981, mean_q: 0.717784\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9441/10000: episode: 2062, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.750 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1290.831787, mean_absolute_error: 3.032564, mean_q: 0.749004\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9448/10000: episode: 2063, duration: 0.026s, episode steps: 7, steps per second: 268, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 984.205688, mean_absolute_error: 2.350483, mean_q: 0.698966\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9451/10000: episode: 2064, duration: 0.013s, episode steps: 3, steps per second: 237, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.333 [6.000, 7.000], mean observation: 0.333 [0.000, 2.000], loss: 1152.455933, mean_absolute_error: 2.756642, mean_q: 0.715241\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9454/10000: episode: 2065, duration: 0.013s, episode steps: 3, steps per second: 226, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [3.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1358.325806, mean_absolute_error: 3.205618, mean_q: 0.767881\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9461/10000: episode: 2066, duration: 0.026s, episode steps: 7, steps per second: 267, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 3.857 [0.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1095.638916, mean_absolute_error: 2.596837, mean_q: 0.728654\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9465/10000: episode: 2067, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 978.479614, mean_absolute_error: 2.337078, mean_q: 0.711794\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9471/10000: episode: 2068, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.167 [2.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 887.914795, mean_absolute_error: 2.134171, mean_q: 0.737718\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9475/10000: episode: 2069, duration: 0.017s, episode steps: 4, steps per second: 235, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.500 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1017.378906, mean_absolute_error: 2.421218, mean_q: 0.731304\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9481/10000: episode: 2070, duration: 0.027s, episode steps: 6, steps per second: 220, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 939.767517, mean_absolute_error: 2.252860, mean_q: 0.736605\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9488/10000: episode: 2071, duration: 0.026s, episode steps: 7, steps per second: 269, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.143 [1.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 850.087830, mean_absolute_error: 2.047214, mean_q: 0.725930\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9494/10000: episode: 2072, duration: 0.024s, episode steps: 6, steps per second: 251, episode reward: -20.000, mean reward: -3.333 [-20.000, 0.000], mean action: 3.333 [0.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 863.618164, mean_absolute_error: 2.105951, mean_q: 0.724380\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9497/10000: episode: 2073, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.667 [3.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 1046.055298, mean_absolute_error: 2.516343, mean_q: 0.768102\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9502/10000: episode: 2074, duration: 0.020s, episode steps: 5, steps per second: 250, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1000.833191, mean_absolute_error: 2.378259, mean_q: 0.731107\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9506/10000: episode: 2075, duration: 0.016s, episode steps: 4, steps per second: 250, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 1.250 [0.000, 4.000], mean observation: 0.400 [0.000, 2.000], loss: 1174.885376, mean_absolute_error: 2.784204, mean_q: 0.747070\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9508/10000: episode: 2076, duration: 0.010s, episode steps: 2, steps per second: 196, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 7.000 [7.000, 7.000], mean observation: 0.200 [0.000, 1.000], loss: 1175.172607, mean_absolute_error: 2.778001, mean_q: 0.784895\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9514/10000: episode: 2077, duration: 0.023s, episode steps: 6, steps per second: 265, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 1461.720581, mean_absolute_error: 3.415774, mean_q: 0.768697\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9519/10000: episode: 2078, duration: 0.020s, episode steps: 5, steps per second: 245, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1157.826904, mean_absolute_error: 2.727708, mean_q: 0.800725\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9526/10000: episode: 2079, duration: 0.027s, episode steps: 7, steps per second: 256, episode reward: -20.000, mean reward: -2.857 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.771 [0.000, 2.000], loss: 1096.949219, mean_absolute_error: 2.606987, mean_q: 0.750765\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9531/10000: episode: 2080, duration: 0.022s, episode steps: 5, steps per second: 223, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 595.229858, mean_absolute_error: 1.488350, mean_q: 0.778511\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9536/10000: episode: 2081, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1285.662109, mean_absolute_error: 3.025687, mean_q: 0.781744\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9541/10000: episode: 2082, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.200 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1004.188110, mean_absolute_error: 2.405687, mean_q: 0.808688\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9543/10000: episode: 2083, duration: 0.010s, episode steps: 2, steps per second: 208, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 1409.540283, mean_absolute_error: 3.303625, mean_q: 0.802694\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9548/10000: episode: 2084, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.800 [1.000, 7.000], mean observation: 0.520 [0.000, 2.000], loss: 1068.690186, mean_absolute_error: 2.570846, mean_q: 0.784566\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9552/10000: episode: 2085, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1017.254517, mean_absolute_error: 2.427062, mean_q: 0.768310\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9556/10000: episode: 2086, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.500 [0.000, 8.000], mean observation: 0.400 [0.000, 2.000], loss: 1056.469238, mean_absolute_error: 2.513739, mean_q: 0.797799\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9562/10000: episode: 2087, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.333 [0.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1042.926880, mean_absolute_error: 2.474538, mean_q: 0.761655\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9569/10000: episode: 2088, duration: 0.026s, episode steps: 7, steps per second: 270, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.571 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 960.023743, mean_absolute_error: 2.280584, mean_q: 0.754821\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9576/10000: episode: 2089, duration: 0.025s, episode steps: 7, steps per second: 275, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.286 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 938.761780, mean_absolute_error: 2.240864, mean_q: 0.726067\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9580/10000: episode: 2090, duration: 0.020s, episode steps: 4, steps per second: 196, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.250 [2.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 1136.784912, mean_absolute_error: 2.700016, mean_q: 0.760303\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9584/10000: episode: 2091, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 6.000 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 940.819580, mean_absolute_error: 2.266758, mean_q: 0.731675\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9588/10000: episode: 2092, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [1.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 861.169434, mean_absolute_error: 2.085033, mean_q: 0.743323\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9591/10000: episode: 2093, duration: 0.013s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.667 [0.000, 1.000], mean observation: 0.333 [0.000, 2.000], loss: 1043.750000, mean_absolute_error: 2.482540, mean_q: 0.750461\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9593/10000: episode: 2094, duration: 0.010s, episode steps: 2, steps per second: 203, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.200 [0.000, 1.000], loss: 1099.899902, mean_absolute_error: 2.639362, mean_q: 0.770092\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9598/10000: episode: 2095, duration: 0.019s, episode steps: 5, steps per second: 263, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.600 [2.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 1065.062012, mean_absolute_error: 2.530139, mean_q: 0.738729\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9601/10000: episode: 2096, duration: 0.013s, episode steps: 3, steps per second: 234, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.000 [0.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1200.204834, mean_absolute_error: 2.834256, mean_q: 0.783020\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9608/10000: episode: 2097, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.143 [1.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1074.127930, mean_absolute_error: 2.556383, mean_q: 0.773150\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9611/10000: episode: 2098, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.333 [0.000, 4.000], mean observation: 0.333 [0.000, 2.000], loss: 989.809021, mean_absolute_error: 2.356278, mean_q: 0.774839\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9617/10000: episode: 2099, duration: 0.022s, episode steps: 6, steps per second: 267, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 3.333 [0.000, 6.000], mean observation: 0.700 [0.000, 2.000], loss: 1022.807129, mean_absolute_error: 2.484689, mean_q: 0.748807\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9624/10000: episode: 2100, duration: 0.026s, episode steps: 7, steps per second: 270, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.857 [2.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 1118.849243, mean_absolute_error: 2.649233, mean_q: 0.742446\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9629/10000: episode: 2101, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.000 [0.000, 7.000], mean observation: 0.520 [0.000, 2.000], loss: 1064.088135, mean_absolute_error: 2.524180, mean_q: 0.736699\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9633/10000: episode: 2102, duration: 0.019s, episode steps: 4, steps per second: 208, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [3.000, 8.000], mean observation: 0.500 [0.000, 2.000], loss: 862.714600, mean_absolute_error: 2.097888, mean_q: 0.724298\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9638/10000: episode: 2103, duration: 0.022s, episode steps: 5, steps per second: 229, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.000 [0.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 907.590515, mean_absolute_error: 2.178893, mean_q: 0.742153\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9642/10000: episode: 2104, duration: 0.016s, episode steps: 4, steps per second: 245, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 4.000], mean observation: 0.500 [0.000, 2.000], loss: 979.157593, mean_absolute_error: 2.345364, mean_q: 0.734255\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9648/10000: episode: 2105, duration: 0.023s, episode steps: 6, steps per second: 263, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 809.492676, mean_absolute_error: 1.968660, mean_q: 0.745565\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9651/10000: episode: 2106, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 889.805237, mean_absolute_error: 2.170560, mean_q: 0.721052\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9657/10000: episode: 2107, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.667 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1048.189453, mean_absolute_error: 2.528846, mean_q: 0.745346\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9662/10000: episode: 2108, duration: 0.019s, episode steps: 5, steps per second: 260, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.200 [0.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1158.941162, mean_absolute_error: 2.745748, mean_q: 0.747326\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9668/10000: episode: 2109, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.833 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1200.095825, mean_absolute_error: 2.837536, mean_q: 0.763225\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9673/10000: episode: 2110, duration: 0.019s, episode steps: 5, steps per second: 266, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.000 [1.000, 6.000], mean observation: 0.600 [0.000, 2.000], loss: 969.571655, mean_absolute_error: 2.296450, mean_q: 0.749022\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9678/10000: episode: 2111, duration: 0.019s, episode steps: 5, steps per second: 257, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 4.600 [1.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1126.869873, mean_absolute_error: 2.661336, mean_q: 0.774101\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9681/10000: episode: 2112, duration: 0.013s, episode steps: 3, steps per second: 229, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.667 [1.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1148.113159, mean_absolute_error: 2.723924, mean_q: 0.736003\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9686/10000: episode: 2113, duration: 0.021s, episode steps: 5, steps per second: 241, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.800 [1.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 940.187500, mean_absolute_error: 2.259580, mean_q: 0.740811\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9691/10000: episode: 2114, duration: 0.023s, episode steps: 5, steps per second: 217, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 4.000 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1131.313843, mean_absolute_error: 2.716770, mean_q: 0.735691\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9694/10000: episode: 2115, duration: 0.013s, episode steps: 3, steps per second: 227, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 1.667 [1.000, 2.000], mean observation: 0.433 [0.000, 2.000], loss: 1410.375854, mean_absolute_error: 3.309125, mean_q: 0.715899\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9701/10000: episode: 2116, duration: 0.026s, episode steps: 7, steps per second: 264, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.143 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 761.110779, mean_absolute_error: 1.856555, mean_q: 0.736152\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9706/10000: episode: 2117, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [3.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1315.173828, mean_absolute_error: 3.089211, mean_q: 0.738788\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9713/10000: episode: 2118, duration: 0.026s, episode steps: 7, steps per second: 269, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.000 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 1118.479126, mean_absolute_error: 2.647964, mean_q: 0.767771\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9718/10000: episode: 2119, duration: 0.019s, episode steps: 5, steps per second: 258, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.600 [4.000, 8.000], mean observation: 0.600 [0.000, 2.000], loss: 1001.462280, mean_absolute_error: 2.389549, mean_q: 0.744365\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9724/10000: episode: 2120, duration: 0.023s, episode steps: 6, steps per second: 266, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1042.788696, mean_absolute_error: 2.471505, mean_q: 0.723791\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9728/10000: episode: 2121, duration: 0.017s, episode steps: 4, steps per second: 240, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 2.250 [0.000, 5.000], mean observation: 0.400 [0.000, 2.000], loss: 706.446350, mean_absolute_error: 1.742094, mean_q: 0.715167\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9730/10000: episode: 2122, duration: 0.010s, episode steps: 2, steps per second: 209, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.300 [0.000, 2.000], loss: 1251.099976, mean_absolute_error: 2.928818, mean_q: 0.785585\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9734/10000: episode: 2123, duration: 0.016s, episode steps: 4, steps per second: 248, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.750 [2.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 822.065857, mean_absolute_error: 1.987289, mean_q: 0.743171\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9736/10000: episode: 2124, duration: 0.011s, episode steps: 2, steps per second: 178, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 5.000 [5.000, 5.000], mean observation: 0.300 [0.000, 2.000], loss: 706.718872, mean_absolute_error: 1.732364, mean_q: 0.693130\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9741/10000: episode: 2125, duration: 0.027s, episode steps: 5, steps per second: 189, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.400 [2.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 876.454285, mean_absolute_error: 2.104876, mean_q: 0.745636\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9747/10000: episode: 2126, duration: 0.025s, episode steps: 6, steps per second: 244, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [2.000, 6.000], mean observation: 0.667 [0.000, 2.000], loss: 945.982239, mean_absolute_error: 2.309861, mean_q: 0.774426\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9752/10000: episode: 2127, duration: 0.020s, episode steps: 5, steps per second: 246, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.200 [2.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 877.081360, mean_absolute_error: 2.107840, mean_q: 0.763652\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9754/10000: episode: 2128, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 8.000 [8.000, 8.000], mean observation: 0.300 [0.000, 2.000], loss: 1565.649170, mean_absolute_error: 3.640502, mean_q: 0.815920\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9759/10000: episode: 2129, duration: 0.020s, episode steps: 5, steps per second: 247, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 4.000 [0.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1097.050415, mean_absolute_error: 2.602683, mean_q: 0.742149\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9765/10000: episode: 2130, duration: 0.023s, episode steps: 6, steps per second: 259, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.667 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 1276.751831, mean_absolute_error: 2.983291, mean_q: 0.773778\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9769/10000: episode: 2131, duration: 0.016s, episode steps: 4, steps per second: 251, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [2.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 667.394714, mean_absolute_error: 1.660981, mean_q: 0.790923\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9771/10000: episode: 2132, duration: 0.010s, episode steps: 2, steps per second: 200, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.200 [0.000, 1.000], loss: 784.268738, mean_absolute_error: 1.915298, mean_q: 0.784168\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9774/10000: episode: 2133, duration: 0.013s, episode steps: 3, steps per second: 222, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.333 [0.000, 2.000], loss: 573.116333, mean_absolute_error: 1.413540, mean_q: 0.768982\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9779/10000: episode: 2134, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.800 [0.000, 6.000], mean observation: 0.500 [0.000, 2.000], loss: 1064.076782, mean_absolute_error: 2.525231, mean_q: 0.784523\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9786/10000: episode: 2135, duration: 0.028s, episode steps: 7, steps per second: 248, episode reward: 20.000, mean reward: 2.857 [0.000, 20.000], mean action: 4.286 [1.000, 8.000], mean observation: 0.671 [0.000, 2.000], loss: 1094.736938, mean_absolute_error: 2.585421, mean_q: 0.770026\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9789/10000: episode: 2136, duration: 0.016s, episode steps: 3, steps per second: 183, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1096.014526, mean_absolute_error: 2.595134, mean_q: 0.772044\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9794/10000: episode: 2137, duration: 0.021s, episode steps: 5, steps per second: 239, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.600 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1161.556519, mean_absolute_error: 2.765296, mean_q: 0.748070\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9800/10000: episode: 2138, duration: 0.023s, episode steps: 6, steps per second: 258, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.000 [3.000, 7.000], mean observation: 0.567 [0.000, 2.000], loss: 967.919617, mean_absolute_error: 2.339992, mean_q: 0.787865\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9802/10000: episode: 2139, duration: 0.010s, episode steps: 2, steps per second: 209, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 4.000 [4.000, 4.000], mean observation: 0.200 [0.000, 1.000], loss: 1489.473389, mean_absolute_error: 3.468281, mean_q: 0.770109\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9809/10000: episode: 2140, duration: 0.027s, episode steps: 7, steps per second: 260, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.429 [1.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1074.347412, mean_absolute_error: 2.553550, mean_q: 0.781986\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9814/10000: episode: 2141, duration: 0.019s, episode steps: 5, steps per second: 259, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.600 [0.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 1096.432373, mean_absolute_error: 2.603730, mean_q: 0.713733\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9821/10000: episode: 2142, duration: 0.027s, episode steps: 7, steps per second: 260, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 2.571 [0.000, 6.000], mean observation: 0.657 [0.000, 2.000], loss: 783.620667, mean_absolute_error: 1.911560, mean_q: 0.722708\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9824/10000: episode: 2143, duration: 0.013s, episode steps: 3, steps per second: 224, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.667 [4.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 890.021667, mean_absolute_error: 2.168928, mean_q: 0.751410\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9830/10000: episode: 2144, duration: 0.024s, episode steps: 6, steps per second: 249, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.667 [0.000, 2.000], loss: 1177.270386, mean_absolute_error: 2.805011, mean_q: 0.745780\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9833/10000: episode: 2145, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [2.000, 4.000], mean observation: 0.433 [0.000, 2.000], loss: 837.710938, mean_absolute_error: 2.049401, mean_q: 0.753441\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9837/10000: episode: 2146, duration: 0.019s, episode steps: 4, steps per second: 205, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [2.000, 6.000], mean observation: 0.400 [0.000, 2.000], loss: 976.801636, mean_absolute_error: 2.323350, mean_q: 0.710139\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9841/10000: episode: 2147, duration: 0.020s, episode steps: 4, steps per second: 204, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 5.250 [4.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 981.426392, mean_absolute_error: 2.359246, mean_q: 0.756263\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9844/10000: episode: 2148, duration: 0.015s, episode steps: 3, steps per second: 202, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.333 [4.000, 6.000], mean observation: 0.433 [0.000, 2.000], loss: 1359.574585, mean_absolute_error: 3.200401, mean_q: 0.775177\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9847/10000: episode: 2149, duration: 0.014s, episode steps: 3, steps per second: 214, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [0.000, 7.000], mean observation: 0.433 [0.000, 2.000], loss: 1724.163940, mean_absolute_error: 4.003351, mean_q: 0.729541\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9852/10000: episode: 2150, duration: 0.021s, episode steps: 5, steps per second: 239, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 3.400 [1.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1002.662292, mean_absolute_error: 2.382168, mean_q: 0.707661\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9857/10000: episode: 2151, duration: 0.020s, episode steps: 5, steps per second: 251, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 3.600 [1.000, 6.000], mean observation: 0.620 [0.000, 2.000], loss: 910.326355, mean_absolute_error: 2.203766, mean_q: 0.715722\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9863/10000: episode: 2152, duration: 0.024s, episode steps: 6, steps per second: 250, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.333 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 862.624023, mean_absolute_error: 2.095575, mean_q: 0.754884\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9869/10000: episode: 2153, duration: 0.024s, episode steps: 6, steps per second: 246, episode reward: 20.000, mean reward: 3.333 [0.000, 20.000], mean action: 3.167 [0.000, 6.000], mean observation: 0.700 [0.000, 2.000], loss: 1226.710327, mean_absolute_error: 2.890396, mean_q: 0.753173\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9873/10000: episode: 2154, duration: 0.017s, episode steps: 4, steps per second: 238, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.500 [1.000, 7.000], mean observation: 0.400 [0.000, 2.000], loss: 944.483948, mean_absolute_error: 2.303001, mean_q: 0.718837\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9879/10000: episode: 2155, duration: 0.024s, episode steps: 6, steps per second: 254, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.000 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 1199.144165, mean_absolute_error: 2.824450, mean_q: 0.703070\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9883/10000: episode: 2156, duration: 0.019s, episode steps: 4, steps per second: 209, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 3.500 [1.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 978.331177, mean_absolute_error: 2.340431, mean_q: 0.734444\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9886/10000: episode: 2157, duration: 0.016s, episode steps: 3, steps per second: 184, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 3.333 [1.000, 8.000], mean observation: 0.333 [0.000, 2.000], loss: 783.317322, mean_absolute_error: 1.905516, mean_q: 0.719617\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9892/10000: episode: 2158, duration: 0.027s, episode steps: 6, steps per second: 221, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [0.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 758.529541, mean_absolute_error: 1.852893, mean_q: 0.739734\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9897/10000: episode: 2159, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 3.600 [0.000, 6.000], mean observation: 0.520 [0.000, 2.000], loss: 1132.192505, mean_absolute_error: 2.709698, mean_q: 0.733627\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9900/10000: episode: 2160, duration: 0.014s, episode steps: 3, steps per second: 211, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 2.333 [1.000, 5.000], mean observation: 0.333 [0.000, 2.000], loss: 1148.125610, mean_absolute_error: 2.723645, mean_q: 0.727285\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9905/10000: episode: 2161, duration: 0.020s, episode steps: 5, steps per second: 249, episode reward: 20.000, mean reward: 4.000 [0.000, 20.000], mean action: 6.000 [4.000, 8.000], mean observation: 0.520 [0.000, 2.000], loss: 908.740723, mean_absolute_error: 2.200854, mean_q: 0.726880\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9907/10000: episode: 2162, duration: 0.010s, episode steps: 2, steps per second: 199, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.300 [0.000, 2.000], loss: 1328.343628, mean_absolute_error: 3.113296, mean_q: 0.789125\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9910/10000: episode: 2163, duration: 0.013s, episode steps: 3, steps per second: 228, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 4.333 [3.000, 5.000], mean observation: 0.433 [0.000, 2.000], loss: 1050.020020, mean_absolute_error: 2.562261, mean_q: 0.723973\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9917/10000: episode: 2164, duration: 0.026s, episode steps: 7, steps per second: 271, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 4.000 [1.000, 8.000], mean observation: 0.657 [0.000, 2.000], loss: 876.723083, mean_absolute_error: 2.142668, mean_q: 0.738537\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9922/10000: episode: 2165, duration: 0.020s, episode steps: 5, steps per second: 252, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 5.000 [2.000, 7.000], mean observation: 0.600 [0.000, 2.000], loss: 1130.471924, mean_absolute_error: 2.710701, mean_q: 0.773597\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9930/10000: episode: 2166, duration: 0.029s, episode steps: 8, steps per second: 271, episode reward: 20.000, mean reward: 2.500 [0.000, 20.000], mean action: 3.625 [0.000, 8.000], mean observation: 0.850 [0.000, 2.000], loss: 1174.363037, mean_absolute_error: 2.778165, mean_q: 0.748160\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9932/10000: episode: 2167, duration: 0.011s, episode steps: 2, steps per second: 178, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 3.000 [3.000, 3.000], mean observation: 0.300 [0.000, 2.000], loss: 1019.003479, mean_absolute_error: 2.440455, mean_q: 0.776755\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9936/10000: episode: 2168, duration: 0.019s, episode steps: 4, steps per second: 208, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.000 [0.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1137.516602, mean_absolute_error: 2.694374, mean_q: 0.698718\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9939/10000: episode: 2169, duration: 0.014s, episode steps: 3, steps per second: 221, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 6.000 [5.000, 8.000], mean observation: 0.433 [0.000, 2.000], loss: 1043.811646, mean_absolute_error: 2.482804, mean_q: 0.713300\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9944/10000: episode: 2170, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -100.000, mean reward: -20.000 [-100.000, 0.000], mean action: 2.600 [1.000, 5.000], mean observation: 0.500 [0.000, 2.000], loss: 1007.429993, mean_absolute_error: 2.450633, mean_q: 0.737943\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9946/10000: episode: 2171, duration: 0.010s, episode steps: 2, steps per second: 198, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 2.000 [2.000, 2.000], mean observation: 0.300 [0.000, 2.000], loss: 937.704712, mean_absolute_error: 2.238742, mean_q: 0.747154\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9951/10000: episode: 2172, duration: 0.020s, episode steps: 5, steps per second: 256, episode reward: -20.000, mean reward: -4.000 [-20.000, 0.000], mean action: 5.200 [1.000, 8.000], mean observation: 0.620 [0.000, 2.000], loss: 1035.267334, mean_absolute_error: 2.471675, mean_q: 0.777277\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9958/10000: episode: 2173, duration: 0.026s, episode steps: 7, steps per second: 274, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [0.000, 7.000], mean observation: 0.757 [0.000, 2.000], loss: 1227.902710, mean_absolute_error: 2.876230, mean_q: 0.754988\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9965/10000: episode: 2174, duration: 0.027s, episode steps: 7, steps per second: 262, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.571 [0.000, 8.000], mean observation: 0.757 [0.000, 2.000], loss: 939.280945, mean_absolute_error: 2.242584, mean_q: 0.791729\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9967/10000: episode: 2175, duration: 0.010s, episode steps: 2, steps per second: 195, episode reward: -100.000, mean reward: -50.000 [-100.000, 0.000], mean action: 6.000 [6.000, 6.000], mean observation: 0.300 [0.000, 2.000], loss: 1172.070068, mean_absolute_error: 2.753027, mean_q: 0.819218\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9973/10000: episode: 2176, duration: 0.023s, episode steps: 6, steps per second: 262, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 5.500 [2.000, 8.000], mean observation: 0.667 [0.000, 2.000], loss: 839.669128, mean_absolute_error: 2.062929, mean_q: 0.785182\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9977/10000: episode: 2177, duration: 0.016s, episode steps: 4, steps per second: 247, episode reward: -100.000, mean reward: -25.000 [-100.000, 0.000], mean action: 4.250 [3.000, 7.000], mean observation: 0.500 [0.000, 2.000], loss: 1016.581055, mean_absolute_error: 2.398082, mean_q: 0.754218\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9980/10000: episode: 2178, duration: 0.013s, episode steps: 3, steps per second: 232, episode reward: -100.000, mean reward: -33.333 [-100.000, 0.000], mean action: 5.000 [3.000, 6.000], mean observation: 0.333 [0.000, 2.000], loss: 890.762939, mean_absolute_error: 2.159381, mean_q: 0.758822\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9986/10000: episode: 2179, duration: 0.027s, episode steps: 6, steps per second: 226, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 4.333 [1.000, 8.000], mean observation: 0.567 [0.000, 2.000], loss: 886.639160, mean_absolute_error: 2.122891, mean_q: 0.787588\n",
      "[2, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9992/10000: episode: 2180, duration: 0.024s, episode steps: 6, steps per second: 254, episode reward: -100.000, mean reward: -16.667 [-100.000, 0.000], mean action: 2.667 [0.000, 5.000], mean observation: 0.667 [0.000, 2.000], loss: 1279.606567, mean_absolute_error: 3.017371, mean_q: 0.808423\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " 9999/10000: episode: 2181, duration: 0.026s, episode steps: 7, steps per second: 272, episode reward: -100.000, mean reward: -14.286 [-100.000, 0.000], mean action: 3.714 [0.000, 7.000], mean observation: 0.657 [0.000, 2.000], loss: 1366.756104, mean_absolute_error: 3.215618, mean_q: 0.823476\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "done, took 41.403 seconds\n"
     ]
    }
   ],
   "source": [
    "hist = dqn.fit(env, nb_steps=10000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1362d9d8fd0>]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noM5AURRFAf4/qoeG4mCMgnMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1362ab9ad30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history.get('episode_reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected flatten_11_input to have 3 dimensions, but got array with shape (1, 4)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-195-fcc8095988d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\Dell\\OneDrive\\Github\\keras-rl\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Dell\\OneDrive\\Github\\keras-rl\\rl\\agents\\dqn.py\u001b[0m in \u001b[0;36mcompute_batch_q_values\u001b[1;34m(self, state_batch)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_batch_q_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_state_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mq_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mq_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    921\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 923\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    924\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m     def train_on_batch(self, x, y, class_weight=None,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict_on_batch\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m   1620\u001b[0m         \"\"\"\n\u001b[0;32m   1621\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m-> 1622\u001b[1;33m                                     self._feed_input_shapes)\n\u001b[0m\u001b[0;32m   1623\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muses_learning_phase\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1624\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                                  \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                                  \u001b[1;34m' dimensions, but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                                  str(array.shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_dim\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking : expected flatten_11_input to have 3 dimensions, but got array with shape (1, 4)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "dqn.compute_q_values(env.step(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(np.array(env.state).reshape(1, 1, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "+---+---+---+\n",
      "| 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "+---+---+---+\n",
      "+---+---+---+\n",
      "| 0 | 0 | 0 |\n",
      "| 0 | 1 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "+---+---+---+\n",
      "1\n",
      "+---+---+---+\n",
      "| 0 | 2 | 0 |\n",
      "| 0 | 1 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "+---+---+---+\n",
      "+---+---+---+\n",
      "| 0 | 2 | 1 |\n",
      "| 0 | 1 | 0 |\n",
      "| 0 | 0 | 0 |\n",
      "+---+---+---+\n",
      "6\n",
      "+---+---+---+\n",
      "| 0 | 2 | 1 |\n",
      "| 0 | 1 | 0 |\n",
      "| 2 | 0 | 0 |\n",
      "+---+---+---+\n",
      "+---+---+---+\n",
      "| 1 | 2 | 1 |\n",
      "| 0 | 1 | 0 |\n",
      "| 2 | 0 | 0 |\n",
      "+---+---+---+\n",
      "8\n",
      "+---+---+---+\n",
      "| 1 | 2 | 1 |\n",
      "| 0 | 1 | 0 |\n",
      "| 2 | 0 | 2 |\n",
      "+---+---+---+\n",
      "+---+---+---+\n",
      "| 1 | 2 | 1 |\n",
      "| 0 | 1 | 1 |\n",
      "| 2 | 0 | 2 |\n",
      "+---+---+---+\n",
      "7\n",
      "+---+---+---+\n",
      "| 1 | 2 | 1 |\n",
      "| 0 | 1 | 1 |\n",
      "| 2 | 2 | 2 |\n",
      "+---+---+---+\n",
      "-20\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "while(True):\n",
    "    if env.current == env.ai:\n",
    "        act = np.argmax(model.predict(np.array(env.state).reshape(1, 1, 10)))\n",
    "    else:\n",
    "        act = int(input())\n",
    "    _, reward, done, _ = env.step(act)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 2 episodes ...\n",
      "2 [[0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]]\n",
      "2 [[0 0 0]\n",
      " [1 0 2]\n",
      " [0 0 0]]\n",
      "2 [[0 0 0]\n",
      " [1 0 2]\n",
      " [1 0 0]]\n",
      "2 [[0 0 0]\n",
      " [1 2 2]\n",
      " [1 0 0]]\n",
      "2 [[1 0 0]\n",
      " [1 2 2]\n",
      " [1 0 0]]\n",
      "Episode 1: reward: -10.000, steps: 5\n",
      "2 [[0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]]\n",
      "2 [[0 0 0]\n",
      " [1 0 2]\n",
      " [0 0 0]]\n",
      "2 [[0 0 0]\n",
      " [1 0 2]\n",
      " [1 0 0]]\n",
      "2 [[0 0 0]\n",
      " [1 2 2]\n",
      " [1 0 0]]\n",
      "2 [[1 0 0]\n",
      " [1 2 2]\n",
      " [1 0 0]]\n",
      "Episode 2: reward: -10.000, steps: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14e82b1df28>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, 2, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
