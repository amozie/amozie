{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestEnv(Env):\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.__max_step = 9\n",
    "        \n",
    "        # 0-8分别代表此次落子位置\n",
    "        self.action_space = spaces.Discrete(9)\n",
    "        # 初始全零，下标0为：1、2分别代表黑白两方（1黑方先行），下标1-9为：0空、1黑方、2白方\n",
    "        self.observation_space = spaces.Box(\n",
    "            np.zeros(10, int), np.zeros(10, int) + 2)\n",
    "        # 代表AI所属方（1或2）\n",
    "        self.ai = None\n",
    "        self._seed()\n",
    "        self.state = None\n",
    "        self.__step = None\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        # 随机AI所属方\n",
    "        self.ai = np.random.choice([1, 2])\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.__step = 0\n",
    "        self.state = [self.ai]\n",
    "        self.state = self.state.extend(np.zeros(9, int).tolist())\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def _step(self, action):\n",
    "        grid = self.state[1:]\n",
    "        # 不可重复落子\n",
    "        if grid[action] != 0:\n",
    "            return np.array(self.state), -100, True, {}\n",
    "        # 黑方先手\n",
    "        grid = np.array(grid, int)\n",
    "        dif = np.sum(grid == 1) - np.sum(grid == 2)\n",
    "        if dif == 0:\n",
    "            current = 1\n",
    "        elif dif == 1:\n",
    "            current = 2\n",
    "        else:\n",
    "            return np.array(self.state), -100, True, {}\n",
    "        # 胜负\n",
    "        grid[action] = current\n",
    "        self.state = [self.ai]\n",
    "        self.state = self.state.extend(grid.tolist())\n",
    "        is_win = lambda b: b[0:3].all() or b[3:6].all() or b[6:9].all() or \\\n",
    "            b[0::3].all() or b[1::3].all() or b[2::3] or \\\n",
    "            b[0::4].all() or b[2:7:2].all()\n",
    "        if is_win(grid == current):\n",
    "            if self.ai == current:\n",
    "                reward = 10\n",
    "            else:\n",
    "                reward = -10\n",
    "            return np.array(self.state), reward, True, {}\n",
    "\n",
    "        reward = 0\n",
    "        self.__step += 1\n",
    "        if self.__step < self.__max_step:\n",
    "            done = False\n",
    "        else:\n",
    "            done = True\n",
    "        return np.array(self.state), reward, done, {}\n",
    "\n",
    "    def _render(self, mode='ansi', close=False):\n",
    "        print(self.state[0], np.array(self.state[1:], int).reshape(3, 3))\n",
    "\n",
    "    def _close(self):\n",
    "        super()._close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TestEnv()\n",
    "env.reset()\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dd0609cdbaee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0minfo\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0mauxiliary\u001b[0m \u001b[0mdiagnostic\u001b[0m \u001b[0minformation\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhelpful\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msometimes\u001b[0m \u001b[0mlearning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \"\"\"\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-acd45bee0869>\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;31m# 不可重复落子\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(env.render())\n",
    "    print(action, reward, done, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_2 (Flatten)          (None, 2)                 0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 16)                48        \n_________________________________________________________________\nactivation_4 (Activation)    (None, 16)                0         \n_________________________________________________________________\ndense_5 (Dense)              (None, 16)                272       \n_________________________________________________________________\nactivation_5 (Activation)    (None, 16)                0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 2)                 34        \n_________________________________________________________________\nactivation_6 (Activation)    (None, 2)                 0         \n=================================================================\nTotal params: 354\nTrainable params: 354\nNon-trainable params: 0\n_________________________________________________________________\nNone\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(16))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=500, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "               target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500 steps ...\n[0, 0, 0]\n[0, 0, 0, 1]\n[0, 0, 0, 1, 0]\n[0, 0, 0, 1, 0, 1]\n[0, 0, 0, 1, 0, 1, 0]\n[0, 0, 0, 1, 0, 1, 0, 1]\n[0, 0, 0, 1, 0, 1, 0, 1, 0]\n[0, 0, 0, 1, 0, 1, 0, 1, 0, 0]\n[0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0]\n[0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n  10/500: episode: 1, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 5.000, mean reward: 0.500 [0.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.300 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n[0, 0, 0]\n[0, 0, 0, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\rl\\memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1]\n[0, 0, 0, 1, 1, 0]\n[0, 0, 0, 1, 1, 0, 0]\n[0, 0, 0, 1, 1, 0, 0, 1]\n[0, 0, 0, 1, 1, 0, 0, 1, 1]\n[0, 0, 0, 1, 1, 0, 0, 1, 1, 0]\n[0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1]\n[0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1]\n  20/500: episode: 2, duration: 0.982s, episode steps: 10, steps per second: 10, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.550 [0.000, 1.000], loss: 0.269254, mean_absolute_error: 0.327091, mean_q: 0.081300\n[0, 0, 0]\n[0, 0, 0, 1]\n[0, 0, 0, 1, 1]\n[0, 0, 0, 1, 1, 0]\n[0, 0, 0, 1, 1, 0, 1]\n[0, 0, 0, 1, 1, 0, 1, 1]\n[0, 0, 0, 1, 1, 0, 1, 1, 0]\n[0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n[0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0]\n[0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]\n  30/500: episode: 3, duration: 0.096s, episode steps: 10, steps per second: 104, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.289700, mean_absolute_error: 0.366818, mean_q: 0.136284\n[1, 0, 0]\n[1, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\rl\\memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 1]\n[1, 0, 0, 0, 1, 1]\n[1, 0, 0, 0, 1, 1, 1]\n[1, 0, 0, 0, 1, 1, 1, 0]\n[1, 0, 0, 0, 1, 1, 1, 0, 1]\n[1, 0, 0, 0, 1, 1, 1, 0, 1, 1]\n[1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0]\n[1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0]\n  40/500: episode: 4, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.241171, mean_absolute_error: 0.362811, mean_q: 0.211130\n[1, 0, 0]\n[1, 0, 0, 0]\n[1, 0, 0, 0, 0]\n[1, 0, 0, 0, 0, 0]\n[1, 0, 0, 0, 0, 0, 1]\n[1, 0, 0, 0, 0, 0, 1, 0]\n[1, 0, 0, 0, 0, 0, 1, 0, 0]\n[1, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50/500: episode: 5, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 5.000, mean reward: 0.500 [0.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.250 [0.000, 1.000], loss: 0.209490, mean_absolute_error: 0.380769, mean_q: 0.291070\n[1, 0, 0]\n[1, 0, 0, 1]\n[1, 0, 0, 1, 1]\n[1, 0, 0, 1, 1, 1]\n[1, 0, 0, 1, 1, 1, 0]\n[1, 0, 0, 1, 1, 1, 0, 0]\n[1, 0, 0, 1, 1, 1, 0, 0, 1]\n[1, 0, 0, 1, 1, 1, 0, 0, 1, 1]\n[1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0]\n[1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n  60/500: episode: 6, duration: 0.130s, episode steps: 10, steps per second: 77, episode reward: 5.000, mean reward: 0.500 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.550 [0.000, 1.000], loss: 0.178774, mean_absolute_error: 0.393363, mean_q: 0.355312\n[1, 1, 1]\n[1, 1, 1, 1]\n[1, 1, 1, 1, 0]\n[1, 1, 1, 1, 0, 1]\n[1, 1, 1, 1, 0, 1, 0]\n[1, 1, 1, 1, 0, 1, 0, 1]\n[1, 1, 1, 1, 0, 1, 0, 1, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n[1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0]\n[1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1]\n  70/500: episode: 7, duration: 0.136s, episode steps: 10, steps per second: 74, episode reward: 4.000, mean reward: 0.400 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.148677, mean_absolute_error: 0.415424, mean_q: 0.446061\n[1, 1, 0]\n[1, 1, 0, 0]\n[1, 1, 0, 0, 0]\n[1, 1, 0, 0, 0, 0]\n[1, 1, 0, 0, 0, 0, 1]\n[1, 1, 0, 0, 0, 0, 1, 0]\n[1, 1, 0, 0, 0, 0, 1, 0, 1]\n[1, 1, 0, 0, 0, 0, 1, 0, 1, 0]\n[1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0]\n[1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n  80/500: episode: 8, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 4.000, mean reward: 0.400 [0.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.300 [0.000, 1.000], loss: 0.122876, mean_absolute_error: 0.450900, mean_q: 0.548571\n[1, 0, 1]\n[1, 0, 1, 1]\n[1, 0, 1, 1, 1]\n[1, 0, 1, 1, 1, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 0, 1, 1, 1, 0, 0]\n[1, 0, 1, 1, 1, 0, 0, 1]\n[1, 0, 1, 1, 1, 0, 0, 1, 0]\n[1, 0, 1, 1, 1, 0, 0, 1, 0, 1]\n[1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0]\n[1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1]\n  90/500: episode: 9, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 5.000, mean reward: 0.500 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.550 [0.000, 1.000], loss: 0.095010, mean_absolute_error: 0.468839, mean_q: 0.653137\n[0, 1, 0]\n[0, 1, 0, 1]\n[0, 1, 0, 1, 1]\n[0, 1, 0, 1, 1, 1]\n[0, 1, 0, 1, 1, 1, 1]\n[0, 1, 0, 1, 1, 1, 1, 0]\n[0, 1, 0, 1, 1, 1, 1, 0, 1]\n[0, 1, 0, 1, 1, 1, 1, 0, 1, 1]\n[0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1]\n[0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0]\n 100/500: episode: 10, duration: 0.093s, episode steps: 10, steps per second: 107, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.750 [0.000, 1.000], loss: 0.084146, mean_absolute_error: 0.483551, mean_q: 0.751638\n[0, 0, 0]\n[0, 0, 0, 1]\n[0, 0, 0, 1, 1]\n[0, 0, 0, 1, 1, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 0, 1]\n[0, 0, 0, 1, 1, 0, 1, 0]\n[0, 0, 0, 1, 1, 0, 1, 0, 0]\n[0, 0, 0, 1, 1, 0, 1, 0, 0, 0]\n[0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1]\n[0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1]\n 110/500: episode: 11, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.450 [0.000, 1.000], loss: 0.086877, mean_absolute_error: 0.530042, mean_q: 0.819648\n[0, 1, 1]\n[0, 1, 1, 0]\n[0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 1, 0]\n[0, 1, 1, 0, 1, 1, 0, 0]\n[0, 1, 1, 0, 1, 1, 0, 0, 0]\n[0, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n[0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n[0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n 120/500: episode: 12, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 9.000, mean reward: 0.900 [0.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.350 [0.000, 1.000], loss: 0.080334, mean_absolute_error: 0.530983, mean_q: 0.871117\n[0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 1]\n[0, 0, 0, 0, 0, 1, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[0, 0, 0, 0, 0, 1, 0, 1]\n[0, 0, 0, 0, 0, 1, 0, 1, 0]\n[0, 0, 0, 0, 0, 1, 0, 1, 0, 1]\n[0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]\n[0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1]\n 130/500: episode: 13, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.350 [0.000, 1.000], loss: 0.075550, mean_absolute_error: 0.533048, mean_q: 0.902327\n[0, 1, 1]\n[0, 1, 1, 0]\n[0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 1, 0]\n[0, 1, 1, 0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n[0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]\n[0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0]\n 140/500: episode: 14, duration: 0.103s, episode steps: 10, steps per second: 97, episode reward: 8.000, mean reward: 0.800 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.750 [0.000, 1.000], loss: 0.075981, mean_absolute_error: 0.552129, mean_q: 0.980188\n[1, 1, 1]\n[1, 1, 1, 0]\n[1, 1, 1, 0, 0]\n[1, 1, 1, 0, 0, 1]\n[1, 1, 1, 0, 0, 1, 0]\n[1, 1, 1, 0, 0, 1, 0, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 1, 1, 0, 0, 1, 0, 0, 1]\n[1, 1, 1, 0, 0, 1, 0, 0, 1, 1]\n[1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0]\n[1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1]\n 150/500: episode: 15, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 4.000, mean reward: 0.400 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.069901, mean_absolute_error: 0.601390, mean_q: 1.079453\n[0, 1, 1]\n[0, 1, 1, 1]\n[0, 1, 1, 1, 0]\n[0, 1, 1, 1, 0, 1]\n[0, 1, 1, 1, 0, 1, 1]\n[0, 1, 1, 1, 0, 1, 1, 0]\n[0, 1, 1, 1, 0, 1, 1, 0, 1]\n[0, 1, 1, 1, 0, 1, 1, 0, 1, 1]\n[0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n[0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n 160/500: episode: 16, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 9.000, mean reward: 0.900 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.700 [0.000, 1.000], loss: 0.062792, mean_absolute_error: 0.595768, mean_q: 1.113277\n[1, 1, 0]\n[1, 1, 0, 0]\n[1, 1, 0, 0, 1]\n[1, 1, 0, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 1, 1, 1]\n[1, 1, 0, 0, 1, 1, 1, 0]\n[1, 1, 0, 0, 1, 1, 1, 0, 0]\n[1, 1, 0, 0, 1, 1, 1, 0, 0, 0]\n[1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0]\n[1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1]\n 170/500: episode: 17, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 5.000, mean reward: 0.500 [0.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.400 [0.000, 1.000], loss: 0.068018, mean_absolute_error: 0.619285, mean_q: 1.167351\n[0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0, 1]\n[0, 0, 0, 0, 1, 0]\n[0, 0, 0, 0, 1, 0, 1]\n[0, 0, 0, 0, 1, 0, 1, 0]\n[0, 0, 0, 0, 1, 0, 1, 0, 1]\n[0, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n[0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1]\n[0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0]\n 180/500: episode: 18, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.050341, mean_absolute_error: 0.632588, mean_q: 1.246435\n[1, 1, 1]\n[1, 1, 1, 0]\n[1, 1, 1, 0, 1]\n[1, 1, 1, 0, 1, 1]\n[1, 1, 1, 0, 1, 1, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 0, 1]\n[1, 1, 1, 0, 1, 1, 0, 1, 1]\n[1, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n[1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0]\n[1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0]\n 190/500: episode: 19, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.650 [0.000, 1.000], loss: 0.059351, mean_absolute_error: 0.641359, mean_q: 1.300895\n[1, 1, 0]\n[1, 1, 0, 1]\n[1, 1, 0, 1, 0]\n[1, 1, 0, 1, 0, 1]\n[1, 1, 0, 1, 0, 1, 0]\n[1, 1, 0, 1, 0, 1, 0, 0]\n[1, 1, 0, 1, 0, 1, 0, 0, 0]\n[1, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n[1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n[1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1]\n 200/500: episode: 20, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.300 [0.000, 1.000], loss: 0.054247, mean_absolute_error: 0.656456, mean_q: 1.376240\n[1, 1, 1]\n[1, 1, 1, 1]\n[1, 1, 1, 1, 1]\n[1, 1, 1, 1, 1, 0]\n[1, 1, 1, 1, 1, 0, 1]\n[1, 1, 1, 1, 1, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 0, 1, 1, 0]\n[1, 1, 1, 1, 1, 0, 1, 1, 0, 1]\n[1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0]\n[1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1]\n 210/500: episode: 21, duration: 0.105s, episode steps: 10, steps per second: 95, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.700 [0.000, 1.000], loss: 0.065969, mean_absolute_error: 0.717294, mean_q: 1.497033\n[0, 1, 1]\n[0, 1, 1, 0]\n[0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 1, 0]\n[0, 1, 1, 0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n[0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0]\n 220/500: episode: 22, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 9.000, mean reward: 0.900 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.650 [0.000, 1.000], loss: 0.055879, mean_absolute_error: 0.732470, mean_q: 1.595320\n[1, 1, 1]\n[1, 1, 1, 1]\n[1, 1, 1, 1, 0]\n[1, 1, 1, 1, 0, 1]\n[1, 1, 1, 1, 0, 1, 1]\n[1, 1, 1, 1, 0, 1, 1, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 1, 1, 1, 0, 1, 1, 0, 1]\n[1, 1, 1, 1, 0, 1, 1, 0, 1, 1]\n[1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n[1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n 230/500: episode: 23, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.650 [0.000, 1.000], loss: 0.057372, mean_absolute_error: 0.730190, mean_q: 1.647084\n[0, 0, 0]\n[0, 0, 0, 1]\n[0, 0, 0, 1, 1]\n[0, 0, 0, 1, 1, 0]\n[0, 0, 0, 1, 1, 0, 0]\n[0, 0, 0, 1, 1, 0, 0, 0]\n[0, 0, 0, 1, 1, 0, 0, 0, 0]\n[0, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n[0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]\n[0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0]\n 240/500: episode: 24, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.400 [0.000, 1.000], loss: 0.058817, mean_absolute_error: 0.774644, mean_q: 1.706526\n[1, 1, 0]\n[1, 1, 0, 1]\n[1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 0, 1, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 1, 0, 1, 1, 0, 1, 0, 1]\n[1, 1, 0, 1, 1, 0, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n 250/500: episode: 25, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 9.000, mean reward: 0.900 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.600 [0.000, 1.000], loss: 0.071371, mean_absolute_error: 0.826683, mean_q: 1.826334\n[1, 1, 0]\n[1, 1, 0, 1]\n[1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n 260/500: episode: 26, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 9.000, mean reward: 0.900 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.700 [0.000, 1.000], loss: 0.072941, mean_absolute_error: 0.838560, mean_q: 1.872267\n[1, 0, 1]\n[1, 0, 1, 0]\n[1, 0, 1, 0, 1]\n[1, 0, 1, 0, 1, 0]\n[1, 0, 1, 0, 1, 0, 1]\n[1, 0, 1, 0, 1, 0, 1, 0]\n[1, 0, 1, 0, 1, 0, 1, 0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 0, 1, 0, 1, 0, 1, 1]\n[1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0]\n[1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1]\n 270/500: episode: 27, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.550 [0.000, 1.000], loss: 0.084253, mean_absolute_error: 0.852800, mean_q: 1.910809\n[0, 1, 1]\n[0, 1, 1, 0]\n[0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 0]\n[0, 1, 1, 0, 1, 0, 0]\n[0, 1, 1, 0, 1, 0, 0, 1]\n[0, 1, 1, 0, 1, 0, 0, 1, 0]\n[0, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n[0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0]\n 280/500: episode: 28, duration: 0.096s, episode steps: 10, steps per second: 104, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.550 [0.000, 1.000], loss: 0.068525, mean_absolute_error: 0.875100, mean_q: 2.055014\n[0, 1, 0]\n[0, 1, 0, 1]\n[0, 1, 0, 1, 1]\n[0, 1, 0, 1, 1, 0]\n[0, 1, 0, 1, 1, 0, 1]\n[0, 1, 0, 1, 1, 0, 1, 0]\n[0, 1, 0, 1, 1, 0, 1, 0, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[0, 1, 0, 1, 1, 0, 1, 0, 0, 0]\n[0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n[0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0]\n 290/500: episode: 29, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.350 [0.000, 1.000], loss: 0.068281, mean_absolute_error: 0.898057, mean_q: 2.152126\n[0, 0, 0]\n[0, 0, 0, 1]\n[0, 0, 0, 1, 1]\n[0, 0, 0, 1, 1, 0]\n[0, 0, 0, 1, 1, 0, 1]\n[0, 0, 0, 1, 1, 0, 1, 1]\n[0, 0, 0, 1, 1, 0, 1, 1, 0]\n[0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n[0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0]\n[0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1]\n 300/500: episode: 30, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 8.000, mean reward: 0.800 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.550 [0.000, 1.000], loss: 0.105475, mean_absolute_error: 0.987345, mean_q: 2.204072\n[0, 1, 1]\n[0, 1, 1, 1]\n[0, 1, 1, 1, 0]\n[0, 1, 1, 1, 0, 0]\n[0, 1, 1, 1, 0, 0, 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[0, 1, 1, 1, 0, 0, 1, 1]\n[0, 1, 1, 1, 0, 0, 1, 1, 0]\n[0, 1, 1, 1, 0, 0, 1, 1, 0, 1]\n[0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1]\n[0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0]\n 310/500: episode: 31, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.650 [0.000, 1.000], loss: 0.104838, mean_absolute_error: 0.979407, mean_q: 2.247802\n[0, 1, 1]\n[0, 1, 1, 0]\n[0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 0]\n[0, 1, 1, 0, 1, 0, 1]\n[0, 1, 1, 0, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 0, 1, 1, 0]\n[0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1]\n 320/500: episode: 32, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 8.000, mean reward: 0.800 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.700 [0.000, 1.000], loss: 0.117579, mean_absolute_error: 1.005916, mean_q: 2.340500\n[1, 0, 1]\n[1, 0, 1, 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1]\n[1, 0, 1, 1, 0, 1, 1]\n[1, 0, 1, 1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1, 1, 0, 1]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0]\n 330/500: episode: 33, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 9.000, mean reward: 0.900 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.700 [0.000, 1.000], loss: 0.106108, mean_absolute_error: 1.075372, mean_q: 2.417346\n[0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0, 1]\n[0, 0, 0, 0, 1, 1]\n[0, 0, 0, 0, 1, 1, 0]\n[0, 0, 0, 0, 1, 1, 0, 1]\n[0, 0, 0, 0, 1, 1, 0, 1, 1]\n[0, 0, 0, 0, 1, 1, 0, 1, 1, 0]\n[0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1]\n[0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n 340/500: episode: 34, duration: 0.094s, episode steps: 10, steps per second: 107, episode reward: 9.000, mean reward: 0.900 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.550 [0.000, 1.000], loss: 0.139572, mean_absolute_error: 1.100892, mean_q: 2.488685\n[1, 1, 0]\n[1, 1, 0, 1]\n[1, 1, 0, 1, 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0]\n 350/500: episode: 35, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 9.000, mean reward: 0.900 [0.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.450 [0.000, 1.000], loss: 0.176558, mean_absolute_error: 1.141138, mean_q: 2.490771\n[1, 0, 1]\n[1, 0, 1, 1]\n[1, 0, 1, 1, 1]\n[1, 0, 1, 1, 1, 0]\n[1, 0, 1, 1, 1, 0, 1]\n[1, 0, 1, 1, 1, 0, 1, 1]\n[1, 0, 1, 1, 1, 0, 1, 1, 0]\n[1, 0, 1, 1, 1, 0, 1, 1, 0, 1]\n[1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0]\n[1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1]\n 360/500: episode: 36, duration: 0.093s, episode steps: 10, steps per second: 107, episode reward: 8.000, mean reward: 0.800 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.650 [0.000, 1.000], loss: 0.107361, mean_absolute_error: 1.107930, mean_q: 2.565652\n[1, 1, 0]\n[1, 1, 0, 1]\n[1, 1, 0, 1, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 1, 1, 1]\n[1, 1, 0, 1, 1, 1, 0]\n[1, 1, 0, 1, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1]\n[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0]\n 370/500: episode: 37, duration: 0.102s, episode steps: 10, steps per second: 98, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.750 [0.000, 1.000], loss: 0.199844, mean_absolute_error: 1.216608, mean_q: 2.771676\n[1, 0, 1]\n[1, 0, 1, 0]\n[1, 0, 1, 0, 1]\n[1, 0, 1, 0, 1, 1]\n[1, 0, 1, 0, 1, 1, 0]\n[1, 0, 1, 0, 1, 1, 0, 1]\n[1, 0, 1, 0, 1, 1, 0, 1, 0]\n[1, 0, 1, 0, 1, 1, 0, 1, 0, 0]\n[1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0]\n[1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1]\n 380/500: episode: 38, duration: 0.098s, episode steps: 10, steps per second: 102, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.450 [0.000, 1.000], loss: 0.143917, mean_absolute_error: 1.219301, mean_q: 2.705633\n[0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0, 1]\n[0, 0, 0, 0, 1, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 1, 0, 1]\n[0, 0, 0, 0, 1, 0, 1, 1]\n[0, 0, 0, 0, 1, 0, 1, 1, 1]\n[0, 0, 0, 0, 1, 0, 1, 1, 1, 1]\n[0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1]\n[0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n 390/500: episode: 39, duration: 0.107s, episode steps: 10, steps per second: 93, episode reward: 5.000, mean reward: 0.500 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.600 [0.000, 1.000], loss: 0.169549, mean_absolute_error: 1.231380, mean_q: 2.880740\n[0, 1, 0]\n[0, 1, 0, 1]\n[0, 1, 0, 1, 1]\n[0, 1, 0, 1, 1, 1]\n[0, 1, 0, 1, 1, 1, 0]\n[0, 1, 0, 1, 1, 1, 0, 1]\n[0, 1, 0, 1, 1, 1, 0, 1, 0]\n[0, 1, 0, 1, 1, 1, 0, 1, 0, 0]\n[0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1]\n[0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1]\n 400/500: episode: 40, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 5.000, mean reward: 0.500 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.600 [0.000, 1.000], loss: 0.234701, mean_absolute_error: 1.321127, mean_q: 2.894804\n[1, 0, 1]\n[1, 0, 1, 1]\n[1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 0, 1, 1]\n[1, 0, 1, 1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1, 1, 0, 1]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1]\n 410/500: episode: 41, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 8.000, mean reward: 0.800 [0.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: 0.750 [0.000, 1.000], loss: 0.291819, mean_absolute_error: 1.388030, mean_q: 2.919295\n[0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0, 1]\n[0, 0, 0, 0, 1, 1]\n[0, 0, 0, 0, 1, 1, 0]\n[0, 0, 0, 0, 1, 1, 0, 1]\n[0, 0, 0, 0, 1, 1, 0, 1, 1]\n[0, 0, 0, 0, 1, 1, 0, 1, 1, 1]\n[0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0]\n[0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0]\n 420/500: episode: 42, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.190540, mean_absolute_error: 1.362183, mean_q: 2.955077\n[1, 0, 1]\n[1, 0, 1, 1]\n[1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1]\n[1, 0, 1, 1, 0, 1, 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 0, 1, 1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1, 1, 0, 1]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n 430/500: episode: 43, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.650 [0.000, 1.000], loss: 0.294704, mean_absolute_error: 1.445764, mean_q: 3.144175\n[0, 0, 1]\n[0, 0, 1, 1]\n[0, 0, 1, 1, 0]\n[0, 0, 1, 1, 0, 0]\n[0, 0, 1, 1, 0, 0, 0]\n[0, 0, 1, 1, 0, 0, 0, 1]\n[0, 0, 1, 1, 0, 0, 0, 1, 0]\n[0, 0, 1, 1, 0, 0, 0, 1, 0, 1]\n[0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1]\n[0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0]\n 440/500: episode: 44, duration: 0.100s, episode steps: 10, steps per second: 100, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.261528, mean_absolute_error: 1.424281, mean_q: 3.087670\n[1, 0, 0]\n[1, 0, 0, 0]\n[1, 0, 0, 0, 1]\n[1, 0, 0, 0, 1, 1]\n[1, 0, 0, 0, 1, 1, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 0, 0, 0, 1, 1, 0, 1]\n[1, 0, 0, 0, 1, 1, 0, 1, 0]\n[1, 0, 0, 0, 1, 1, 0, 1, 0, 1]\n[1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1]\n[1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1]\n 450/500: episode: 45, duration: 0.101s, episode steps: 10, steps per second: 99, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.550 [0.000, 1.000], loss: 0.301147, mean_absolute_error: 1.487703, mean_q: 3.216314\n[1, 1, 0]\n[1, 1, 0, 1]\n[1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 1]\n[1, 1, 0, 1, 1, 1, 0]\n[1, 1, 0, 1, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0]\n 460/500: episode: 46, duration: 0.089s, episode steps: 10, steps per second: 113, episode reward: 8.000, mean reward: 0.800 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.650 [0.000, 1.000], loss: 0.249543, mean_absolute_error: 1.530695, mean_q: 3.274679\n[1, 0, 1]\n[1, 0, 1, 1]\n[1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1]\n[1, 0, 1, 1, 0, 1, 0]\n[1, 0, 1, 1, 0, 1, 0, 1]\n[1, 0, 1, 1, 0, 1, 0, 1, 1]\n[1, 0, 1, 1, 0, 1, 0, 1, 1, 0]\n[1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n[1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0]\n 470/500: episode: 47, duration: 0.091s, episode steps: 10, steps per second: 109, episode reward: 8.000, mean reward: 0.800 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.301767, mean_absolute_error: 1.562073, mean_q: 3.393026\n[0, 0, 1]\n[0, 0, 1, 1]\n[0, 0, 1, 1, 0]\n[0, 0, 1, 1, 0, 1]\n[0, 0, 1, 1, 0, 1, 1]\n[0, 0, 1, 1, 0, 1, 1, 1]\n[0, 0, 1, 1, 0, 1, 1, 1, 1]\n[0, 0, 1, 1, 0, 1, 1, 1, 1, 0]\n[0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n[0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0]\n 480/500: episode: 48, duration: 0.095s, episode steps: 10, steps per second: 105, episode reward: 6.000, mean reward: 0.600 [0.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: 0.700 [0.000, 1.000], loss: 0.248140, mean_absolute_error: 1.583939, mean_q: 3.446417\n[1, 0, 1]\n[1, 0, 1, 0]\n[1, 0, 1, 0, 1]\n[1, 0, 1, 0, 1, 1]\n[1, 0, 1, 0, 1, 1, 0]\n[1, 0, 1, 0, 1, 1, 0, 1]\n[1, 0, 1, 0, 1, 1, 0, 1, 1]\n[1, 0, 1, 0, 1, 1, 0, 1, 1, 0]\n[1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0]\n[1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 490/500: episode: 49, duration: 0.086s, episode steps: 10, steps per second: 117, episode reward: 8.000, mean reward: 0.800 [0.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.500 [0.000, 1.000], loss: 0.317350, mean_absolute_error: 1.620282, mean_q: 3.551276\n[0, 1, 1]\n[0, 1, 1, 0]\n[0, 1, 1, 0, 1]\n[0, 1, 1, 0, 1, 1]\n[0, 1, 1, 0, 1, 1, 0]\n[0, 1, 1, 0, 1, 1, 0, 0]\n[0, 1, 1, 0, 1, 1, 0, 0, 1]\n[0, 1, 1, 0, 1, 1, 0, 0, 1, 1]\n[0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1]\n[0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n 500/500: episode: 50, duration: 0.098s, episode steps: 10, steps per second: 103, episode reward: 7.000, mean reward: 0.700 [0.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.650 [0.000, 1.000], loss: 0.360748, mean_absolute_error: 1.667379, mean_q: 3.601879\ndone, took 6.166 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25d1fef5320>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=500, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\nEpisode 1: reward: 10.000, steps: 10\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "dqn.test(env)\n",
    "print(env.state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(model.predict(env.state.reshape(1, 1, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0]), 1, False, {})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'flatten_2_input:0' shape=(?, 1, 2) dtype=float32>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(index=0).input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 2 episodes ...\n[0, 0, 0]\n[0, 0, 0, 0]\n[0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nEpisode 1: reward: 10.000, steps: 10\n[1, 1, 0]\n[1, 1, 0, 1]\n[1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0, 1, 1, 0]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 1]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1]\n[1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0]\nEpisode 2: reward: 10.000, steps: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25d88f546d8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, 2, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
