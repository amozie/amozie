{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.agents import *\n",
    "from rl.policy import *\n",
    "from rl.memory import *\n",
    "from rl.random import *\n",
    "import gym\n",
    "from gym import Env, Space, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(123)\n",
    "env.reset()\n",
    "nb_actions = env.action_space.n\n",
    "observation_shape = env.observation_space.shape\n",
    "window_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_x = Input((window_length, ) + observation_shape)\n",
    "model_y = Flatten()(model_x)\n",
    "model_y = Dense(4, activation='elu')(model_y)\n",
    "model_y = Dense(4, activation='elu')(model_y)\n",
    "model_y = Dense(4, activation='elu')(model_y)\n",
    "model_y = Dense(nb_actions)(model_y)\n",
    "model = Model(model_x, model_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = model.layers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_1/Elu:0' shape=(?, 4) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(100000, window_length=window_length)\n",
    "dqn = DQNAgent(model, nb_actions=nb_actions, policy=policy, memory=memory, \n",
    "               nb_steps_warmup=100, target_model_update=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.compile(Adam(), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "0 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 9}\n",
      "     9/100000: episode: 1, duration: 0.503s, episode steps: 9, steps per second: 18, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.158 [-1.581, 2.538], loss: --, mean_squared_error: --, mean_q: --\n",
      "1 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 18}\n",
      "    18/100000: episode: 2, duration: 0.016s, episode steps: 9, steps per second: 551, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.796, 1.720], loss: --, mean_squared_error: --, mean_q: --\n",
      "2 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 27}\n",
      "    27/100000: episode: 3, duration: 0.018s, episode steps: 9, steps per second: 505, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.790, 1.787], loss: --, mean_squared_error: --, mean_q: --\n",
      "3 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 37}\n",
      "    37/100000: episode: 4, duration: 0.015s, episode steps: 10, steps per second: 654, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.067, 1.908], loss: --, mean_squared_error: --, mean_q: --\n",
      "4 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 46}\n",
      "    46/100000: episode: 5, duration: 0.013s, episode steps: 9, steps per second: 714, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.120 [-1.420, 2.219], loss: --, mean_squared_error: --, mean_q: --\n",
      "5 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 56}\n",
      "    56/100000: episode: 6, duration: 0.014s, episode steps: 10, steps per second: 705, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-3.103, 1.945], loss: --, mean_squared_error: --, mean_q: --\n",
      "6 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 65}\n",
      "    65/100000: episode: 7, duration: 0.012s, episode steps: 9, steps per second: 743, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.864, 1.736], loss: --, mean_squared_error: --, mean_q: --\n",
      "7 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 74}\n",
      "    74/100000: episode: 8, duration: 0.014s, episode steps: 9, steps per second: 622, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.814, 1.769], loss: --, mean_squared_error: --, mean_q: --\n",
      "8 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 82}\n",
      "    82/100000: episode: 9, duration: 0.011s, episode steps: 8, steps per second: 748, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.568, 1.613], loss: --, mean_squared_error: --, mean_q: --\n",
      "9 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 90}\n",
      "    90/100000: episode: 10, duration: 0.013s, episode steps: 8, steps per second: 630, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.163 [-1.537, 2.597], loss: --, mean_squared_error: --, mean_q: --\n",
      "10 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 99}\n",
      "    99/100000: episode: 11, duration: 0.012s, episode steps: 9, steps per second: 729, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.830, 1.804], loss: --, mean_squared_error: --, mean_q: --\n",
      "11 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 109}\n",
      "   109/100000: episode: 12, duration: 2.189s, episode steps: 10, steps per second: 5, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.561, 1.602], loss: 0.669034, mean_squared_error: 1.043506, mean_q: 0.593345\n",
      "12 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 119}\n",
      "   119/100000: episode: 13, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-3.047, 1.945], loss: 0.648060, mean_squared_error: 1.062039, mean_q: 0.731676\n",
      "13 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 127}\n",
      "   127/100000: episode: 14, duration: 0.098s, episode steps: 8, steps per second: 81, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.596, 1.591], loss: 0.693678, mean_squared_error: 1.112926, mean_q: 0.845370\n",
      "14 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 137}\n",
      "   137/100000: episode: 15, duration: 0.131s, episode steps: 10, steps per second: 76, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.116 [-2.572, 1.576], loss: 0.684971, mean_squared_error: 1.121617, mean_q: 0.974560\n",
      "15 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 146}\n",
      "   146/100000: episode: 16, duration: 0.126s, episode steps: 9, steps per second: 71, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.164 [-2.257, 1.330], loss: 0.698110, mean_squared_error: 1.135398, mean_q: 1.102476\n",
      "16 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 155}\n",
      "   155/100000: episode: 17, duration: 0.110s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.145 [-2.304, 1.415], loss: 0.843272, mean_squared_error: 1.288461, mean_q: 1.256181\n",
      "17 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 165}\n",
      "   165/100000: episode: 18, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.947, 3.122], loss: 0.847647, mean_squared_error: 1.294447, mean_q: 1.384714\n",
      "18 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 176}\n",
      "   176/100000: episode: 19, duration: 0.142s, episode steps: 11, steps per second: 77, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.136 [-2.167, 1.324], loss: 0.928157, mean_squared_error: 1.360999, mean_q: 1.448785\n",
      "19 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 186}\n",
      "   186/100000: episode: 20, duration: 0.131s, episode steps: 10, steps per second: 77, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.142 [-1.576, 2.585], loss: 1.056421, mean_squared_error: 1.528468, mean_q: 1.640631\n",
      "20 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 196}\n",
      "   196/100000: episode: 21, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.116 [-2.968, 1.956], loss: 1.173830, mean_squared_error: 1.637198, mean_q: 1.785934\n",
      "21 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 207}\n",
      "   207/100000: episode: 22, duration: 0.141s, episode steps: 11, steps per second: 78, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-3.300, 2.194], loss: 1.243191, mean_squared_error: 1.674807, mean_q: 1.947715\n",
      "22 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 217}\n",
      "   217/100000: episode: 23, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.107 [-2.423, 1.613], loss: 1.098504, mean_squared_error: 1.512620, mean_q: 2.272492\n",
      "23 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 227}\n",
      "   227/100000: episode: 24, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-2.954, 1.940], loss: 1.602753, mean_squared_error: 2.183051, mean_q: 2.446892\n",
      "24 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 239}\n",
      "   239/100000: episode: 25, duration: 0.136s, episode steps: 12, steps per second: 89, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.095 [-2.407, 1.582], loss: 1.744771, mean_squared_error: 2.287123, mean_q: 2.745487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 249}\n",
      "   249/100000: episode: 26, duration: 0.139s, episode steps: 10, steps per second: 72, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-3.074, 1.949], loss: 1.810319, mean_squared_error: 2.349306, mean_q: 2.944232\n",
      "26 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 259}\n",
      "   259/100000: episode: 27, duration: 0.181s, episode steps: 10, steps per second: 55, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-3.058, 1.974], loss: 2.288061, mean_squared_error: 3.063325, mean_q: 3.255407\n",
      "27 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 267}\n",
      "   267/100000: episode: 28, duration: 0.237s, episode steps: 8, steps per second: 34, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.563, 1.578], loss: 2.316702, mean_squared_error: 2.978908, mean_q: 3.337515\n",
      "28 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 278}\n",
      "   278/100000: episode: 29, duration: 0.145s, episode steps: 11, steps per second: 76, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.118 [-2.861, 1.808], loss: 2.306083, mean_squared_error: 3.125722, mean_q: 3.487430\n",
      "29 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 288}\n",
      "   288/100000: episode: 30, duration: 0.148s, episode steps: 10, steps per second: 68, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-3.004, 1.944], loss: 2.460225, mean_squared_error: 3.057443, mean_q: 3.866826\n",
      "30 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 296}\n",
      "   296/100000: episode: 31, duration: 0.136s, episode steps: 8, steps per second: 59, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.589, 1.538], loss: 3.084476, mean_squared_error: 3.949661, mean_q: 4.129406\n",
      "31 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 305}\n",
      "   305/100000: episode: 32, duration: 0.126s, episode steps: 9, steps per second: 71, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.766, 1.737], loss: 2.869159, mean_squared_error: 3.656499, mean_q: 4.195766\n",
      "32 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 315}\n",
      "   315/100000: episode: 33, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-3.093, 1.907], loss: 2.999508, mean_squared_error: 3.861484, mean_q: 4.477065\n",
      "33 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 325}\n",
      "   325/100000: episode: 34, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.014, 1.939], loss: 2.781517, mean_squared_error: 3.803244, mean_q: 4.649250\n",
      "34 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 334}\n",
      "   334/100000: episode: 35, duration: 0.109s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.168 [-2.867, 1.716], loss: 3.096526, mean_squared_error: 4.295755, mean_q: 4.834877\n",
      "35 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 344}\n",
      "   344/100000: episode: 36, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-3.028, 1.944], loss: 3.228954, mean_squared_error: 4.381072, mean_q: 5.198133\n",
      "36 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 354}\n",
      "   354/100000: episode: 37, duration: 0.125s, episode steps: 10, steps per second: 80, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.147 [-2.549, 1.523], loss: 2.932184, mean_squared_error: 4.232756, mean_q: 5.297999\n",
      "37 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 363}\n",
      "   363/100000: episode: 38, duration: 0.118s, episode steps: 9, steps per second: 76, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.815, 1.740], loss: 3.589328, mean_squared_error: 5.240041, mean_q: 5.800722\n",
      "38 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 374}\n",
      "   374/100000: episode: 39, duration: 0.124s, episode steps: 11, steps per second: 89, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.111 [-2.180, 1.380], loss: 3.955765, mean_squared_error: 6.385167, mean_q: 5.910754\n",
      "39 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 383}\n",
      "   383/100000: episode: 40, duration: 0.105s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.149 [-2.485, 1.592], loss: 4.229986, mean_squared_error: 6.476581, mean_q: 6.151489\n",
      "40 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 391}\n",
      "   391/100000: episode: 41, duration: 0.105s, episode steps: 8, steps per second: 76, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.503, 1.569], loss: 4.385012, mean_squared_error: 7.218863, mean_q: 6.378430\n",
      "41 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 401}\n",
      "   401/100000: episode: 42, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-3.062, 2.003], loss: 4.441267, mean_squared_error: 7.278958, mean_q: 6.468389\n",
      "42 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 410}\n",
      "   410/100000: episode: 43, duration: 0.111s, episode steps: 9, steps per second: 81, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.171 [-2.832, 1.728], loss: 3.722828, mean_squared_error: 7.328781, mean_q: 6.491352\n",
      "43 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 420}\n",
      "   420/100000: episode: 44, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-3.025, 1.917], loss: 4.147083, mean_squared_error: 7.851256, mean_q: 6.719453\n",
      "44 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 430}\n",
      "   430/100000: episode: 45, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.122 [-2.503, 1.601], loss: 4.084157, mean_squared_error: 8.519179, mean_q: 6.956809\n",
      "45 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 444}\n",
      "   444/100000: episode: 46, duration: 0.160s, episode steps: 14, steps per second: 87, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.079 [-2.944, 1.980], loss: 4.809936, mean_squared_error: 9.497009, mean_q: 7.474915\n",
      "46 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 454}\n",
      "   454/100000: episode: 47, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-3.001, 1.955], loss: 4.245432, mean_squared_error: 9.499949, mean_q: 7.421031\n",
      "47 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 464}\n",
      "   464/100000: episode: 48, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.126 [-2.507, 1.587], loss: 4.989272, mean_squared_error: 11.084095, mean_q: 7.665564\n",
      "48 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 474}\n",
      "   474/100000: episode: 49, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.537, 1.537], loss: 4.072259, mean_squared_error: 10.319910, mean_q: 7.714369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 485}\n",
      "   485/100000: episode: 50, duration: 0.130s, episode steps: 11, steps per second: 84, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.112 [-2.758, 1.755], loss: 4.375133, mean_squared_error: 12.005654, mean_q: 7.932797\n",
      "50 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 494}\n",
      "   494/100000: episode: 51, duration: 0.103s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.165 [-2.896, 1.743], loss: 5.064598, mean_squared_error: 13.466959, mean_q: 8.349032\n",
      "51 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 504}\n",
      "   504/100000: episode: 52, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.583, 1.585], loss: 4.947829, mean_squared_error: 14.084683, mean_q: 8.386816\n",
      "52 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 513}\n",
      "   513/100000: episode: 53, duration: 0.119s, episode steps: 9, steps per second: 76, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.168 [-2.836, 1.716], loss: 6.314831, mean_squared_error: 16.561214, mean_q: 8.699054\n",
      "53 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 523}\n",
      "   523/100000: episode: 54, duration: 0.126s, episode steps: 10, steps per second: 79, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.134 [-2.557, 1.554], loss: 6.160435, mean_squared_error: 17.618971, mean_q: 8.976074\n",
      "54 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 533}\n",
      "   533/100000: episode: 55, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.118 [-2.987, 1.942], loss: 5.700921, mean_squared_error: 17.405087, mean_q: 8.754376\n",
      "55 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 543}\n",
      "   543/100000: episode: 56, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-3.055, 1.947], loss: 5.908390, mean_squared_error: 18.339283, mean_q: 8.657133\n",
      "56 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 553}\n",
      "   553/100000: episode: 57, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-2.968, 1.930], loss: 5.629140, mean_squared_error: 18.618015, mean_q: 8.675077\n",
      "57 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 563}\n",
      "   563/100000: episode: 58, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-3.074, 1.919], loss: 5.298980, mean_squared_error: 18.118271, mean_q: 8.518501\n",
      "58 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 573}\n",
      "   573/100000: episode: 59, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.594, 1.580], loss: 4.510267, mean_squared_error: 18.120165, mean_q: 8.600458\n",
      "59 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 582}\n",
      "   582/100000: episode: 60, duration: 0.109s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.165 [-2.826, 1.736], loss: 4.599836, mean_squared_error: 17.176231, mean_q: 8.458795\n",
      "60 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 594}\n",
      "   594/100000: episode: 61, duration: 0.143s, episode steps: 12, steps per second: 84, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.147 [-2.586, 1.528], loss: 4.747342, mean_squared_error: 18.338495, mean_q: 8.649426\n",
      "61 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 603}\n",
      "   603/100000: episode: 62, duration: 0.111s, episode steps: 9, steps per second: 81, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.853, 1.798], loss: 4.086779, mean_squared_error: 18.091881, mean_q: 8.612768\n",
      "62 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 612}\n",
      "   612/100000: episode: 63, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-2.790, 1.779], loss: 3.881424, mean_squared_error: 18.020477, mean_q: 8.471248\n",
      "63 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 622}\n",
      "   622/100000: episode: 64, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.080, 1.943], loss: 3.863925, mean_squared_error: 18.656404, mean_q: 8.882185\n",
      "64 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 632}\n",
      "   632/100000: episode: 65, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.141 [-2.735, 1.733], loss: 4.908611, mean_squared_error: 19.733379, mean_q: 8.675932\n",
      "65 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 642}\n",
      "   642/100000: episode: 66, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.148 [-2.537, 1.531], loss: 4.652069, mean_squared_error: 20.256512, mean_q: 8.820004\n",
      "66 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 650}\n",
      "   650/100000: episode: 67, duration: 0.092s, episode steps: 8, steps per second: 87, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.520, 1.526], loss: 4.148998, mean_squared_error: 20.326960, mean_q: 8.957441\n",
      "67 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 660}\n",
      "   660/100000: episode: 68, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-3.075, 1.926], loss: 4.907745, mean_squared_error: 21.068302, mean_q: 8.710539\n",
      "68 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 668}\n",
      "   668/100000: episode: 69, duration: 0.102s, episode steps: 8, steps per second: 78, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.587, 1.568], loss: 3.556651, mean_squared_error: 19.860071, mean_q: 8.735155\n",
      "69 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 677}\n",
      "   677/100000: episode: 70, duration: 0.100s, episode steps: 9, steps per second: 90, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.825, 1.740], loss: 3.272578, mean_squared_error: 20.571129, mean_q: 8.846066\n",
      "70 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 686}\n",
      "   686/100000: episode: 71, duration: 0.112s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-2.853, 1.747], loss: 3.744933, mean_squared_error: 21.849234, mean_q: 9.050991\n",
      "71 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 696}\n",
      "   696/100000: episode: 72, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.145 [-2.538, 1.525], loss: 3.802130, mean_squared_error: 23.105450, mean_q: 9.179255\n",
      "72 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 704}\n",
      "   704/100000: episode: 73, duration: 0.094s, episode steps: 8, steps per second: 85, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-2.565, 1.611], loss: 3.819498, mean_squared_error: 24.373646, mean_q: 9.369535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 {'episode_reward': 18.0, 'nb_episode_steps': 18, 'nb_steps': 722}\n",
      "   722/100000: episode: 74, duration: 0.200s, episode steps: 18, steps per second: 90, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.067 [-2.224, 1.399], loss: 4.550766, mean_squared_error: 26.151711, mean_q: 9.428696\n",
      "74 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 732}\n",
      "   732/100000: episode: 75, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-3.015, 1.950], loss: 4.077215, mean_squared_error: 25.978638, mean_q: 9.318647\n",
      "75 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 743}\n",
      "   743/100000: episode: 76, duration: 0.135s, episode steps: 11, steps per second: 82, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.133 [-2.733, 1.736], loss: 3.494031, mean_squared_error: 25.489330, mean_q: 9.253020\n",
      "76 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 753}\n",
      "   753/100000: episode: 77, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.110 [-3.010, 1.997], loss: 3.218394, mean_squared_error: 26.305744, mean_q: 9.218007\n",
      "77 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 763}\n",
      "   763/100000: episode: 78, duration: 0.121s, episode steps: 10, steps per second: 82, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.166 [-3.114, 1.928], loss: 3.881150, mean_squared_error: 28.672277, mean_q: 9.652311\n",
      "78 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 775}\n",
      "   775/100000: episode: 79, duration: 0.152s, episode steps: 12, steps per second: 79, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.118 [-3.074, 1.968], loss: 4.131342, mean_squared_error: 28.776442, mean_q: 9.383426\n",
      "79 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 791}\n",
      "   791/100000: episode: 80, duration: 0.187s, episode steps: 16, steps per second: 86, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.072 [-3.036, 1.999], loss: 3.030128, mean_squared_error: 28.456417, mean_q: 9.384886\n",
      "80 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 803}\n",
      "   803/100000: episode: 81, duration: 0.142s, episode steps: 12, steps per second: 85, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.117 [-3.069, 1.938], loss: 3.375206, mean_squared_error: 29.944725, mean_q: 9.484447\n",
      "81 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 813}\n",
      "   813/100000: episode: 82, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-3.115, 1.918], loss: 3.481360, mean_squared_error: 30.225910, mean_q: 9.386454\n",
      "82 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 823}\n",
      "   823/100000: episode: 83, duration: 0.110s, episode steps: 10, steps per second: 91, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.045, 1.916], loss: 2.881971, mean_squared_error: 29.449778, mean_q: 9.298202\n",
      "83 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 831}\n",
      "   831/100000: episode: 84, duration: 0.093s, episode steps: 8, steps per second: 86, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.518, 1.610], loss: 2.719100, mean_squared_error: 29.538790, mean_q: 9.320251\n",
      "84 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 840}\n",
      "   840/100000: episode: 85, duration: 0.105s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.802, 1.772], loss: 3.104188, mean_squared_error: 29.570360, mean_q: 9.202405\n",
      "85 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 849}\n",
      "   849/100000: episode: 86, duration: 0.108s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.143 [-2.299, 1.422], loss: 2.501930, mean_squared_error: 29.466780, mean_q: 9.246525\n",
      "86 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 859}\n",
      "   859/100000: episode: 87, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.064, 1.911], loss: 3.120076, mean_squared_error: 30.445759, mean_q: 9.178750\n",
      "87 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 868}\n",
      "   868/100000: episode: 88, duration: 0.108s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.807, 1.768], loss: 2.333438, mean_squared_error: 29.131361, mean_q: 9.038779\n",
      "88 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 879}\n",
      "   879/100000: episode: 89, duration: 0.128s, episode steps: 11, steps per second: 86, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.119 [-2.804, 1.716], loss: 3.023020, mean_squared_error: 31.466633, mean_q: 9.168059\n",
      "89 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 887}\n",
      "   887/100000: episode: 90, duration: 0.101s, episode steps: 8, steps per second: 79, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.522, 1.576], loss: 2.137345, mean_squared_error: 29.816181, mean_q: 9.089481\n",
      "90 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 897}\n",
      "   897/100000: episode: 91, duration: 0.112s, episode steps: 10, steps per second: 90, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.118 [-2.536, 1.600], loss: 2.176569, mean_squared_error: 29.967859, mean_q: 9.024919\n",
      "91 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 906}\n",
      "   906/100000: episode: 92, duration: 0.108s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-2.762, 1.725], loss: 1.922836, mean_squared_error: 30.196360, mean_q: 9.062016\n",
      "92 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 914}\n",
      "   914/100000: episode: 93, duration: 0.095s, episode steps: 8, steps per second: 84, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.522, 1.589], loss: 2.206059, mean_squared_error: 30.448158, mean_q: 8.803434\n",
      "93 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 923}\n",
      "   923/100000: episode: 94, duration: 0.100s, episode steps: 9, steps per second: 90, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.821, 1.769], loss: 2.336237, mean_squared_error: 30.529366, mean_q: 8.955288\n",
      "94 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 932}\n",
      "   932/100000: episode: 95, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.785, 1.716], loss: 1.711702, mean_squared_error: 31.631416, mean_q: 9.031186\n",
      "95 {'episode_reward': 13.0, 'nb_episode_steps': 13, 'nb_steps': 945}\n",
      "   945/100000: episode: 96, duration: 0.144s, episode steps: 13, steps per second: 90, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.110 [-2.865, 1.763], loss: 2.402156, mean_squared_error: 31.207520, mean_q: 8.867806\n",
      "96 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 954}\n",
      "   954/100000: episode: 97, duration: 0.110s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-2.792, 1.773], loss: 1.661900, mean_squared_error: 30.560463, mean_q: 8.848817\n",
      "97 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 964}\n",
      "   964/100000: episode: 98, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.071, 1.970], loss: 1.802654, mean_squared_error: 30.606037, mean_q: 8.635605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 972}\n",
      "   972/100000: episode: 99, duration: 0.090s, episode steps: 8, steps per second: 89, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.528, 1.526], loss: 1.477752, mean_squared_error: 31.352566, mean_q: 8.787029\n",
      "99 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 980}\n",
      "   980/100000: episode: 100, duration: 0.099s, episode steps: 8, steps per second: 80, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.588, 1.573], loss: 1.298925, mean_squared_error: 28.374722, mean_q: 8.363178\n",
      "100 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 990}\n",
      "   990/100000: episode: 101, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.522, 1.539], loss: 1.316400, mean_squared_error: 29.215811, mean_q: 8.366216\n",
      "101 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1001}\n",
      "  1001/100000: episode: 102, duration: 0.124s, episode steps: 11, steps per second: 89, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.153 [-2.825, 1.735], loss: 1.571143, mean_squared_error: 29.625879, mean_q: 8.386728\n",
      "102 {'episode_reward': 13.0, 'nb_episode_steps': 13, 'nb_steps': 1014}\n",
      "  1014/100000: episode: 103, duration: 0.153s, episode steps: 13, steps per second: 85, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.106 [-2.729, 1.729], loss: 1.265783, mean_squared_error: 28.930256, mean_q: 8.247153\n",
      "103 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1025}\n",
      "  1025/100000: episode: 104, duration: 0.121s, episode steps: 11, steps per second: 91, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.140 [-2.730, 1.711], loss: 1.234962, mean_squared_error: 29.033791, mean_q: 8.100186\n",
      "104 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1035}\n",
      "  1035/100000: episode: 105, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-3.117, 1.975], loss: 0.936595, mean_squared_error: 29.356480, mean_q: 8.180745\n",
      "105 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1045}\n",
      "  1045/100000: episode: 106, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.128 [-3.050, 1.965], loss: 1.188684, mean_squared_error: 29.474110, mean_q: 8.046972\n",
      "106 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1055}\n",
      "  1055/100000: episode: 107, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.568, 1.548], loss: 0.874205, mean_squared_error: 30.154179, mean_q: 8.184154\n",
      "107 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1065}\n",
      "  1065/100000: episode: 108, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.133 [-2.595, 1.600], loss: 1.254022, mean_squared_error: 30.690960, mean_q: 8.047857\n",
      "108 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1075}\n",
      "  1075/100000: episode: 109, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.506, 1.603], loss: 0.748599, mean_squared_error: 30.856884, mean_q: 8.074121\n",
      "109 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1085}\n",
      "  1085/100000: episode: 110, duration: 0.116s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.135 [-2.634, 1.590], loss: 0.921264, mean_squared_error: 30.242767, mean_q: 7.937268\n",
      "110 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1095}\n",
      "  1095/100000: episode: 111, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.056, 1.945], loss: 0.814439, mean_squared_error: 30.708725, mean_q: 7.908693\n",
      "111 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1106}\n",
      "  1106/100000: episode: 112, duration: 0.120s, episode steps: 11, steps per second: 92, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.126 [-2.757, 1.783], loss: 0.799742, mean_squared_error: 28.699244, mean_q: 7.583325\n",
      "112 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1116}\n",
      "  1116/100000: episode: 113, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.139 [-2.593, 1.610], loss: 0.820497, mean_squared_error: 29.348703, mean_q: 7.616887\n",
      "113 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1125}\n",
      "  1125/100000: episode: 114, duration: 0.101s, episode steps: 9, steps per second: 89, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.809, 1.739], loss: 0.740646, mean_squared_error: 27.173317, mean_q: 7.236726\n",
      "114 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 1137}\n",
      "  1137/100000: episode: 115, duration: 0.132s, episode steps: 12, steps per second: 91, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.127 [-3.092, 1.962], loss: 0.544267, mean_squared_error: 30.303457, mean_q: 7.615589\n",
      "115 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1146}\n",
      "  1146/100000: episode: 116, duration: 0.108s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.141 [-2.781, 1.725], loss: 0.583719, mean_squared_error: 27.676554, mean_q: 7.268770\n",
      "116 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1155}\n",
      "  1155/100000: episode: 117, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.722, 2.789], loss: 0.593995, mean_squared_error: 27.736948, mean_q: 7.142169\n",
      "117 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1165}\n",
      "  1165/100000: episode: 118, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.011, 1.910], loss: 0.632104, mean_squared_error: 25.857300, mean_q: 6.822015\n",
      "118 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1174}\n",
      "  1174/100000: episode: 119, duration: 0.099s, episode steps: 9, steps per second: 91, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.165 [-2.350, 1.338], loss: 0.557098, mean_squared_error: 26.515129, mean_q: 6.877119\n",
      "119 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1184}\n",
      "  1184/100000: episode: 120, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.139 [-1.530, 2.539], loss: 0.599081, mean_squared_error: 24.144369, mean_q: 6.536719\n",
      "120 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1193}\n",
      "  1193/100000: episode: 121, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.806, 2.812], loss: 0.571485, mean_squared_error: 26.444521, mean_q: 6.761091\n",
      "121 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1202}\n",
      "  1202/100000: episode: 122, duration: 0.103s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.183 [-1.747, 2.901], loss: 0.523147, mean_squared_error: 25.052561, mean_q: 6.553228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1211}\n",
      "  1211/100000: episode: 123, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.803, 2.847], loss: 0.597966, mean_squared_error: 25.752005, mean_q: 6.569121\n",
      "123 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1221}\n",
      "  1221/100000: episode: 124, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.132 [-1.570, 2.452], loss: 0.631067, mean_squared_error: 23.323650, mean_q: 6.172790\n",
      "124 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1231}\n",
      "  1231/100000: episode: 125, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.990, 3.089], loss: 0.558319, mean_squared_error: 21.969528, mean_q: 5.994380\n",
      "125 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1241}\n",
      "  1241/100000: episode: 126, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-1.997, 3.066], loss: 0.441228, mean_squared_error: 23.039959, mean_q: 6.196658\n",
      "126 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1252}\n",
      "  1252/100000: episode: 127, duration: 0.125s, episode steps: 11, steps per second: 88, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [-2.122, 3.291], loss: 0.515970, mean_squared_error: 22.305809, mean_q: 6.110736\n",
      "127 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1261}\n",
      "  1261/100000: episode: 128, duration: 0.105s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.776, 2.782], loss: 0.514604, mean_squared_error: 20.548067, mean_q: 5.712475\n",
      "128 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1272}\n",
      "  1272/100000: episode: 129, duration: 0.129s, episode steps: 11, steps per second: 85, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-2.193, 3.284], loss: 0.574918, mean_squared_error: 20.230444, mean_q: 5.810960\n",
      "129 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1283}\n",
      "  1283/100000: episode: 130, duration: 0.123s, episode steps: 11, steps per second: 89, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-2.109, 3.282], loss: 0.581964, mean_squared_error: 22.424126, mean_q: 6.111029\n",
      "130 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 1295}\n",
      "  1295/100000: episode: 131, duration: 0.146s, episode steps: 12, steps per second: 82, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.113 [-1.977, 3.100], loss: 0.502228, mean_squared_error: 20.027523, mean_q: 5.714190\n",
      "131 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 1307}\n",
      "  1307/100000: episode: 132, duration: 0.143s, episode steps: 12, steps per second: 84, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.095 [-1.962, 3.011], loss: 0.616912, mean_squared_error: 18.385492, mean_q: 5.497219\n",
      "132 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1316}\n",
      "  1316/100000: episode: 133, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.808, 2.875], loss: 0.676243, mean_squared_error: 19.018072, mean_q: 5.607774\n",
      "133 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1325}\n",
      "  1325/100000: episode: 134, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.134 [-1.785, 2.772], loss: 0.682799, mean_squared_error: 16.657623, mean_q: 5.246427\n",
      "134 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1335}\n",
      "  1335/100000: episode: 135, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.963, 3.041], loss: 0.755747, mean_squared_error: 17.861094, mean_q: 5.452274\n",
      "135 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1345}\n",
      "  1345/100000: episode: 136, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-2.006, 3.076], loss: 0.674833, mean_squared_error: 18.636196, mean_q: 5.579161\n",
      "136 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 1357}\n",
      "  1357/100000: episode: 137, duration: 0.142s, episode steps: 12, steps per second: 84, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.122 [-1.734, 2.668], loss: 0.640239, mean_squared_error: 17.809214, mean_q: 5.461398\n",
      "137 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1366}\n",
      "  1366/100000: episode: 138, duration: 0.100s, episode steps: 9, steps per second: 90, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.150 [-1.764, 2.834], loss: 0.695174, mean_squared_error: 17.227077, mean_q: 5.345906\n",
      "138 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1375}\n",
      "  1375/100000: episode: 139, duration: 0.110s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.170 [-1.755, 2.898], loss: 0.907160, mean_squared_error: 17.243410, mean_q: 5.445693\n",
      "139 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 1387}\n",
      "  1387/100000: episode: 140, duration: 0.139s, episode steps: 12, steps per second: 87, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.091 [-1.573, 2.469], loss: 0.746023, mean_squared_error: 17.732224, mean_q: 5.529384\n",
      "140 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1396}\n",
      "  1396/100000: episode: 141, duration: 0.109s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.734, 2.816], loss: 0.796414, mean_squared_error: 17.926498, mean_q: 5.588121\n",
      "141 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1406}\n",
      "  1406/100000: episode: 142, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-3.069, 1.907], loss: 1.158270, mean_squared_error: 17.456059, mean_q: 5.588964\n",
      "142 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1415}\n",
      "  1415/100000: episode: 143, duration: 0.110s, episode steps: 9, steps per second: 81, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.255, 1.341], loss: 0.895348, mean_squared_error: 15.776035, mean_q: 5.271041\n",
      "143 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1424}\n",
      "  1424/100000: episode: 144, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.800, 1.743], loss: 1.245516, mean_squared_error: 18.024096, mean_q: 5.738441\n",
      "144 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1433}\n",
      "  1433/100000: episode: 145, duration: 0.105s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.124 [-2.808, 1.805], loss: 1.129022, mean_squared_error: 15.099560, mean_q: 5.290344\n",
      "145 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1444}\n",
      "  1444/100000: episode: 146, duration: 0.128s, episode steps: 11, steps per second: 86, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.127 [-2.814, 1.747], loss: 1.011523, mean_squared_error: 15.522016, mean_q: 5.384341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1454}\n",
      "  1454/100000: episode: 147, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.148 [-2.624, 1.553], loss: 0.919206, mean_squared_error: 16.182621, mean_q: 5.629184\n",
      "147 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 1462}\n",
      "  1462/100000: episode: 148, duration: 0.089s, episode steps: 8, steps per second: 90, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-2.579, 1.540], loss: 1.127437, mean_squared_error: 15.079788, mean_q: 5.354965\n",
      "148 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1471}\n",
      "  1471/100000: episode: 149, duration: 0.104s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.430, 1.556], loss: 1.071798, mean_squared_error: 15.751541, mean_q: 5.533964\n",
      "149 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1482}\n",
      "  1482/100000: episode: 150, duration: 0.132s, episode steps: 11, steps per second: 83, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.120 [-2.821, 1.768], loss: 0.719194, mean_squared_error: 15.625934, mean_q: 5.594736\n",
      "150 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1492}\n",
      "  1492/100000: episode: 151, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.062, 1.951], loss: 0.979326, mean_squared_error: 15.087657, mean_q: 5.464563\n",
      "151 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1501}\n",
      "  1501/100000: episode: 152, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.769, 1.747], loss: 1.303328, mean_squared_error: 14.965603, mean_q: 5.529294\n",
      "152 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1511}\n",
      "  1511/100000: episode: 153, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.131 [-3.015, 1.952], loss: 0.913324, mean_squared_error: 13.963774, mean_q: 5.364106\n",
      "153 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 1519}\n",
      "  1519/100000: episode: 154, duration: 0.098s, episode steps: 8, steps per second: 82, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.579, 1.558], loss: 1.606712, mean_squared_error: 15.150755, mean_q: 5.479155\n",
      "154 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1530}\n",
      "  1530/100000: episode: 155, duration: 0.130s, episode steps: 11, steps per second: 85, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.093 [-2.738, 1.787], loss: 1.332946, mean_squared_error: 15.343653, mean_q: 5.543744\n",
      "155 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1539}\n",
      "  1539/100000: episode: 156, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.849, 1.775], loss: 1.169096, mean_squared_error: 13.580750, mean_q: 5.295592\n",
      "156 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1548}\n",
      "  1548/100000: episode: 157, duration: 0.101s, episode steps: 9, steps per second: 89, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.173 [-2.884, 1.717], loss: 1.282889, mean_squared_error: 15.637482, mean_q: 5.614793\n",
      "157 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 1562}\n",
      "  1562/100000: episode: 158, duration: 0.159s, episode steps: 14, steps per second: 88, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.116 [-2.509, 1.530], loss: 1.035929, mean_squared_error: 14.804402, mean_q: 5.383035\n",
      "158 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1571}\n",
      "  1571/100000: episode: 159, duration: 0.110s, episode steps: 9, steps per second: 81, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-2.825, 1.762], loss: 0.884329, mean_squared_error: 14.079905, mean_q: 5.347933\n",
      "159 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1581}\n",
      "  1581/100000: episode: 160, duration: 0.127s, episode steps: 10, steps per second: 79, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.142 [-2.594, 1.603], loss: 1.286915, mean_squared_error: 15.601509, mean_q: 5.560777\n",
      "160 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1591}\n",
      "  1591/100000: episode: 161, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.990, 1.904], loss: 0.829340, mean_squared_error: 14.931043, mean_q: 5.510083\n",
      "161 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1601}\n",
      "  1601/100000: episode: 162, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.123 [-2.470, 1.545], loss: 1.212109, mean_squared_error: 15.845571, mean_q: 5.586040\n",
      "162 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1610}\n",
      "  1610/100000: episode: 163, duration: 0.105s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.176 [-2.839, 1.721], loss: 1.099371, mean_squared_error: 15.494230, mean_q: 5.556617\n",
      "163 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 1618}\n",
      "  1618/100000: episode: 164, duration: 0.101s, episode steps: 8, steps per second: 79, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.171 [-2.575, 1.554], loss: 1.101218, mean_squared_error: 16.793102, mean_q: 5.855655\n",
      "164 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 1630}\n",
      "  1630/100000: episode: 165, duration: 0.139s, episode steps: 12, steps per second: 86, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-2.432, 1.606], loss: 1.062705, mean_squared_error: 15.206084, mean_q: 5.516088\n",
      "165 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1640}\n",
      "  1640/100000: episode: 166, duration: 0.132s, episode steps: 10, steps per second: 76, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.153 [-2.534, 1.533], loss: 1.307334, mean_squared_error: 16.096256, mean_q: 5.688975\n",
      "166 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 1652}\n",
      "  1652/100000: episode: 167, duration: 0.135s, episode steps: 12, steps per second: 89, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.097 [-3.024, 1.957], loss: 0.966068, mean_squared_error: 15.167380, mean_q: 5.542172\n",
      "167 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1662}\n",
      "  1662/100000: episode: 168, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-3.019, 1.950], loss: 1.252027, mean_squared_error: 14.684860, mean_q: 5.577175\n",
      "168 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1672}\n",
      "  1672/100000: episode: 169, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-3.103, 1.922], loss: 0.998384, mean_squared_error: 14.052408, mean_q: 5.446367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1681}\n",
      "  1681/100000: episode: 170, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.875, 1.764], loss: 0.967388, mean_squared_error: 13.845928, mean_q: 5.269801\n",
      "170 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1690}\n",
      "  1690/100000: episode: 171, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.163 [-2.862, 1.759], loss: 0.892995, mean_squared_error: 14.472955, mean_q: 5.504492\n",
      "171 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 1698}\n",
      "  1698/100000: episode: 172, duration: 0.093s, episode steps: 8, steps per second: 86, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.564, 1.609], loss: 1.005026, mean_squared_error: 13.815805, mean_q: 5.425508\n",
      "172 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1708}\n",
      "  1708/100000: episode: 173, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.148 [-3.086, 1.953], loss: 0.955430, mean_squared_error: 13.055014, mean_q: 5.183726\n",
      "173 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1718}\n",
      "  1718/100000: episode: 174, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.982, 1.935], loss: 1.214197, mean_squared_error: 13.621912, mean_q: 5.349214\n",
      "174 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1727}\n",
      "  1727/100000: episode: 175, duration: 0.097s, episode steps: 9, steps per second: 93, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.182 [-2.865, 1.719], loss: 0.918608, mean_squared_error: 15.336689, mean_q: 5.700016\n",
      "175 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1737}\n",
      "  1737/100000: episode: 176, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-3.026, 1.908], loss: 0.859005, mean_squared_error: 14.339338, mean_q: 5.481061\n",
      "176 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1747}\n",
      "  1747/100000: episode: 177, duration: 0.128s, episode steps: 10, steps per second: 78, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.091, 1.983], loss: 1.298625, mean_squared_error: 14.843893, mean_q: 5.554284\n",
      "177 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1756}\n",
      "  1756/100000: episode: 178, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.838, 1.730], loss: 0.886805, mean_squared_error: 15.459647, mean_q: 5.679235\n",
      "178 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1766}\n",
      "  1766/100000: episode: 179, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.120 [-3.058, 1.990], loss: 1.228972, mean_squared_error: 15.294107, mean_q: 5.760083\n",
      "179 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 1774}\n",
      "  1774/100000: episode: 180, duration: 0.101s, episode steps: 8, steps per second: 79, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.578, 1.612], loss: 1.405729, mean_squared_error: 15.136800, mean_q: 5.496384\n",
      "180 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1784}\n",
      "  1784/100000: episode: 181, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-3.054, 1.923], loss: 0.864618, mean_squared_error: 15.269003, mean_q: 5.613265\n",
      "181 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1793}\n",
      "  1793/100000: episode: 182, duration: 0.104s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.164 [-2.892, 1.729], loss: 1.446587, mean_squared_error: 15.365705, mean_q: 5.632318\n",
      "182 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 1801}\n",
      "  1801/100000: episode: 183, duration: 0.100s, episode steps: 8, steps per second: 80, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.495, 1.613], loss: 0.689620, mean_squared_error: 15.817117, mean_q: 5.647096\n",
      "183 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1812}\n",
      "  1812/100000: episode: 184, duration: 0.126s, episode steps: 11, steps per second: 87, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.120 [-2.809, 1.803], loss: 1.126698, mean_squared_error: 13.828063, mean_q: 5.315445\n",
      "184 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 1824}\n",
      "  1824/100000: episode: 185, duration: 0.141s, episode steps: 12, steps per second: 85, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.108 [-2.111, 1.350], loss: 1.504436, mean_squared_error: 15.254333, mean_q: 5.521984\n",
      "185 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1834}\n",
      "  1834/100000: episode: 186, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.127 [-2.551, 1.599], loss: 1.008198, mean_squared_error: 13.627161, mean_q: 5.210039\n",
      "186 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1844}\n",
      "  1844/100000: episode: 187, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-3.111, 1.910], loss: 0.874912, mean_squared_error: 14.116824, mean_q: 5.349406\n",
      "187 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1854}\n",
      "  1854/100000: episode: 188, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.125 [-2.982, 1.935], loss: 0.921950, mean_squared_error: 12.928334, mean_q: 5.119293\n",
      "188 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1863}\n",
      "  1863/100000: episode: 189, duration: 0.109s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.472, 1.604], loss: 0.854688, mean_squared_error: 12.879117, mean_q: 5.039316\n",
      "189 {'episode_reward': 13.0, 'nb_episode_steps': 13, 'nb_steps': 1876}\n",
      "  1876/100000: episode: 190, duration: 0.150s, episode steps: 13, steps per second: 87, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.108 [-2.802, 1.810], loss: 0.933239, mean_squared_error: 14.650467, mean_q: 5.382794\n",
      "190 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1886}\n",
      "  1886/100000: episode: 191, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-3.016, 1.958], loss: 0.997561, mean_squared_error: 13.717616, mean_q: 5.236627\n",
      "191 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 1897}\n",
      "  1897/100000: episode: 192, duration: 0.126s, episode steps: 11, steps per second: 87, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.095 [-2.722, 1.791], loss: 0.824493, mean_squared_error: 13.986883, mean_q: 5.266151\n",
      "192 {'episode_reward': 13.0, 'nb_episode_steps': 13, 'nb_steps': 1910}\n",
      "  1910/100000: episode: 193, duration: 0.153s, episode steps: 13, steps per second: 85, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.099 [-2.712, 1.757], loss: 0.930208, mean_squared_error: 13.269703, mean_q: 5.137694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1919}\n",
      "  1919/100000: episode: 194, duration: 0.103s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.125 [-2.279, 1.367], loss: 0.785119, mean_squared_error: 15.258711, mean_q: 5.447824\n",
      "194 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1929}\n",
      "  1929/100000: episode: 195, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-3.068, 1.955], loss: 0.636239, mean_squared_error: 13.130396, mean_q: 5.137650\n",
      "195 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1939}\n",
      "  1939/100000: episode: 196, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-3.093, 1.922], loss: 0.905927, mean_squared_error: 13.423162, mean_q: 5.154670\n",
      "196 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1949}\n",
      "  1949/100000: episode: 197, duration: 0.124s, episode steps: 10, steps per second: 80, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.143 [-3.041, 1.969], loss: 1.082804, mean_squared_error: 14.958255, mean_q: 5.521604\n",
      "197 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1958}\n",
      "  1958/100000: episode: 198, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.162 [-2.810, 1.713], loss: 1.195228, mean_squared_error: 14.444729, mean_q: 5.336090\n",
      "198 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 1967}\n",
      "  1967/100000: episode: 199, duration: 0.103s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.129 [-2.767, 1.775], loss: 0.811816, mean_squared_error: 15.629172, mean_q: 5.538439\n",
      "199 {'episode_reward': 15.0, 'nb_episode_steps': 15, 'nb_steps': 1982}\n",
      "  1982/100000: episode: 200, duration: 0.166s, episode steps: 15, steps per second: 90, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.099 [-2.766, 1.722], loss: 0.896339, mean_squared_error: 15.386242, mean_q: 5.489028\n",
      "200 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 1992}\n",
      "  1992/100000: episode: 201, duration: 0.120s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.121 [-2.993, 1.941], loss: 0.857175, mean_squared_error: 14.910973, mean_q: 5.405782\n",
      "201 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2002}\n",
      "  2002/100000: episode: 202, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.146 [-2.501, 1.524], loss: 0.696257, mean_squared_error: 15.546839, mean_q: 5.494205\n",
      "202 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2012}\n",
      "  2012/100000: episode: 203, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.136 [-3.050, 1.991], loss: 0.965321, mean_squared_error: 13.310972, mean_q: 5.089746\n",
      "203 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2023}\n",
      "  2023/100000: episode: 204, duration: 0.128s, episode steps: 11, steps per second: 86, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.146 [-2.769, 1.713], loss: 0.805598, mean_squared_error: 14.484089, mean_q: 5.333109\n",
      "204 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2033}\n",
      "  2033/100000: episode: 205, duration: 0.114s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-3.069, 2.004], loss: 0.923796, mean_squared_error: 14.257715, mean_q: 5.271863\n",
      "205 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2042}\n",
      "  2042/100000: episode: 206, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.827, 1.767], loss: 0.851218, mean_squared_error: 13.799940, mean_q: 5.127441\n",
      "206 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2052}\n",
      "  2052/100000: episode: 207, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.129 [-2.228, 1.399], loss: 0.766625, mean_squared_error: 14.982491, mean_q: 5.365602\n",
      "207 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 2064}\n",
      "  2064/100000: episode: 208, duration: 0.133s, episode steps: 12, steps per second: 90, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.100 [-2.978, 1.973], loss: 0.659124, mean_squared_error: 16.012125, mean_q: 5.587013\n",
      "208 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2074}\n",
      "  2074/100000: episode: 209, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.121 [-2.540, 1.584], loss: 0.729702, mean_squared_error: 15.762019, mean_q: 5.489457\n",
      "209 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 2086}\n",
      "  2086/100000: episode: 210, duration: 0.142s, episode steps: 12, steps per second: 85, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.087 [-2.951, 1.994], loss: 0.724516, mean_squared_error: 14.180036, mean_q: 5.217261\n",
      "210 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2095}\n",
      "  2095/100000: episode: 211, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-2.821, 1.793], loss: 0.656247, mean_squared_error: 14.257928, mean_q: 5.219477\n",
      "211 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2105}\n",
      "  2105/100000: episode: 212, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-3.055, 1.945], loss: 0.585672, mean_squared_error: 14.626079, mean_q: 5.306173\n",
      "212 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 2113}\n",
      "  2113/100000: episode: 213, duration: 0.097s, episode steps: 8, steps per second: 82, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.175 [-2.590, 1.551], loss: 0.684625, mean_squared_error: 15.267378, mean_q: 5.469645\n",
      "213 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2122}\n",
      "  2122/100000: episode: 214, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.825, 1.734], loss: 0.588442, mean_squared_error: 15.018570, mean_q: 5.463045\n",
      "214 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2131}\n",
      "  2131/100000: episode: 215, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.158 [-2.237, 1.332], loss: 0.892302, mean_squared_error: 14.133300, mean_q: 5.287075\n",
      "215 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2141}\n",
      "  2141/100000: episode: 216, duration: 0.114s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-3.011, 1.935], loss: 0.784533, mean_squared_error: 15.537412, mean_q: 5.468780\n",
      "216 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 2153}\n",
      "  2153/100000: episode: 217, duration: 0.137s, episode steps: 12, steps per second: 87, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.093 [-2.942, 1.922], loss: 0.733242, mean_squared_error: 15.382893, mean_q: 5.511926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2162}\n",
      "  2162/100000: episode: 218, duration: 0.104s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.131 [-2.256, 1.347], loss: 0.771861, mean_squared_error: 15.329686, mean_q: 5.533823\n",
      "218 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2173}\n",
      "  2173/100000: episode: 219, duration: 0.127s, episode steps: 11, steps per second: 86, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.125 [-2.841, 1.802], loss: 0.871522, mean_squared_error: 15.042536, mean_q: 5.436727\n",
      "219 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2183}\n",
      "  2183/100000: episode: 220, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.125 [-2.482, 1.557], loss: 0.723590, mean_squared_error: 15.059469, mean_q: 5.439389\n",
      "220 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2192}\n",
      "  2192/100000: episode: 221, duration: 0.105s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.138 [-2.818, 1.798], loss: 0.788043, mean_squared_error: 15.287774, mean_q: 5.543822\n",
      "221 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2201}\n",
      "  2201/100000: episode: 222, duration: 0.104s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.125 [-2.477, 1.611], loss: 1.219825, mean_squared_error: 15.795979, mean_q: 5.631158\n",
      "222 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2210}\n",
      "  2210/100000: episode: 223, duration: 0.108s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-2.800, 1.805], loss: 0.919906, mean_squared_error: 16.974615, mean_q: 5.777885\n",
      "223 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2219}\n",
      "  2219/100000: episode: 224, duration: 0.105s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.855, 1.736], loss: 1.024974, mean_squared_error: 15.526676, mean_q: 5.537727\n",
      "224 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 2235}\n",
      "  2235/100000: episode: 225, duration: 0.180s, episode steps: 16, steps per second: 89, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.083 [-2.534, 1.554], loss: 1.006848, mean_squared_error: 16.238028, mean_q: 5.666940\n",
      "225 {'episode_reward': 13.0, 'nb_episode_steps': 13, 'nb_steps': 2248}\n",
      "  2248/100000: episode: 226, duration: 0.144s, episode steps: 13, steps per second: 90, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.923 [0.000, 1.000], mean observation: -0.091 [-3.271, 2.167], loss: 0.743681, mean_squared_error: 16.404310, mean_q: 5.711219\n",
      "226 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2258}\n",
      "  2258/100000: episode: 227, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.146 [-3.122, 1.951], loss: 0.963587, mean_squared_error: 15.729906, mean_q: 5.525900\n",
      "227 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2269}\n",
      "  2269/100000: episode: 228, duration: 0.130s, episode steps: 11, steps per second: 85, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.142 [-2.498, 1.518], loss: 0.959293, mean_squared_error: 15.938285, mean_q: 5.613720\n",
      "228 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 2283}\n",
      "  2283/100000: episode: 229, duration: 0.154s, episode steps: 14, steps per second: 91, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.104 [-2.074, 1.212], loss: 0.815667, mean_squared_error: 15.861241, mean_q: 5.614621\n",
      "229 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 2291}\n",
      "  2291/100000: episode: 230, duration: 0.095s, episode steps: 8, steps per second: 85, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.595, 1.544], loss: 0.867594, mean_squared_error: 16.664776, mean_q: 5.734941\n",
      "230 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2301}\n",
      "  2301/100000: episode: 231, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.154 [-2.755, 1.712], loss: 0.758278, mean_squared_error: 16.604849, mean_q: 5.755705\n",
      "231 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2311}\n",
      "  2311/100000: episode: 232, duration: 0.123s, episode steps: 10, steps per second: 81, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.147 [-3.037, 1.950], loss: 1.175819, mean_squared_error: 16.443727, mean_q: 5.627236\n",
      "232 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2320}\n",
      "  2320/100000: episode: 233, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.115 [-2.179, 1.379], loss: 0.938806, mean_squared_error: 15.036745, mean_q: 5.402089\n",
      "233 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2330}\n",
      "  2330/100000: episode: 234, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.117 [-2.979, 1.986], loss: 0.684092, mean_squared_error: 16.056179, mean_q: 5.568673\n",
      "234 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2339}\n",
      "  2339/100000: episode: 235, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.144 [-2.824, 1.759], loss: 0.603284, mean_squared_error: 16.598677, mean_q: 5.663171\n",
      "235 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2349}\n",
      "  2349/100000: episode: 236, duration: 0.109s, episode steps: 10, steps per second: 92, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.140 [-3.085, 1.935], loss: 0.873110, mean_squared_error: 15.259082, mean_q: 5.410537\n",
      "236 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2358}\n",
      "  2358/100000: episode: 237, duration: 0.122s, episode steps: 9, steps per second: 74, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.142 [-2.740, 1.742], loss: 0.787664, mean_squared_error: 14.833956, mean_q: 5.309124\n",
      "237 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2368}\n",
      "  2368/100000: episode: 238, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.623, 1.606], loss: 0.662874, mean_squared_error: 15.612543, mean_q: 5.479070\n",
      "238 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2377}\n",
      "  2377/100000: episode: 239, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.740, 1.722], loss: 0.620908, mean_squared_error: 15.807017, mean_q: 5.551589\n",
      "239 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 2385}\n",
      "  2385/100000: episode: 240, duration: 0.090s, episode steps: 8, steps per second: 89, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.559, 1.569], loss: 0.906577, mean_squared_error: 16.648144, mean_q: 5.678293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2395}\n",
      "  2395/100000: episode: 241, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-2.992, 1.978], loss: 0.787574, mean_squared_error: 15.640402, mean_q: 5.556851\n",
      "241 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2406}\n",
      "  2406/100000: episode: 242, duration: 0.126s, episode steps: 11, steps per second: 88, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.113 [-2.738, 1.793], loss: 0.580615, mean_squared_error: 15.826211, mean_q: 5.558809\n",
      "242 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2416}\n",
      "  2416/100000: episode: 243, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.160 [-2.594, 1.536], loss: 0.750334, mean_squared_error: 15.127527, mean_q: 5.343603\n",
      "243 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2426}\n",
      "  2426/100000: episode: 244, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.135 [-2.543, 1.556], loss: 0.724149, mean_squared_error: 16.641012, mean_q: 5.670986\n",
      "244 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2436}\n",
      "  2436/100000: episode: 245, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.151 [-3.013, 1.918], loss: 1.116004, mean_squared_error: 16.876970, mean_q: 5.683133\n",
      "245 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2446}\n",
      "  2446/100000: episode: 246, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.115 [-2.460, 1.548], loss: 1.242063, mean_squared_error: 16.411749, mean_q: 5.566700\n",
      "246 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2457}\n",
      "  2457/100000: episode: 247, duration: 0.133s, episode steps: 11, steps per second: 83, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.132 [-2.788, 1.759], loss: 0.925983, mean_squared_error: 17.066648, mean_q: 5.690161\n",
      "247 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2466}\n",
      "  2466/100000: episode: 248, duration: 0.105s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.113 [-2.746, 1.803], loss: 0.610946, mean_squared_error: 16.359125, mean_q: 5.549304\n",
      "248 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2475}\n",
      "  2475/100000: episode: 249, duration: 0.112s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.157 [-2.810, 1.726], loss: 0.700352, mean_squared_error: 14.964028, mean_q: 5.326499\n",
      "249 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2485}\n",
      "  2485/100000: episode: 250, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.133 [-3.047, 1.929], loss: 0.823750, mean_squared_error: 16.169075, mean_q: 5.529623\n",
      "250 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 2499}\n",
      "  2499/100000: episode: 251, duration: 0.158s, episode steps: 14, steps per second: 88, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.129 [-2.131, 1.135], loss: 0.876798, mean_squared_error: 15.763968, mean_q: 5.405017\n",
      "251 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2509}\n",
      "  2509/100000: episode: 252, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-3.090, 1.972], loss: 0.843521, mean_squared_error: 17.554396, mean_q: 5.736022\n",
      "252 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 2521}\n",
      "  2521/100000: episode: 253, duration: 0.136s, episode steps: 12, steps per second: 88, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.116 [-2.048, 1.159], loss: 0.844105, mean_squared_error: 18.332144, mean_q: 5.836562\n",
      "253 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2531}\n",
      "  2531/100000: episode: 254, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.132 [-3.039, 1.927], loss: 0.929564, mean_squared_error: 17.519669, mean_q: 5.710773\n",
      "254 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2541}\n",
      "  2541/100000: episode: 255, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.139 [-3.045, 1.916], loss: 0.793063, mean_squared_error: 16.463808, mean_q: 5.457644\n",
      "255 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2551}\n",
      "  2551/100000: episode: 256, duration: 0.109s, episode steps: 10, steps per second: 91, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.009, 1.922], loss: 0.887368, mean_squared_error: 16.640650, mean_q: 5.520799\n",
      "256 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2560}\n",
      "  2560/100000: episode: 257, duration: 0.110s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.155 [-2.278, 1.337], loss: 1.048982, mean_squared_error: 18.947145, mean_q: 5.816343\n",
      "257 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2570}\n",
      "  2570/100000: episode: 258, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.148 [-2.570, 1.520], loss: 0.592501, mean_squared_error: 17.621042, mean_q: 5.618416\n",
      "258 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2581}\n",
      "  2581/100000: episode: 259, duration: 0.129s, episode steps: 11, steps per second: 85, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.122 [-3.278, 2.130], loss: 0.785919, mean_squared_error: 17.547651, mean_q: 5.570415\n",
      "259 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2591}\n",
      "  2591/100000: episode: 260, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.120 [-2.489, 1.600], loss: 0.887308, mean_squared_error: 17.876829, mean_q: 5.638782\n",
      "260 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2602}\n",
      "  2602/100000: episode: 261, duration: 0.131s, episode steps: 11, steps per second: 84, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.130 [-1.983, 1.188], loss: 0.836002, mean_squared_error: 17.393997, mean_q: 5.564689\n",
      "261 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 2610}\n",
      "  2610/100000: episode: 262, duration: 0.091s, episode steps: 8, steps per second: 88, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.158 [-1.926, 1.150], loss: 1.038128, mean_squared_error: 16.365395, mean_q: 5.369781\n",
      "262 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2620}\n",
      "  2620/100000: episode: 263, duration: 0.111s, episode steps: 10, steps per second: 90, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.119 [-1.627, 1.010], loss: 0.692396, mean_squared_error: 16.774338, mean_q: 5.466036\n",
      "263 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2630}\n",
      "  2630/100000: episode: 264, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.129 [-1.392, 0.798], loss: 0.805063, mean_squared_error: 17.199200, mean_q: 5.491985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2641}\n",
      "  2641/100000: episode: 265, duration: 0.130s, episode steps: 11, steps per second: 85, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.123 [-1.381, 0.800], loss: 0.953119, mean_squared_error: 17.036612, mean_q: 5.451298\n",
      "265 {'episode_reward': 27.0, 'nb_episode_steps': 27, 'nb_steps': 2668}\n",
      "  2668/100000: episode: 266, duration: 0.302s, episode steps: 27, steps per second: 89, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.027 [-1.899, 2.559], loss: 0.729309, mean_squared_error: 17.027227, mean_q: 5.466307\n",
      "266 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2678}\n",
      "  2678/100000: episode: 267, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.956, 3.083], loss: 0.663800, mean_squared_error: 17.818096, mean_q: 5.559966\n",
      "267 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2688}\n",
      "  2688/100000: episode: 268, duration: 0.116s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.137 [-1.531, 2.586], loss: 0.573963, mean_squared_error: 18.529156, mean_q: 5.728631\n",
      "268 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 2696}\n",
      "  2696/100000: episode: 269, duration: 0.096s, episode steps: 8, steps per second: 84, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.573, 2.581], loss: 0.867064, mean_squared_error: 19.874706, mean_q: 5.942705\n",
      "269 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2706}\n",
      "  2706/100000: episode: 270, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.943, 3.017], loss: 1.180166, mean_squared_error: 20.010162, mean_q: 5.999830\n",
      "270 {'episode_reward': 18.0, 'nb_episode_steps': 18, 'nb_steps': 2724}\n",
      "  2724/100000: episode: 271, duration: 0.197s, episode steps: 18, steps per second: 91, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.092 [-1.026, 0.554], loss: 0.981596, mean_squared_error: 20.479998, mean_q: 6.035826\n",
      "271 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2735}\n",
      "  2735/100000: episode: 272, duration: 0.123s, episode steps: 11, steps per second: 89, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.150 [-1.531, 0.766], loss: 0.780178, mean_squared_error: 19.578108, mean_q: 5.921376\n",
      "272 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 2751}\n",
      "  2751/100000: episode: 273, duration: 0.173s, episode steps: 16, steps per second: 93, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.104 [-1.226, 0.759], loss: 0.977611, mean_squared_error: 20.933315, mean_q: 6.098581\n",
      "273 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2762}\n",
      "  2762/100000: episode: 274, duration: 0.136s, episode steps: 11, steps per second: 81, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.116 [-1.357, 0.780], loss: 1.192581, mean_squared_error: 21.760550, mean_q: 6.267983\n",
      "274 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 2774}\n",
      "  2774/100000: episode: 275, duration: 0.141s, episode steps: 12, steps per second: 85, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.119 [-1.545, 0.973], loss: 1.084844, mean_squared_error: 22.430742, mean_q: 6.342246\n",
      "275 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 2786}\n",
      "  2786/100000: episode: 276, duration: 0.137s, episode steps: 12, steps per second: 88, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.103 [-1.322, 0.825], loss: 1.059559, mean_squared_error: 20.573168, mean_q: 6.110264\n",
      "276 {'episode_reward': 13.0, 'nb_episode_steps': 13, 'nb_steps': 2799}\n",
      "  2799/100000: episode: 277, duration: 0.142s, episode steps: 13, steps per second: 91, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.115 [-1.329, 0.752], loss: 0.983210, mean_squared_error: 20.131899, mean_q: 5.943763\n",
      "277 {'episode_reward': 15.0, 'nb_episode_steps': 15, 'nb_steps': 2814}\n",
      "  2814/100000: episode: 278, duration: 0.171s, episode steps: 15, steps per second: 88, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.099 [-1.242, 0.750], loss: 1.077134, mean_squared_error: 21.753700, mean_q: 6.200828\n",
      "278 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 2828}\n",
      "  2828/100000: episode: 279, duration: 0.155s, episode steps: 14, steps per second: 90, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.202, 0.781], loss: 1.043548, mean_squared_error: 22.080542, mean_q: 6.284534\n",
      "279 {'episode_reward': 29.0, 'nb_episode_steps': 29, 'nb_steps': 2857}\n",
      "  2857/100000: episode: 280, duration: 0.320s, episode steps: 29, steps per second: 91, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.241 [0.000, 1.000], mean observation: -0.025 [-2.849, 3.611], loss: 0.987258, mean_squared_error: 22.564461, mean_q: 6.318172\n",
      "280 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2868}\n",
      "  2868/100000: episode: 281, duration: 0.123s, episode steps: 11, steps per second: 89, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.134 [-1.754, 2.827], loss: 1.205931, mean_squared_error: 22.979021, mean_q: 6.324399\n",
      "281 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2878}\n",
      "  2878/100000: episode: 282, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.912, 3.090], loss: 0.938712, mean_squared_error: 21.779243, mean_q: 6.252530\n",
      "282 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2887}\n",
      "  2887/100000: episode: 283, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.787, 2.749], loss: 1.292445, mean_squared_error: 24.107256, mean_q: 6.549530\n",
      "283 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2896}\n",
      "  2896/100000: episode: 284, duration: 0.097s, episode steps: 9, steps per second: 92, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.792, 2.862], loss: 0.874325, mean_squared_error: 23.807211, mean_q: 6.535017\n",
      "284 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2907}\n",
      "  2907/100000: episode: 285, duration: 0.129s, episode steps: 11, steps per second: 85, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.109 [-1.776, 2.755], loss: 1.005764, mean_squared_error: 24.081270, mean_q: 6.556868\n",
      "285 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 2915}\n",
      "  2915/100000: episode: 286, duration: 0.091s, episode steps: 8, steps per second: 88, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.582, 2.565], loss: 0.737872, mean_squared_error: 25.236769, mean_q: 6.701664\n",
      "286 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2925}\n",
      "  2925/100000: episode: 287, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.136 [-2.004, 3.137], loss: 0.903294, mean_squared_error: 25.700226, mean_q: 6.735710\n",
      "287 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 2939}\n",
      "  2939/100000: episode: 288, duration: 0.159s, episode steps: 14, steps per second: 88, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.128 [-1.914, 3.109], loss: 0.994958, mean_squared_error: 27.604551, mean_q: 7.070075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "288 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2949}\n",
      "  2949/100000: episode: 289, duration: 0.121s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.966, 3.026], loss: 0.848625, mean_squared_error: 27.639563, mean_q: 7.106053\n",
      "289 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 2961}\n",
      "  2961/100000: episode: 290, duration: 0.139s, episode steps: 12, steps per second: 86, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.129 [-1.589, 2.610], loss: 0.924653, mean_squared_error: 26.589109, mean_q: 6.891869\n",
      "290 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 2972}\n",
      "  2972/100000: episode: 291, duration: 0.123s, episode steps: 11, steps per second: 90, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.126 [-1.758, 2.742], loss: 1.183235, mean_squared_error: 27.101931, mean_q: 6.945430\n",
      "291 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 2980}\n",
      "  2980/100000: episode: 292, duration: 0.094s, episode steps: 8, steps per second: 85, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.520, 2.591], loss: 1.102855, mean_squared_error: 28.137896, mean_q: 7.129524\n",
      "292 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 2990}\n",
      "  2990/100000: episode: 293, duration: 0.108s, episode steps: 10, steps per second: 93, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.944, 3.121], loss: 1.284025, mean_squared_error: 27.260891, mean_q: 6.971301\n",
      "293 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 2999}\n",
      "  2999/100000: episode: 294, duration: 0.109s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.131 [-1.579, 2.482], loss: 0.984423, mean_squared_error: 29.070028, mean_q: 7.247347\n",
      "294 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3010}\n",
      "  3010/100000: episode: 295, duration: 0.128s, episode steps: 11, steps per second: 86, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.106 [-1.807, 2.818], loss: 0.796437, mean_squared_error: 28.184645, mean_q: 7.058118\n",
      "295 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3020}\n",
      "  3020/100000: episode: 296, duration: 0.122s, episode steps: 10, steps per second: 82, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.904, 3.054], loss: 0.881498, mean_squared_error: 26.718525, mean_q: 6.877543\n",
      "296 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3030}\n",
      "  3030/100000: episode: 297, duration: 0.112s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.133 [-1.937, 3.041], loss: 1.000180, mean_squared_error: 27.808264, mean_q: 7.096684\n",
      "297 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3040}\n",
      "  3040/100000: episode: 298, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.953, 3.072], loss: 0.868459, mean_squared_error: 26.265060, mean_q: 6.869887\n",
      "298 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3049}\n",
      "  3049/100000: episode: 299, duration: 0.109s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.777, 2.808], loss: 1.085486, mean_squared_error: 27.507147, mean_q: 6.976357\n",
      "299 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3058}\n",
      "  3058/100000: episode: 300, duration: 0.113s, episode steps: 9, steps per second: 80, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.142 [-1.735, 2.801], loss: 1.267343, mean_squared_error: 28.716793, mean_q: 7.213356\n",
      "300 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3069}\n",
      "  3069/100000: episode: 301, duration: 0.125s, episode steps: 11, steps per second: 88, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.095 [-1.768, 2.753], loss: 0.835520, mean_squared_error: 31.095976, mean_q: 7.551907\n",
      "301 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3080}\n",
      "  3080/100000: episode: 302, duration: 0.131s, episode steps: 11, steps per second: 84, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.124 [-1.789, 2.760], loss: 0.934957, mean_squared_error: 29.197832, mean_q: 7.283713\n",
      "302 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3089}\n",
      "  3089/100000: episode: 303, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.803, 2.821], loss: 0.982416, mean_squared_error: 27.225986, mean_q: 6.997879\n",
      "303 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3099}\n",
      "  3099/100000: episode: 304, duration: 0.118s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.944, 3.066], loss: 1.244437, mean_squared_error: 29.383301, mean_q: 7.313170\n",
      "304 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3109}\n",
      "  3109/100000: episode: 305, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.125 [-1.567, 2.539], loss: 1.078782, mean_squared_error: 26.621426, mean_q: 6.914755\n",
      "305 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3118}\n",
      "  3118/100000: episode: 306, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.123 [-1.375, 2.265], loss: 1.125799, mean_squared_error: 27.787550, mean_q: 7.126848\n",
      "306 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 3126}\n",
      "  3126/100000: episode: 307, duration: 0.104s, episode steps: 8, steps per second: 77, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.144 [-1.338, 2.168], loss: 1.324877, mean_squared_error: 27.115288, mean_q: 6.987261\n",
      "307 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3137}\n",
      "  3137/100000: episode: 308, duration: 0.128s, episode steps: 11, steps per second: 86, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.114 [-2.160, 3.273], loss: 1.647200, mean_squared_error: 27.657879, mean_q: 7.064296\n",
      "308 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3146}\n",
      "  3146/100000: episode: 309, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.162 [-1.780, 2.884], loss: 1.148162, mean_squared_error: 30.395718, mean_q: 7.372336\n",
      "309 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3156}\n",
      "  3156/100000: episode: 310, duration: 0.117s, episode steps: 10, steps per second: 86, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.914, 3.068], loss: 1.070335, mean_squared_error: 31.738419, mean_q: 7.532564\n",
      "310 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3165}\n",
      "  3165/100000: episode: 311, duration: 0.115s, episode steps: 9, steps per second: 78, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.799, 2.821], loss: 0.782516, mean_squared_error: 30.043074, mean_q: 7.304147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3174}\n",
      "  3174/100000: episode: 312, duration: 0.110s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.808, 2.851], loss: 0.648555, mean_squared_error: 29.864908, mean_q: 7.300298\n",
      "312 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3183}\n",
      "  3183/100000: episode: 313, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.128 [-1.381, 2.256], loss: 1.004437, mean_squared_error: 33.670540, mean_q: 7.801104\n",
      "313 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 3195}\n",
      "  3195/100000: episode: 314, duration: 0.135s, episode steps: 12, steps per second: 89, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.099 [-1.923, 2.970], loss: 0.879284, mean_squared_error: 28.561264, mean_q: 7.153702\n",
      "314 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3204}\n",
      "  3204/100000: episode: 315, duration: 0.103s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.713, 2.802], loss: 0.816811, mean_squared_error: 30.377359, mean_q: 7.364060\n",
      "315 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3215}\n",
      "  3215/100000: episode: 316, duration: 0.121s, episode steps: 11, steps per second: 91, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.129 [-1.720, 2.779], loss: 0.947761, mean_squared_error: 31.355724, mean_q: 7.493135\n",
      "316 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3224}\n",
      "  3224/100000: episode: 317, duration: 0.107s, episode steps: 9, steps per second: 84, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.741, 2.761], loss: 1.279502, mean_squared_error: 30.756826, mean_q: 7.505205\n",
      "317 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3234}\n",
      "  3234/100000: episode: 318, duration: 0.118s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.931, 3.049], loss: 0.952189, mean_squared_error: 31.711643, mean_q: 7.521639\n",
      "318 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3243}\n",
      "  3243/100000: episode: 319, duration: 0.102s, episode steps: 9, steps per second: 88, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.734, 2.838], loss: 1.166569, mean_squared_error: 31.334372, mean_q: 7.559089\n",
      "319 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3253}\n",
      "  3253/100000: episode: 320, duration: 0.120s, episode steps: 10, steps per second: 83, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.128 [-1.547, 2.533], loss: 0.881298, mean_squared_error: 33.857365, mean_q: 7.775390\n",
      "320 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3263}\n",
      "  3263/100000: episode: 321, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.971, 2.988], loss: 1.192016, mean_squared_error: 32.432941, mean_q: 7.664169\n",
      "321 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3273}\n",
      "  3273/100000: episode: 322, duration: 0.113s, episode steps: 10, steps per second: 88, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.125 [-1.989, 3.075], loss: 0.790775, mean_squared_error: 35.325184, mean_q: 8.027800\n",
      "322 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3284}\n",
      "  3284/100000: episode: 323, duration: 0.125s, episode steps: 11, steps per second: 88, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [-2.145, 3.314], loss: 1.085736, mean_squared_error: 34.567799, mean_q: 7.874478\n",
      "323 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 3296}\n",
      "  3296/100000: episode: 324, duration: 0.140s, episode steps: 12, steps per second: 86, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.101 [-1.966, 3.015], loss: 0.930610, mean_squared_error: 34.501495, mean_q: 7.852501\n",
      "324 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3305}\n",
      "  3305/100000: episode: 325, duration: 0.100s, episode steps: 9, steps per second: 90, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.146 [-1.130, 1.865], loss: 1.313152, mean_squared_error: 36.540039, mean_q: 8.028123\n",
      "325 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3315}\n",
      "  3315/100000: episode: 326, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.132 [-1.980, 3.081], loss: 0.800278, mean_squared_error: 34.581078, mean_q: 7.889459\n",
      "326 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3325}\n",
      "  3325/100000: episode: 327, duration: 0.117s, episode steps: 10, steps per second: 85, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.950, 3.128], loss: 0.893360, mean_squared_error: 33.103535, mean_q: 7.623518\n",
      "327 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3334}\n",
      "  3334/100000: episode: 328, duration: 0.104s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.801, 2.817], loss: 0.953580, mean_squared_error: 33.706097, mean_q: 7.731970\n",
      "328 {'episode_reward': 15.0, 'nb_episode_steps': 15, 'nb_steps': 3349}\n",
      "  3349/100000: episode: 329, duration: 0.166s, episode steps: 15, steps per second: 90, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.133 [0.000, 1.000], mean observation: 0.087 [-2.150, 3.242], loss: 0.743947, mean_squared_error: 33.305763, mean_q: 7.687263\n",
      "329 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3359}\n",
      "  3359/100000: episode: 330, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.973, 3.112], loss: 0.569363, mean_squared_error: 37.307419, mean_q: 8.158763\n",
      "330 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3369}\n",
      "  3369/100000: episode: 331, duration: 0.119s, episode steps: 10, steps per second: 84, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.160 [-1.915, 3.098], loss: 0.700511, mean_squared_error: 37.155552, mean_q: 8.121614\n",
      "331 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 3377}\n",
      "  3377/100000: episode: 332, duration: 0.089s, episode steps: 8, steps per second: 90, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.175 [-1.545, 2.592], loss: 0.885823, mean_squared_error: 37.463791, mean_q: 8.136353\n",
      "332 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3387}\n",
      "  3387/100000: episode: 333, duration: 0.124s, episode steps: 10, steps per second: 81, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.145 [-1.537, 2.635], loss: 0.802500, mean_squared_error: 35.428852, mean_q: 7.944734\n",
      "333 {'episode_reward': 13.0, 'nb_episode_steps': 13, 'nb_steps': 3400}\n",
      "  3400/100000: episode: 334, duration: 0.156s, episode steps: 13, steps per second: 84, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.077 [0.000, 1.000], mean observation: 0.126 [-2.109, 3.305], loss: 0.794745, mean_squared_error: 39.266277, mean_q: 8.339930\n",
      "334 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3410}\n",
      "  3410/100000: episode: 335, duration: 0.113s, episode steps: 10, steps per second: 89, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.950, 3.109], loss: 0.874725, mean_squared_error: 37.237835, mean_q: 8.158336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3419}\n",
      "  3419/100000: episode: 336, duration: 0.106s, episode steps: 9, steps per second: 85, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.147 [-1.782, 2.820], loss: 0.848519, mean_squared_error: 37.801601, mean_q: 8.266828\n",
      "336 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3428}\n",
      "  3428/100000: episode: 337, duration: 0.113s, episode steps: 9, steps per second: 79, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.126 [-1.771, 2.772], loss: 0.981376, mean_squared_error: 35.571079, mean_q: 7.876650\n",
      "337 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 3440}\n",
      "  3440/100000: episode: 338, duration: 0.134s, episode steps: 12, steps per second: 89, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.108 [-1.597, 2.503], loss: 0.774810, mean_squared_error: 35.764027, mean_q: 7.913615\n",
      "338 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3451}\n",
      "  3451/100000: episode: 339, duration: 0.126s, episode steps: 11, steps per second: 87, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.152 [-1.724, 2.879], loss: 0.608954, mean_squared_error: 37.374641, mean_q: 8.121837\n",
      "339 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3460}\n",
      "  3460/100000: episode: 340, duration: 0.105s, episode steps: 9, steps per second: 86, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.156 [-1.742, 2.791], loss: 0.992823, mean_squared_error: 36.596691, mean_q: 8.007970\n",
      "340 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3471}\n",
      "  3471/100000: episode: 341, duration: 0.126s, episode steps: 11, steps per second: 87, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.129 [-1.756, 2.819], loss: 0.874433, mean_squared_error: 34.136623, mean_q: 7.741335\n",
      "341 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3480}\n",
      "  3480/100000: episode: 342, duration: 0.110s, episode steps: 9, steps per second: 82, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.759, 2.766], loss: 0.742784, mean_squared_error: 35.866848, mean_q: 7.905221\n",
      "342 {'episode_reward': 10.0, 'nb_episode_steps': 10, 'nb_steps': 3490}\n",
      "  3490/100000: episode: 343, duration: 0.115s, episode steps: 10, steps per second: 87, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.128 [-1.929, 3.021], loss: 0.865798, mean_squared_error: 35.730537, mean_q: 7.915246\n",
      "343 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3499}\n",
      "  3499/100000: episode: 344, duration: 0.108s, episode steps: 9, steps per second: 83, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.749, 2.786], loss: 0.557532, mean_squared_error: 31.740496, mean_q: 7.412243\n",
      "344 {'episode_reward': 15.0, 'nb_episode_steps': 15, 'nb_steps': 3514}\n",
      "  3514/100000: episode: 345, duration: 0.166s, episode steps: 15, steps per second: 90, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.082 [-1.805, 2.781], loss: 0.654251, mean_squared_error: 32.220158, mean_q: 7.434577\n",
      "345 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3523}\n",
      "  3523/100000: episode: 346, duration: 0.119s, episode steps: 9, steps per second: 76, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.720, 2.794], loss: 0.559676, mean_squared_error: 30.929844, mean_q: 7.324349\n",
      "346 {'episode_reward': 15.0, 'nb_episode_steps': 15, 'nb_steps': 3538}\n",
      "  3538/100000: episode: 347, duration: 0.155s, episode steps: 15, steps per second: 97, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.079 [-1.806, 2.882], loss: 0.649582, mean_squared_error: 31.366657, mean_q: 7.446558\n",
      "347 {'episode_reward': 13.0, 'nb_episode_steps': 13, 'nb_steps': 3551}\n",
      "  3551/100000: episode: 348, duration: 0.151s, episode steps: 13, steps per second: 86, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.121 [-1.722, 2.805], loss: 0.682489, mean_squared_error: 30.729219, mean_q: 7.321854\n",
      "348 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3562}\n",
      "  3562/100000: episode: 349, duration: 0.124s, episode steps: 11, steps per second: 89, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.131 [-1.766, 2.792], loss: 0.678944, mean_squared_error: 33.472206, mean_q: 7.643363\n",
      "349 {'episode_reward': 8.0, 'nb_episode_steps': 8, 'nb_steps': 3570}\n",
      "  3570/100000: episode: 350, duration: 0.094s, episode steps: 8, steps per second: 85, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.520, 2.506], loss: 0.784420, mean_squared_error: 33.775864, mean_q: 7.688033\n",
      "350 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3579}\n",
      "  3579/100000: episode: 351, duration: 0.104s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.165 [-1.804, 2.900], loss: 0.720181, mean_squared_error: 35.593945, mean_q: 7.902862\n",
      "351 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3588}\n",
      "  3588/100000: episode: 352, duration: 0.103s, episode steps: 9, steps per second: 87, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.772, 2.849], loss: 0.746207, mean_squared_error: 33.574375, mean_q: 7.611302\n",
      "352 {'episode_reward': 9.0, 'nb_episode_steps': 9, 'nb_steps': 3597}\n",
      "  3597/100000: episode: 353, duration: 0.119s, episode steps: 9, steps per second: 76, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.145 [-1.798, 2.832], loss: 0.579562, mean_squared_error: 34.402248, mean_q: 7.745298\n",
      "353 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 3611}\n",
      "  3611/100000: episode: 354, duration: 0.151s, episode steps: 14, steps per second: 92, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.102 [-1.605, 2.553], loss: 0.852076, mean_squared_error: 35.800114, mean_q: 7.890269\n",
      "354 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 3627}\n",
      "  3627/100000: episode: 355, duration: 0.180s, episode steps: 16, steps per second: 89, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.087 [-1.924, 3.061], loss: 0.653866, mean_squared_error: 34.653316, mean_q: 7.754657\n",
      "355 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 3649}\n",
      "  3649/100000: episode: 356, duration: 0.247s, episode steps: 22, steps per second: 89, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.018 [-1.940, 2.852], loss: 0.626908, mean_squared_error: 34.250980, mean_q: 7.697304\n",
      "356 {'episode_reward': 27.0, 'nb_episode_steps': 27, 'nb_steps': 3676}\n",
      "  3676/100000: episode: 357, duration: 0.294s, episode steps: 27, steps per second: 92, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: -0.034 [-1.930, 2.359], loss: 0.734951, mean_squared_error: 30.662746, mean_q: 7.232062\n",
      "357 {'episode_reward': 21.0, 'nb_episode_steps': 21, 'nb_steps': 3697}\n",
      "  3697/100000: episode: 358, duration: 0.230s, episode steps: 21, steps per second: 91, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.047 [-1.566, 2.377], loss: 0.611363, mean_squared_error: 32.638714, mean_q: 7.519876\n",
      "358 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 3711}\n",
      "  3711/100000: episode: 359, duration: 0.155s, episode steps: 14, steps per second: 90, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.102 [-1.332, 2.212], loss: 0.719788, mean_squared_error: 32.094402, mean_q: 7.400861\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359 {'episode_reward': 11.0, 'nb_episode_steps': 11, 'nb_steps': 3722}\n",
      "  3722/100000: episode: 360, duration: 0.127s, episode steps: 11, steps per second: 87, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.126 [-1.335, 2.241], loss: 0.508158, mean_squared_error: 33.046440, mean_q: 7.535036\n",
      "360 {'episode_reward': 25.0, 'nb_episode_steps': 25, 'nb_steps': 3747}\n",
      "  3747/100000: episode: 361, duration: 0.262s, episode steps: 25, steps per second: 96, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.052 [-1.158, 1.803], loss: 0.641280, mean_squared_error: 33.077282, mean_q: 7.524918\n",
      "361 {'episode_reward': 117.0, 'nb_episode_steps': 117, 'nb_steps': 3864}\n",
      "  3864/100000: episode: 362, duration: 1.223s, episode steps: 117, steps per second: 96, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.139 [-1.190, 1.157], loss: 0.637837, mean_squared_error: 34.818562, mean_q: 7.775799\n",
      "362 {'episode_reward': 65.0, 'nb_episode_steps': 65, 'nb_steps': 3929}\n",
      "  3929/100000: episode: 363, duration: 0.675s, episode steps: 65, steps per second: 96, episode reward: 65.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.114 [-0.744, 1.124], loss: 0.746870, mean_squared_error: 40.864822, mean_q: 8.495762\n",
      "363 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 3949}\n",
      "  3949/100000: episode: 364, duration: 0.224s, episode steps: 20, steps per second: 89, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.051 [-0.638, 1.167], loss: 0.961681, mean_squared_error: 48.477436, mean_q: 9.300692\n",
      "364 {'episode_reward': 17.0, 'nb_episode_steps': 17, 'nb_steps': 3966}\n",
      "  3966/100000: episode: 365, duration: 0.183s, episode steps: 17, steps per second: 93, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.074 [-0.758, 1.172], loss: 0.805987, mean_squared_error: 49.071358, mean_q: 9.299184\n",
      "365 {'episode_reward': 24.0, 'nb_episode_steps': 24, 'nb_steps': 3990}\n",
      "  3990/100000: episode: 366, duration: 0.267s, episode steps: 24, steps per second: 90, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.070 [-0.560, 0.981], loss: 0.973797, mean_squared_error: 51.464069, mean_q: 9.501305\n",
      "366 {'episode_reward': 26.0, 'nb_episode_steps': 26, 'nb_steps': 4016}\n",
      "  4016/100000: episode: 367, duration: 0.276s, episode steps: 26, steps per second: 94, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.450, 0.852], loss: 0.867406, mean_squared_error: 51.013115, mean_q: 9.409600\n",
      "367 {'episode_reward': 19.0, 'nb_episode_steps': 19, 'nb_steps': 4035}\n",
      "  4035/100000: episode: 368, duration: 0.225s, episode steps: 19, steps per second: 84, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.101 [-0.379, 0.785], loss: 0.890217, mean_squared_error: 54.911274, mean_q: 9.767085\n",
      "368 {'episode_reward': 17.0, 'nb_episode_steps': 17, 'nb_steps': 4052}\n",
      "  4052/100000: episode: 369, duration: 0.194s, episode steps: 17, steps per second: 88, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.078 [-0.783, 1.449], loss: 1.218655, mean_squared_error: 56.049065, mean_q: 9.843089\n",
      "369 {'episode_reward': 23.0, 'nb_episode_steps': 23, 'nb_steps': 4075}\n",
      "  4075/100000: episode: 370, duration: 0.245s, episode steps: 23, steps per second: 94, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.076 [-0.554, 1.221], loss: 1.031913, mean_squared_error: 57.596249, mean_q: 10.033115\n",
      "370 {'episode_reward': 23.0, 'nb_episode_steps': 23, 'nb_steps': 4098}\n",
      "  4098/100000: episode: 371, duration: 0.254s, episode steps: 23, steps per second: 91, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.083 [-0.620, 0.930], loss: 1.220371, mean_squared_error: 63.866234, mean_q: 10.547934\n",
      "371 {'episode_reward': 28.0, 'nb_episode_steps': 28, 'nb_steps': 4126}\n",
      "  4126/100000: episode: 372, duration: 0.301s, episode steps: 28, steps per second: 93, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.073 [-0.551, 1.185], loss: 0.994881, mean_squared_error: 70.050293, mean_q: 11.129207\n",
      "372 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 4146}\n",
      "  4146/100000: episode: 373, duration: 0.227s, episode steps: 20, steps per second: 88, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.096 [-0.403, 1.088], loss: 1.946414, mean_squared_error: 74.376083, mean_q: 11.367616\n",
      "373 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 4158}\n",
      "  4158/100000: episode: 374, duration: 0.134s, episode steps: 12, steps per second: 89, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.114 [-0.803, 1.261], loss: 0.985950, mean_squared_error: 73.076042, mean_q: 11.270762\n",
      "374 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 4174}\n",
      "  4174/100000: episode: 375, duration: 0.180s, episode steps: 16, steps per second: 89, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.095 [-0.630, 0.972], loss: 1.691441, mean_squared_error: 76.859879, mean_q: 11.492486\n",
      "375 {'episode_reward': 35.0, 'nb_episode_steps': 35, 'nb_steps': 4209}\n",
      "  4209/100000: episode: 376, duration: 0.382s, episode steps: 35, steps per second: 92, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.065 [-0.403, 0.769], loss: 1.225266, mean_squared_error: 75.440903, mean_q: 11.383323\n",
      "376 {'episode_reward': 19.0, 'nb_episode_steps': 19, 'nb_steps': 4228}\n",
      "  4228/100000: episode: 377, duration: 0.207s, episode steps: 19, steps per second: 92, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.113 [-0.358, 0.868], loss: 1.702365, mean_squared_error: 78.902885, mean_q: 11.604107\n",
      "377 {'episode_reward': 17.0, 'nb_episode_steps': 17, 'nb_steps': 4245}\n",
      "  4245/100000: episode: 378, duration: 0.198s, episode steps: 17, steps per second: 86, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.099 [-0.589, 1.091], loss: 2.013294, mean_squared_error: 85.624512, mean_q: 12.221670\n",
      "378 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 4257}\n",
      "  4257/100000: episode: 379, duration: 0.137s, episode steps: 12, steps per second: 87, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.120 [-0.556, 1.074], loss: 0.917981, mean_squared_error: 81.419838, mean_q: 11.924273\n",
      "379 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 4279}\n",
      "  4279/100000: episode: 380, duration: 0.236s, episode steps: 22, steps per second: 93, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.089 [-0.389, 0.856], loss: 1.716452, mean_squared_error: 84.511909, mean_q: 12.158372\n",
      "380 {'episode_reward': 19.0, 'nb_episode_steps': 19, 'nb_steps': 4298}\n",
      "  4298/100000: episode: 381, duration: 0.221s, episode steps: 19, steps per second: 86, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.088 [-0.624, 0.999], loss: 1.541206, mean_squared_error: 86.075554, mean_q: 12.260287\n",
      "381 {'episode_reward': 24.0, 'nb_episode_steps': 24, 'nb_steps': 4322}\n",
      "  4322/100000: episode: 382, duration: 0.276s, episode steps: 24, steps per second: 87, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.550, 0.964], loss: 1.871214, mean_squared_error: 91.812645, mean_q: 12.608054\n",
      "382 {'episode_reward': 17.0, 'nb_episode_steps': 17, 'nb_steps': 4339}\n",
      "  4339/100000: episode: 383, duration: 0.189s, episode steps: 17, steps per second: 90, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.099 [-0.405, 0.859], loss: 1.849154, mean_squared_error: 98.339775, mean_q: 13.134628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "383 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 4355}\n",
      "  4355/100000: episode: 384, duration: 0.177s, episode steps: 16, steps per second: 91, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.108 [-0.373, 0.841], loss: 3.118850, mean_squared_error: 103.171967, mean_q: 13.409611\n",
      "384 {'episode_reward': 31.0, 'nb_episode_steps': 31, 'nb_steps': 4386}\n",
      "  4386/100000: episode: 385, duration: 0.333s, episode steps: 31, steps per second: 93, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.091 [-0.240, 0.853], loss: 2.198848, mean_squared_error: 107.212196, mean_q: 13.743685\n",
      "385 {'episode_reward': 17.0, 'nb_episode_steps': 17, 'nb_steps': 4403}\n",
      "  4403/100000: episode: 386, duration: 0.181s, episode steps: 17, steps per second: 94, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.106 [-0.379, 1.008], loss: 2.477663, mean_squared_error: 110.808533, mean_q: 13.984390\n",
      "386 {'episode_reward': 12.0, 'nb_episode_steps': 12, 'nb_steps': 4415}\n",
      "  4415/100000: episode: 387, duration: 0.143s, episode steps: 12, steps per second: 84, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.122 [-0.563, 1.247], loss: 1.076477, mean_squared_error: 112.128418, mean_q: 14.105586\n",
      "387 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 4437}\n",
      "  4437/100000: episode: 388, duration: 0.244s, episode steps: 22, steps per second: 90, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.069 [-0.599, 1.054], loss: 2.728778, mean_squared_error: 125.803223, mean_q: 14.890052\n",
      "388 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 4457}\n",
      "  4457/100000: episode: 389, duration: 0.217s, episode steps: 20, steps per second: 92, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.097 [-0.553, 0.861], loss: 2.749193, mean_squared_error: 129.014862, mean_q: 15.078028\n",
      "389 {'episode_reward': 21.0, 'nb_episode_steps': 21, 'nb_steps': 4478}\n",
      "  4478/100000: episode: 390, duration: 0.234s, episode steps: 21, steps per second: 90, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.085 [-0.389, 0.916], loss: 3.724042, mean_squared_error: 126.929291, mean_q: 14.793107\n",
      "390 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 4494}\n",
      "  4494/100000: episode: 391, duration: 0.171s, episode steps: 16, steps per second: 94, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.596, 0.888], loss: 3.212744, mean_squared_error: 127.805855, mean_q: 14.974231\n",
      "391 {'episode_reward': 17.0, 'nb_episode_steps': 17, 'nb_steps': 4511}\n",
      "  4511/100000: episode: 392, duration: 0.200s, episode steps: 17, steps per second: 85, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.109 [-0.560, 1.100], loss: 2.350035, mean_squared_error: 131.897598, mean_q: 15.209633\n",
      "392 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 4533}\n",
      "  4533/100000: episode: 393, duration: 0.238s, episode steps: 22, steps per second: 92, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.041 [-0.627, 1.148], loss: 2.996015, mean_squared_error: 136.115417, mean_q: 15.529346\n",
      "393 {'episode_reward': 18.0, 'nb_episode_steps': 18, 'nb_steps': 4551}\n",
      "  4551/100000: episode: 394, duration: 0.191s, episode steps: 18, steps per second: 94, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.070 [-0.812, 1.148], loss: 2.317370, mean_squared_error: 141.974350, mean_q: 15.908613\n",
      "394 {'episode_reward': 27.0, 'nb_episode_steps': 27, 'nb_steps': 4578}\n",
      "  4578/100000: episode: 395, duration: 0.288s, episode steps: 27, steps per second: 94, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.049 [-0.424, 1.040], loss: 2.157870, mean_squared_error: 150.197739, mean_q: 16.233091\n",
      "395 {'episode_reward': 18.0, 'nb_episode_steps': 18, 'nb_steps': 4596}\n",
      "  4596/100000: episode: 396, duration: 0.197s, episode steps: 18, steps per second: 91, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.065 [-0.610, 1.173], loss: 4.690833, mean_squared_error: 147.161850, mean_q: 15.767714\n",
      "396 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 4612}\n",
      "  4612/100000: episode: 397, duration: 0.184s, episode steps: 16, steps per second: 87, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.085 [-0.646, 1.277], loss: 1.808019, mean_squared_error: 157.387756, mean_q: 16.692970\n",
      "397 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 4626}\n",
      "  4626/100000: episode: 398, duration: 0.154s, episode steps: 14, steps per second: 91, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.119 [-0.598, 1.137], loss: 4.355151, mean_squared_error: 170.207001, mean_q: 17.214067\n",
      "398 {'episode_reward': 19.0, 'nb_episode_steps': 19, 'nb_steps': 4645}\n",
      "  4645/100000: episode: 399, duration: 0.206s, episode steps: 19, steps per second: 92, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.067 [-0.604, 0.963], loss: 4.451699, mean_squared_error: 166.346252, mean_q: 16.982447\n",
      "399 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 4665}\n",
      "  4665/100000: episode: 400, duration: 0.218s, episode steps: 20, steps per second: 92, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.068 [-0.454, 1.091], loss: 3.152175, mean_squared_error: 160.717880, mean_q: 16.644611\n",
      "400 {'episode_reward': 21.0, 'nb_episode_steps': 21, 'nb_steps': 4686}\n",
      "  4686/100000: episode: 401, duration: 0.237s, episode steps: 21, steps per second: 88, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.085 [-0.378, 1.025], loss: 2.663157, mean_squared_error: 170.893295, mean_q: 17.285894\n",
      "401 {'episode_reward': 18.0, 'nb_episode_steps': 18, 'nb_steps': 4704}\n",
      "  4704/100000: episode: 402, duration: 0.191s, episode steps: 18, steps per second: 94, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.093 [-0.564, 1.080], loss: 3.035986, mean_squared_error: 170.966324, mean_q: 17.249062\n",
      "402 {'episode_reward': 25.0, 'nb_episode_steps': 25, 'nb_steps': 4729}\n",
      "  4729/100000: episode: 403, duration: 0.274s, episode steps: 25, steps per second: 91, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.052 [-0.456, 1.055], loss: 2.788102, mean_squared_error: 180.532425, mean_q: 17.776793\n",
      "403 {'episode_reward': 21.0, 'nb_episode_steps': 21, 'nb_steps': 4750}\n",
      "  4750/100000: episode: 404, duration: 0.233s, episode steps: 21, steps per second: 90, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.071 [-0.574, 1.082], loss: 4.530404, mean_squared_error: 189.069550, mean_q: 18.107595\n",
      "404 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 4766}\n",
      "  4766/100000: episode: 405, duration: 0.171s, episode steps: 16, steps per second: 93, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.092 [-0.566, 1.212], loss: 4.129467, mean_squared_error: 193.462799, mean_q: 18.564951\n",
      "405 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 4780}\n",
      "  4780/100000: episode: 406, duration: 0.160s, episode steps: 14, steps per second: 87, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.086 [-0.627, 1.052], loss: 4.486587, mean_squared_error: 203.862869, mean_q: 19.206915\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 4800}\n",
      "  4800/100000: episode: 407, duration: 0.210s, episode steps: 20, steps per second: 95, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.098 [-0.399, 1.103], loss: 4.149978, mean_squared_error: 199.488708, mean_q: 18.682795\n",
      "407 {'episode_reward': 14.0, 'nb_episode_steps': 14, 'nb_steps': 4814}\n",
      "  4814/100000: episode: 408, duration: 0.160s, episode steps: 14, steps per second: 87, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.095 [-0.609, 1.025], loss: 4.686360, mean_squared_error: 199.420502, mean_q: 18.821939\n",
      "408 {'episode_reward': 19.0, 'nb_episode_steps': 19, 'nb_steps': 4833}\n",
      "  4833/100000: episode: 409, duration: 0.221s, episode steps: 19, steps per second: 86, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.068 [-0.610, 1.119], loss: 5.432656, mean_squared_error: 204.139038, mean_q: 18.959406\n",
      "409 {'episode_reward': 15.0, 'nb_episode_steps': 15, 'nb_steps': 4848}\n",
      "  4848/100000: episode: 410, duration: 0.158s, episode steps: 15, steps per second: 95, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.105 [-0.612, 1.066], loss: 6.329765, mean_squared_error: 195.100784, mean_q: 18.279596\n",
      "410 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 4870}\n",
      "  4870/100000: episode: 411, duration: 0.239s, episode steps: 22, steps per second: 92, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.082 [-0.381, 1.086], loss: 4.051157, mean_squared_error: 199.060043, mean_q: 18.748390\n",
      "411 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 4890}\n",
      "  4890/100000: episode: 412, duration: 0.226s, episode steps: 20, steps per second: 88, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.079 [-0.438, 0.976], loss: 4.276997, mean_squared_error: 200.135422, mean_q: 18.662693\n",
      "412 {'episode_reward': 23.0, 'nb_episode_steps': 23, 'nb_steps': 4913}\n",
      "  4913/100000: episode: 413, duration: 0.256s, episode steps: 23, steps per second: 90, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.080 [-0.734, 1.174], loss: 5.558626, mean_squared_error: 191.334244, mean_q: 18.292276\n",
      "413 {'episode_reward': 21.0, 'nb_episode_steps': 21, 'nb_steps': 4934}\n",
      "  4934/100000: episode: 414, duration: 0.227s, episode steps: 21, steps per second: 93, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.076 [-0.583, 1.198], loss: 4.538826, mean_squared_error: 191.331680, mean_q: 18.247305\n",
      "414 {'episode_reward': 31.0, 'nb_episode_steps': 31, 'nb_steps': 4965}\n",
      "  4965/100000: episode: 415, duration: 0.337s, episode steps: 31, steps per second: 92, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.099 [-0.376, 0.763], loss: 4.879410, mean_squared_error: 192.494858, mean_q: 18.367737\n",
      "415 {'episode_reward': 25.0, 'nb_episode_steps': 25, 'nb_steps': 4990}\n",
      "  4990/100000: episode: 416, duration: 0.275s, episode steps: 25, steps per second: 91, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.059 [-0.403, 1.022], loss: 2.657839, mean_squared_error: 192.429764, mean_q: 18.463459\n",
      "416 {'episode_reward': 18.0, 'nb_episode_steps': 18, 'nb_steps': 5008}\n",
      "  5008/100000: episode: 417, duration: 0.194s, episode steps: 18, steps per second: 93, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.110 [-0.374, 1.178], loss: 3.286507, mean_squared_error: 207.377335, mean_q: 19.303823\n",
      "417 {'episode_reward': 21.0, 'nb_episode_steps': 21, 'nb_steps': 5029}\n",
      "  5029/100000: episode: 418, duration: 0.223s, episode steps: 21, steps per second: 94, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.119 [-0.383, 0.990], loss: 3.441933, mean_squared_error: 210.027481, mean_q: 19.263424\n",
      "418 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 5051}\n",
      "  5051/100000: episode: 419, duration: 0.246s, episode steps: 22, steps per second: 90, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.416, 0.955], loss: 4.474834, mean_squared_error: 219.160828, mean_q: 19.679493\n",
      "419 {'episode_reward': 21.0, 'nb_episode_steps': 21, 'nb_steps': 5072}\n",
      "  5072/100000: episode: 420, duration: 0.237s, episode steps: 21, steps per second: 89, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.129 [-0.344, 0.808], loss: 2.995165, mean_squared_error: 216.526779, mean_q: 19.548513\n",
      "420 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 5088}\n",
      "  5088/100000: episode: 421, duration: 0.184s, episode steps: 16, steps per second: 87, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.127 [-0.383, 1.154], loss: 6.143413, mean_squared_error: 233.278809, mean_q: 20.258892\n",
      "421 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 5110}\n",
      "  5110/100000: episode: 422, duration: 0.240s, episode steps: 22, steps per second: 92, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.594, 1.012], loss: 6.244911, mean_squared_error: 236.541550, mean_q: 20.534981\n",
      "422 {'episode_reward': 25.0, 'nb_episode_steps': 25, 'nb_steps': 5135}\n",
      "  5135/100000: episode: 423, duration: 0.292s, episode steps: 25, steps per second: 86, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.055 [-0.604, 0.943], loss: 3.757657, mean_squared_error: 234.313248, mean_q: 20.562027\n",
      "423 {'episode_reward': 16.0, 'nb_episode_steps': 16, 'nb_steps': 5151}\n",
      "  5151/100000: episode: 424, duration: 0.176s, episode steps: 16, steps per second: 91, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.131 [-0.370, 0.838], loss: 6.058222, mean_squared_error: 250.950348, mean_q: 21.285051\n",
      "424 {'episode_reward': 28.0, 'nb_episode_steps': 28, 'nb_steps': 5179}\n",
      "  5179/100000: episode: 425, duration: 0.312s, episode steps: 28, steps per second: 90, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.059 [-0.555, 0.839], loss: 4.308561, mean_squared_error: 246.566422, mean_q: 20.997690\n",
      "425 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 5199}\n",
      "  5199/100000: episode: 426, duration: 0.223s, episode steps: 20, steps per second: 90, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.371, 0.848], loss: 6.022690, mean_squared_error: 255.691406, mean_q: 21.431179\n",
      "426 {'episode_reward': 19.0, 'nb_episode_steps': 19, 'nb_steps': 5218}\n",
      "  5218/100000: episode: 427, duration: 0.228s, episode steps: 19, steps per second: 83, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.094 [-0.621, 1.118], loss: 7.316453, mean_squared_error: 259.073120, mean_q: 21.551641\n",
      "427 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 5238}\n",
      "  5238/100000: episode: 428, duration: 0.211s, episode steps: 20, steps per second: 95, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.121 [-0.393, 0.748], loss: 7.200027, mean_squared_error: 241.967804, mean_q: 20.670307\n",
      "428 {'episode_reward': 31.0, 'nb_episode_steps': 31, 'nb_steps': 5269}\n",
      "  5269/100000: episode: 429, duration: 0.332s, episode steps: 31, steps per second: 93, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.062 [-0.417, 0.812], loss: 4.130565, mean_squared_error: 234.730865, mean_q: 20.493557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 {'episode_reward': 21.0, 'nb_episode_steps': 21, 'nb_steps': 5290}\n",
      "  5290/100000: episode: 430, duration: 0.231s, episode steps: 21, steps per second: 91, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.067 [-0.584, 0.899], loss: 7.132582, mean_squared_error: 238.096405, mean_q: 20.453192\n",
      "430 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 5312}\n",
      "  5312/100000: episode: 431, duration: 0.244s, episode steps: 22, steps per second: 90, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.401, 0.878], loss: 3.283436, mean_squared_error: 226.257874, mean_q: 20.110256\n",
      "431 {'episode_reward': 29.0, 'nb_episode_steps': 29, 'nb_steps': 5341}\n",
      "  5341/100000: episode: 432, duration: 0.315s, episode steps: 29, steps per second: 92, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.044 [-0.441, 0.764], loss: 7.820325, mean_squared_error: 233.707306, mean_q: 20.193937\n",
      "432 {'episode_reward': 25.0, 'nb_episode_steps': 25, 'nb_steps': 5366}\n",
      "  5366/100000: episode: 433, duration: 0.274s, episode steps: 25, steps per second: 91, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: 0.051 [-0.432, 0.899], loss: 4.224433, mean_squared_error: 230.182343, mean_q: 20.346903\n",
      "433 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 5388}\n",
      "  5388/100000: episode: 434, duration: 0.253s, episode steps: 22, steps per second: 87, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.081 [-0.411, 0.872], loss: 6.124038, mean_squared_error: 252.709320, mean_q: 21.404631\n",
      "434 {'episode_reward': 39.0, 'nb_episode_steps': 39, 'nb_steps': 5427}\n",
      "  5427/100000: episode: 435, duration: 0.433s, episode steps: 39, steps per second: 90, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.057 [-0.361, 0.702], loss: 4.102916, mean_squared_error: 255.898315, mean_q: 21.624197\n",
      "435 {'episode_reward': 22.0, 'nb_episode_steps': 22, 'nb_steps': 5449}\n",
      "  5449/100000: episode: 436, duration: 0.245s, episode steps: 22, steps per second: 90, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.430, 0.786], loss: 6.583310, mean_squared_error: 272.749451, mean_q: 22.292692\n",
      "436 {'episode_reward': 29.0, 'nb_episode_steps': 29, 'nb_steps': 5478}\n",
      "  5478/100000: episode: 437, duration: 0.335s, episode steps: 29, steps per second: 86, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.050 [-0.390, 0.897], loss: 8.335279, mean_squared_error: 272.355652, mean_q: 22.258299\n",
      "437 {'episode_reward': 25.0, 'nb_episode_steps': 25, 'nb_steps': 5503}\n",
      "  5503/100000: episode: 438, duration: 0.286s, episode steps: 25, steps per second: 87, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.078 [-0.442, 0.769], loss: 4.946493, mean_squared_error: 267.769897, mean_q: 22.179182\n",
      "438 {'episode_reward': 32.0, 'nb_episode_steps': 32, 'nb_steps': 5535}\n",
      "  5535/100000: episode: 439, duration: 0.348s, episode steps: 32, steps per second: 92, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.069 [-0.366, 0.972], loss: 5.679659, mean_squared_error: 266.360443, mean_q: 21.987955\n",
      "439 {'episode_reward': 24.0, 'nb_episode_steps': 24, 'nb_steps': 5559}\n",
      "  5559/100000: episode: 440, duration: 0.258s, episode steps: 24, steps per second: 93, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.086 [-0.353, 0.782], loss: 4.885485, mean_squared_error: 266.546844, mean_q: 22.007063\n",
      "440 {'episode_reward': 46.0, 'nb_episode_steps': 46, 'nb_steps': 5605}\n",
      "  5605/100000: episode: 441, duration: 0.506s, episode steps: 46, steps per second: 91, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.088 [-0.246, 0.721], loss: 5.403693, mean_squared_error: 256.905884, mean_q: 21.422583\n",
      "441 {'episode_reward': 31.0, 'nb_episode_steps': 31, 'nb_steps': 5636}\n",
      "  5636/100000: episode: 442, duration: 0.352s, episode steps: 31, steps per second: 88, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.049 [-0.376, 0.861], loss: 7.567350, mean_squared_error: 256.775909, mean_q: 21.322031\n",
      "442 {'episode_reward': 25.0, 'nb_episode_steps': 25, 'nb_steps': 5661}\n",
      "  5661/100000: episode: 443, duration: 0.287s, episode steps: 25, steps per second: 87, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.089 [-0.433, 0.732], loss: 4.872984, mean_squared_error: 254.988953, mean_q: 21.529114\n",
      "443 {'episode_reward': 20.0, 'nb_episode_steps': 20, 'nb_steps': 5681}\n",
      "  5681/100000: episode: 444, duration: 0.218s, episode steps: 20, steps per second: 92, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.121 [-0.373, 0.786], loss: 6.366617, mean_squared_error: 258.847229, mean_q: 21.565395\n",
      "444 {'episode_reward': 33.0, 'nb_episode_steps': 33, 'nb_steps': 5714}\n",
      "  5714/100000: episode: 445, duration: 0.362s, episode steps: 33, steps per second: 91, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.071 [-0.395, 0.751], loss: 4.932207, mean_squared_error: 249.645599, mean_q: 21.148767\n",
      "445 {'episode_reward': 44.0, 'nb_episode_steps': 44, 'nb_steps': 5758}\n",
      "  5758/100000: episode: 446, duration: 0.466s, episode steps: 44, steps per second: 94, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.032 [-0.804, 1.062], loss: 7.868262, mean_squared_error: 252.684708, mean_q: 21.238403\n",
      "446 {'episode_reward': 37.0, 'nb_episode_steps': 37, 'nb_steps': 5795}\n",
      "  5795/100000: episode: 447, duration: 0.394s, episode steps: 37, steps per second: 94, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.127 [-0.281, 0.635], loss: 4.488125, mean_squared_error: 233.590393, mean_q: 20.529110\n",
      "447 {'episode_reward': 38.0, 'nb_episode_steps': 38, 'nb_steps': 5833}\n",
      "  5833/100000: episode: 448, duration: 0.408s, episode steps: 38, steps per second: 93, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.090 [-0.565, 0.908], loss: 5.487780, mean_squared_error: 242.723038, mean_q: 20.832378\n",
      "448 {'episode_reward': 52.0, 'nb_episode_steps': 52, 'nb_steps': 5885}\n",
      "  5885/100000: episode: 449, duration: 0.555s, episode steps: 52, steps per second: 94, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.078 [-0.248, 0.667], loss: 5.377116, mean_squared_error: 253.318008, mean_q: 21.479424\n",
      "449 {'episode_reward': 53.0, 'nb_episode_steps': 53, 'nb_steps': 5938}\n",
      "  5938/100000: episode: 450, duration: 0.557s, episode steps: 53, steps per second: 95, episode reward: 53.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.528 [0.000, 1.000], mean observation: 0.097 [-0.170, 0.744], loss: 7.624396, mean_squared_error: 263.133850, mean_q: 21.824703\n",
      "450 {'episode_reward': 51.0, 'nb_episode_steps': 51, 'nb_steps': 5989}\n",
      "  5989/100000: episode: 451, duration: 0.550s, episode steps: 51, steps per second: 93, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.070 [-0.268, 0.808], loss: 6.233578, mean_squared_error: 250.645447, mean_q: 21.339855\n",
      "451 {'episode_reward': 94.0, 'nb_episode_steps': 94, 'nb_steps': 6083}\n",
      "  6083/100000: episode: 452, duration: 0.976s, episode steps: 94, steps per second: 96, episode reward: 94.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.072 [-0.399, 1.033], loss: 5.832728, mean_squared_error: 268.571350, mean_q: 22.158230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452 {'episode_reward': 44.0, 'nb_episode_steps': 44, 'nb_steps': 6127}\n",
      "  6127/100000: episode: 453, duration: 0.457s, episode steps: 44, steps per second: 96, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.103 [-0.427, 0.694], loss: 4.954988, mean_squared_error: 284.294312, mean_q: 22.844221\n",
      "453 {'episode_reward': 30.0, 'nb_episode_steps': 30, 'nb_steps': 6157}\n",
      "  6157/100000: episode: 454, duration: 0.329s, episode steps: 30, steps per second: 91, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: 0.117 [-0.225, 0.699], loss: 9.854832, mean_squared_error: 305.385529, mean_q: 23.439140\n",
      "454 {'episode_reward': 154.0, 'nb_episode_steps': 154, 'nb_steps': 6311}\n",
      "  6311/100000: episode: 455, duration: 1.589s, episode steps: 154, steps per second: 97, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: -0.051 [-0.509, 0.915], loss: 6.207004, mean_squared_error: 295.269043, mean_q: 23.169868\n",
      "455 {'episode_reward': 68.0, 'nb_episode_steps': 68, 'nb_steps': 6379}\n",
      "  6379/100000: episode: 456, duration: 0.724s, episode steps: 68, steps per second: 94, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.090 [-0.324, 0.747], loss: 6.438754, mean_squared_error: 319.766113, mean_q: 24.260443\n",
      "456 {'episode_reward': 51.0, 'nb_episode_steps': 51, 'nb_steps': 6430}\n",
      "  6430/100000: episode: 457, duration: 0.555s, episode steps: 51, steps per second: 92, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.124 [-0.351, 0.829], loss: 6.836557, mean_squared_error: 322.329437, mean_q: 24.415726\n",
      "457 {'episode_reward': 48.0, 'nb_episode_steps': 48, 'nb_steps': 6478}\n",
      "  6478/100000: episode: 458, duration: 0.512s, episode steps: 48, steps per second: 94, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.147 [-0.245, 0.753], loss: 5.841599, mean_squared_error: 330.332672, mean_q: 24.522409\n",
      "458 {'episode_reward': 64.0, 'nb_episode_steps': 64, 'nb_steps': 6542}\n",
      "  6542/100000: episode: 459, duration: 0.687s, episode steps: 64, steps per second: 93, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.086 [-0.375, 0.749], loss: 7.851029, mean_squared_error: 344.782166, mean_q: 25.021814\n",
      "459 {'episode_reward': 56.0, 'nb_episode_steps': 56, 'nb_steps': 6598}\n",
      "  6598/100000: episode: 460, duration: 0.609s, episode steps: 56, steps per second: 92, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.129 [-0.391, 0.728], loss: 8.215281, mean_squared_error: 372.023499, mean_q: 26.235308\n",
      "460 {'episode_reward': 52.0, 'nb_episode_steps': 52, 'nb_steps': 6650}\n",
      "  6650/100000: episode: 461, duration: 0.564s, episode steps: 52, steps per second: 92, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.146 [-0.368, 0.749], loss: 7.458125, mean_squared_error: 388.703278, mean_q: 26.955654\n",
      "461 {'episode_reward': 55.0, 'nb_episode_steps': 55, 'nb_steps': 6705}\n",
      "  6705/100000: episode: 462, duration: 0.572s, episode steps: 55, steps per second: 96, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.122 [-0.366, 0.798], loss: 6.480322, mean_squared_error: 439.313660, mean_q: 28.628963\n",
      "462 {'episode_reward': 59.0, 'nb_episode_steps': 59, 'nb_steps': 6764}\n",
      "  6764/100000: episode: 463, duration: 0.613s, episode steps: 59, steps per second: 96, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.135 [-0.391, 0.905], loss: 10.584818, mean_squared_error: 459.715759, mean_q: 29.114037\n",
      "463 {'episode_reward': 51.0, 'nb_episode_steps': 51, 'nb_steps': 6815}\n",
      "  6815/100000: episode: 464, duration: 0.545s, episode steps: 51, steps per second: 94, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.152 [-0.942, 0.184], loss: 11.701129, mean_squared_error: 467.518585, mean_q: 29.480412\n",
      "464 {'episode_reward': 95.0, 'nb_episode_steps': 95, 'nb_steps': 6910}\n",
      "  6910/100000: episode: 465, duration: 1.016s, episode steps: 95, steps per second: 94, episode reward: 95.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.144 [-0.388, 0.914], loss: 8.665974, mean_squared_error: 494.410370, mean_q: 30.256516\n",
      "465 {'episode_reward': 85.0, 'nb_episode_steps': 85, 'nb_steps': 6995}\n",
      "  6995/100000: episode: 466, duration: 0.922s, episode steps: 85, steps per second: 92, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.112 [-0.395, 0.869], loss: 9.944516, mean_squared_error: 566.191345, mean_q: 32.560215\n",
      "466 {'episode_reward': 97.0, 'nb_episode_steps': 97, 'nb_steps': 7092}\n",
      "  7092/100000: episode: 467, duration: 1.040s, episode steps: 97, steps per second: 93, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.069 [-0.412, 0.881], loss: 12.631371, mean_squared_error: 568.051453, mean_q: 32.302040\n",
      "467 {'episode_reward': 60.0, 'nb_episode_steps': 60, 'nb_steps': 7152}\n",
      "  7152/100000: episode: 468, duration: 0.649s, episode steps: 60, steps per second: 92, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.096 [-0.751, 0.280], loss: 10.765306, mean_squared_error: 585.952393, mean_q: 33.350758\n",
      "468 {'episode_reward': 48.0, 'nb_episode_steps': 48, 'nb_steps': 7200}\n",
      "  7200/100000: episode: 469, duration: 0.518s, episode steps: 48, steps per second: 93, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.151 [-0.771, 0.347], loss: 12.422732, mean_squared_error: 639.015137, mean_q: 34.586353\n",
      "469 {'episode_reward': 81.0, 'nb_episode_steps': 81, 'nb_steps': 7281}\n",
      "  7281/100000: episode: 470, duration: 0.853s, episode steps: 81, steps per second: 95, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.202 [-0.628, 1.343], loss: 11.122306, mean_squared_error: 631.496704, mean_q: 34.186123\n",
      "470 {'episode_reward': 88.0, 'nb_episode_steps': 88, 'nb_steps': 7369}\n",
      "  7369/100000: episode: 471, duration: 0.927s, episode steps: 88, steps per second: 95, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.080 [-1.081, 0.522], loss: 12.249930, mean_squared_error: 707.040405, mean_q: 36.460995\n",
      "471 {'episode_reward': 88.0, 'nb_episode_steps': 88, 'nb_steps': 7457}\n",
      "  7457/100000: episode: 472, duration: 0.930s, episode steps: 88, steps per second: 95, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.144 [-0.498, 0.893], loss: 10.785567, mean_squared_error: 758.084595, mean_q: 37.829704\n",
      "472 {'episode_reward': 69.0, 'nb_episode_steps': 69, 'nb_steps': 7526}\n",
      "  7526/100000: episode: 473, duration: 0.724s, episode steps: 69, steps per second: 95, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.128 [-0.929, 0.527], loss: 14.194626, mean_squared_error: 809.498169, mean_q: 39.019566\n",
      "473 {'episode_reward': 117.0, 'nb_episode_steps': 117, 'nb_steps': 7643}\n",
      "  7643/100000: episode: 474, duration: 1.214s, episode steps: 117, steps per second: 96, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.496 [0.000, 1.000], mean observation: -0.064 [-1.031, 0.428], loss: 16.737667, mean_squared_error: 819.199768, mean_q: 39.398243\n",
      "474 {'episode_reward': 49.0, 'nb_episode_steps': 49, 'nb_steps': 7692}\n",
      "  7692/100000: episode: 475, duration: 0.528s, episode steps: 49, steps per second: 93, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: -0.114 [-0.638, 0.383], loss: 14.780781, mean_squared_error: 815.659119, mean_q: 39.499695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475 {'episode_reward': 80.0, 'nb_episode_steps': 80, 'nb_steps': 7772}\n",
      "  7772/100000: episode: 476, duration: 0.836s, episode steps: 80, steps per second: 96, episode reward: 80.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.078 [-1.012, 0.404], loss: 16.862741, mean_squared_error: 826.616821, mean_q: 39.379585\n",
      "476 {'episode_reward': 84.0, 'nb_episode_steps': 84, 'nb_steps': 7856}\n",
      "  7856/100000: episode: 477, duration: 0.874s, episode steps: 84, steps per second: 96, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: -0.044 [-0.828, 0.426], loss: 15.105824, mean_squared_error: 850.647034, mean_q: 40.144730\n",
      "477 {'episode_reward': 42.0, 'nb_episode_steps': 42, 'nb_steps': 7898}\n",
      "  7898/100000: episode: 478, duration: 0.450s, episode steps: 42, steps per second: 93, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.138 [-0.763, 0.355], loss: 11.921638, mean_squared_error: 867.492065, mean_q: 40.426476\n",
      "478 {'episode_reward': 39.0, 'nb_episode_steps': 39, 'nb_steps': 7937}\n",
      "  7937/100000: episode: 479, duration: 0.416s, episode steps: 39, steps per second: 94, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.160 [-0.765, 0.161], loss: 8.286955, mean_squared_error: 957.309387, mean_q: 42.647717\n",
      "479 {'episode_reward': 61.0, 'nb_episode_steps': 61, 'nb_steps': 7998}\n",
      "  7998/100000: episode: 480, duration: 0.668s, episode steps: 61, steps per second: 91, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.072 [-0.725, 0.412], loss: 15.352947, mean_squared_error: 1058.199707, mean_q: 44.986748\n",
      "480 {'episode_reward': 50.0, 'nb_episode_steps': 50, 'nb_steps': 8048}\n",
      "  8048/100000: episode: 481, duration: 0.543s, episode steps: 50, steps per second: 92, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: -0.172 [-1.082, 0.211], loss: 17.021988, mean_squared_error: 1111.969727, mean_q: 45.897758\n",
      "481 {'episode_reward': 98.0, 'nb_episode_steps': 98, 'nb_steps': 8146}\n",
      "  8146/100000: episode: 482, duration: 1.030s, episode steps: 98, steps per second: 95, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.239 [-0.378, 1.279], loss: 15.833438, mean_squared_error: 1147.992065, mean_q: 46.811512\n",
      "482 {'episode_reward': 86.0, 'nb_episode_steps': 86, 'nb_steps': 8232}\n",
      "  8232/100000: episode: 483, duration: 0.903s, episode steps: 86, steps per second: 95, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.089 [-1.069, 0.417], loss: 18.925646, mean_squared_error: 1233.309692, mean_q: 48.625732\n",
      "483 {'episode_reward': 59.0, 'nb_episode_steps': 59, 'nb_steps': 8291}\n",
      "  8291/100000: episode: 484, duration: 0.621s, episode steps: 59, steps per second: 95, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.212 [-1.271, 0.402], loss: 15.155602, mean_squared_error: 1318.682495, mean_q: 50.097965\n",
      "484 {'episode_reward': 61.0, 'nb_episode_steps': 61, 'nb_steps': 8352}\n",
      "  8352/100000: episode: 485, duration: 0.651s, episode steps: 61, steps per second: 94, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.179 [-1.121, 0.324], loss: 21.769258, mean_squared_error: 1388.296265, mean_q: 51.716915\n",
      "485 {'episode_reward': 54.0, 'nb_episode_steps': 54, 'nb_steps': 8406}\n",
      "  8406/100000: episode: 486, duration: 0.583s, episode steps: 54, steps per second: 93, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.178 [-1.130, 0.373], loss: 19.419100, mean_squared_error: 1363.520142, mean_q: 51.234379\n",
      "486 {'episode_reward': 58.0, 'nb_episode_steps': 58, 'nb_steps': 8464}\n",
      "  8464/100000: episode: 487, duration: 0.607s, episode steps: 58, steps per second: 95, episode reward: 58.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.149 [-1.103, 0.402], loss: 25.002764, mean_squared_error: 1400.367188, mean_q: 51.834457\n",
      "487 {'episode_reward': 62.0, 'nb_episode_steps': 62, 'nb_steps': 8526}\n",
      "  8526/100000: episode: 488, duration: 0.652s, episode steps: 62, steps per second: 95, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: -0.237 [-1.476, 0.353], loss: 12.326078, mean_squared_error: 1418.782715, mean_q: 52.194321\n",
      "488 {'episode_reward': 98.0, 'nb_episode_steps': 98, 'nb_steps': 8624}\n",
      "  8624/100000: episode: 489, duration: 1.015s, episode steps: 98, steps per second: 97, episode reward: 98.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.139 [-1.486, 0.328], loss: 17.313185, mean_squared_error: 1511.569946, mean_q: 53.608013\n",
      "489 {'episode_reward': 70.0, 'nb_episode_steps': 70, 'nb_steps': 8694}\n",
      "  8694/100000: episode: 490, duration: 0.739s, episode steps: 70, steps per second: 95, episode reward: 70.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.443 [0.000, 1.000], mean observation: -0.205 [-1.448, 0.405], loss: 20.428310, mean_squared_error: 1592.231323, mean_q: 55.588955\n",
      "490 {'episode_reward': 101.0, 'nb_episode_steps': 101, 'nb_steps': 8795}\n",
      "  8795/100000: episode: 491, duration: 1.037s, episode steps: 101, steps per second: 97, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.297 [-1.950, 0.501], loss: 21.735868, mean_squared_error: 1612.349365, mean_q: 55.524704\n",
      "491 {'episode_reward': 112.0, 'nb_episode_steps': 112, 'nb_steps': 8907}\n",
      "  8907/100000: episode: 492, duration: 1.176s, episode steps: 112, steps per second: 95, episode reward: 112.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.446 [0.000, 1.000], mean observation: -0.393 [-2.159, 0.507], loss: 17.681749, mean_squared_error: 1713.543213, mean_q: 57.355244\n",
      "492 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 9107}\n",
      "  9107/100000: episode: 493, duration: 2.085s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.158 [-1.688, 0.426], loss: 20.292185, mean_squared_error: 1993.887817, mean_q: 62.120556\n",
      "493 {'episode_reward': 144.0, 'nb_episode_steps': 144, 'nb_steps': 9251}\n",
      "  9251/100000: episode: 494, duration: 1.492s, episode steps: 144, steps per second: 97, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.422 [-2.524, 0.442], loss: 22.107002, mean_squared_error: 2130.236328, mean_q: 64.123344\n",
      "494 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 9451}\n",
      "  9451/100000: episode: 495, duration: 2.096s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.097 [-1.109, 0.517], loss: 23.167086, mean_squared_error: 2203.801025, mean_q: 65.124023\n",
      "495 {'episode_reward': 156.0, 'nb_episode_steps': 156, 'nb_steps': 9607}\n",
      "  9607/100000: episode: 496, duration: 1.615s, episode steps: 156, steps per second: 97, episode reward: 156.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.422 [-2.430, 0.497], loss: 20.628145, mean_squared_error: 2379.402588, mean_q: 67.700676\n",
      "496 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 9807}\n",
      "  9807/100000: episode: 497, duration: 2.069s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.113 [-0.794, 0.688], loss: 16.861435, mean_squared_error: 2569.374512, mean_q: 70.365593\n",
      "497 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 10007}\n",
      " 10007/100000: episode: 498, duration: 2.087s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.018 [-0.583, 0.375], loss: 20.418434, mean_squared_error: 2981.770508, mean_q: 75.950729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 10207}\n",
      " 10207/100000: episode: 499, duration: 2.096s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.111 [-0.795, 0.493], loss: 19.026987, mean_squared_error: 3586.834473, mean_q: 83.473900\n",
      "499 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 10407}\n",
      " 10407/100000: episode: 500, duration: 2.093s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.164 [-0.949, 0.796], loss: 26.504499, mean_squared_error: 3908.051270, mean_q: 86.898308\n",
      "500 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 10607}\n",
      " 10607/100000: episode: 501, duration: 2.069s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.163 [-1.113, 0.407], loss: 23.024429, mean_squared_error: 4331.107910, mean_q: 91.408310\n",
      "501 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 10807}\n",
      " 10807/100000: episode: 502, duration: 2.086s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.045 [-0.668, 0.572], loss: 32.594402, mean_squared_error: 4748.111328, mean_q: 96.109299\n",
      "502 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 11007}\n",
      " 11007/100000: episode: 503, duration: 2.059s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.265 [-1.750, 0.585], loss: 28.259354, mean_squared_error: 5116.644043, mean_q: 99.400505\n",
      "503 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 11207}\n",
      " 11207/100000: episode: 504, duration: 2.085s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.072 [-0.811, 0.766], loss: 34.953453, mean_squared_error: 5373.980469, mean_q: 102.505417\n",
      "504 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 11407}\n",
      " 11407/100000: episode: 505, duration: 2.077s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.057 [-0.509, 0.446], loss: 33.529663, mean_squared_error: 5577.053711, mean_q: 104.251350\n",
      "505 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 11607}\n",
      " 11607/100000: episode: 506, duration: 2.068s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.231 [-1.518, 0.557], loss: 34.407459, mean_squared_error: 5678.504395, mean_q: 105.118721\n",
      "506 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 11807}\n",
      " 11807/100000: episode: 507, duration: 2.066s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.155 [-0.998, 0.542], loss: 31.832993, mean_squared_error: 6038.472656, mean_q: 108.764023\n",
      "507 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 12007}\n",
      " 12007/100000: episode: 508, duration: 2.091s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.119 [-0.780, 0.504], loss: 29.109722, mean_squared_error: 6293.246094, mean_q: 110.819191\n",
      "508 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 12207}\n",
      " 12207/100000: episode: 509, duration: 2.086s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.134 [-0.973, 0.851], loss: 34.983173, mean_squared_error: 6527.585449, mean_q: 113.090675\n",
      "509 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 12407}\n",
      " 12407/100000: episode: 510, duration: 2.065s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.630, 0.524], loss: 24.260735, mean_squared_error: 6976.457031, mean_q: 116.867737\n",
      "510 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 12607}\n",
      " 12607/100000: episode: 511, duration: 2.066s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.130 [-0.943, 0.448], loss: 39.139446, mean_squared_error: 7238.542969, mean_q: 118.863594\n",
      "511 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 12807}\n",
      " 12807/100000: episode: 512, duration: 2.073s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.194 [-1.286, 0.635], loss: 33.257114, mean_squared_error: 7363.701172, mean_q: 120.176544\n",
      "512 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 13007}\n",
      " 13007/100000: episode: 513, duration: 2.115s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.227 [-1.527, 0.573], loss: 41.213455, mean_squared_error: 7604.303223, mean_q: 122.114113\n",
      "513 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 13207}\n",
      " 13207/100000: episode: 514, duration: 2.076s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.182 [-1.213, 0.443], loss: 34.609009, mean_squared_error: 7857.488770, mean_q: 124.373688\n",
      "514 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 13407}\n",
      " 13407/100000: episode: 515, duration: 2.056s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.284 [-1.903, 0.524], loss: 36.050514, mean_squared_error: 8038.267578, mean_q: 125.508240\n",
      "515 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 13607}\n",
      " 13607/100000: episode: 516, duration: 2.066s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.244 [-1.669, 0.946], loss: 32.917572, mean_squared_error: 8016.397461, mean_q: 125.286980\n",
      "516 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 13807}\n",
      " 13807/100000: episode: 517, duration: 2.069s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.008 [-0.697, 0.574], loss: 40.751968, mean_squared_error: 8163.688965, mean_q: 126.465057\n",
      "517 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 14007}\n",
      " 14007/100000: episode: 518, duration: 2.061s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.126 [-0.909, 0.532], loss: 38.386295, mean_squared_error: 8325.277344, mean_q: 128.004990\n",
      "518 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 14207}\n",
      " 14207/100000: episode: 519, duration: 2.074s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.201 [-1.443, 0.604], loss: 35.502323, mean_squared_error: 8614.818359, mean_q: 130.055542\n",
      "519 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 14407}\n",
      " 14407/100000: episode: 520, duration: 2.098s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.269 [-1.825, 0.532], loss: 41.316883, mean_squared_error: 8460.407227, mean_q: 128.714432\n",
      "520 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 14607}\n",
      " 14607/100000: episode: 521, duration: 2.062s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.722, 0.587], loss: 34.042240, mean_squared_error: 8581.960938, mean_q: 129.865585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "521 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 14807}\n",
      " 14807/100000: episode: 522, duration: 2.071s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.046 [-0.636, 0.596], loss: 34.851894, mean_squared_error: 8515.289062, mean_q: 129.322922\n",
      "522 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 15007}\n",
      " 15007/100000: episode: 523, duration: 2.084s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.564, 0.663], loss: 36.577530, mean_squared_error: 8760.042969, mean_q: 131.000381\n",
      "523 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 15207}\n",
      " 15207/100000: episode: 524, duration: 2.078s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.256 [-1.801, 0.603], loss: 35.099403, mean_squared_error: 9024.233398, mean_q: 132.823257\n",
      "524 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 15407}\n",
      " 15407/100000: episode: 525, duration: 2.102s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.275 [-1.938, 0.530], loss: 37.154087, mean_squared_error: 9041.738281, mean_q: 132.736420\n",
      "525 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 15607}\n",
      " 15607/100000: episode: 526, duration: 2.104s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.215 [-1.509, 0.825], loss: 40.664989, mean_squared_error: 8688.289062, mean_q: 130.145782\n",
      "526 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 15807}\n",
      " 15807/100000: episode: 527, duration: 2.101s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.245 [-1.748, 0.507], loss: 34.644226, mean_squared_error: 9232.174805, mean_q: 134.533722\n",
      "527 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 16007}\n",
      " 16007/100000: episode: 528, duration: 2.067s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.290 [-2.043, 0.639], loss: 39.012711, mean_squared_error: 9266.610352, mean_q: 134.940063\n",
      "528 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 16207}\n",
      " 16207/100000: episode: 529, duration: 2.075s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.229 [-1.666, 0.630], loss: 40.701561, mean_squared_error: 9226.277344, mean_q: 134.248550\n",
      "529 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 16407}\n",
      " 16407/100000: episode: 530, duration: 2.102s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.335 [-2.336, 0.485], loss: 27.967501, mean_squared_error: 9576.668945, mean_q: 137.001068\n",
      "530 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 16607}\n",
      " 16607/100000: episode: 531, duration: 2.052s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.256 [-1.849, 0.550], loss: 45.274673, mean_squared_error: 9710.701172, mean_q: 137.981216\n",
      "531 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 16807}\n",
      " 16807/100000: episode: 532, duration: 2.068s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.269 [-1.938, 0.522], loss: 31.735735, mean_squared_error: 9420.722656, mean_q: 135.722046\n",
      "532 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 17007}\n",
      " 17007/100000: episode: 533, duration: 2.208s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.097 [-0.919, 0.547], loss: 42.804882, mean_squared_error: 9814.352539, mean_q: 138.968796\n",
      "533 {'episode_reward': 193.0, 'nb_episode_steps': 193, 'nb_steps': 17200}\n",
      " 17200/100000: episode: 534, duration: 2.057s, episode steps: 193, steps per second: 94, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: -0.351 [-2.401, 0.570], loss: 39.033516, mean_squared_error: 10175.184570, mean_q: 141.357620\n",
      "534 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 17400}\n",
      " 17400/100000: episode: 535, duration: 2.082s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.302 [-2.188, 0.451], loss: 33.023392, mean_squared_error: 10253.712891, mean_q: 142.077438\n",
      "535 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 17600}\n",
      " 17600/100000: episode: 536, duration: 2.061s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.095 [-0.731, 0.489], loss: 38.241886, mean_squared_error: 10819.706055, mean_q: 146.229889\n",
      "536 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 17800}\n",
      " 17800/100000: episode: 537, duration: 2.053s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: -0.276 [-2.000, 0.616], loss: 41.704567, mean_squared_error: 10857.847656, mean_q: 146.370468\n",
      "537 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 18000}\n",
      " 18000/100000: episode: 538, duration: 2.078s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.088 [-0.743, 0.655], loss: 42.948624, mean_squared_error: 10101.817383, mean_q: 140.629608\n",
      "538 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 18200}\n",
      " 18200/100000: episode: 539, duration: 2.095s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.069 [-0.635, 0.817], loss: 39.927547, mean_squared_error: 10209.915039, mean_q: 141.453888\n",
      "539 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 18400}\n",
      " 18400/100000: episode: 540, duration: 2.074s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.068 [-0.737, 0.552], loss: 45.669235, mean_squared_error: 10333.093750, mean_q: 142.486755\n",
      "540 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 18600}\n",
      " 18600/100000: episode: 541, duration: 2.096s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.123 [-0.907, 0.816], loss: 31.522514, mean_squared_error: 10603.482422, mean_q: 144.465668\n",
      "541 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 18800}\n",
      " 18800/100000: episode: 542, duration: 2.066s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.049 [-0.571, 0.446], loss: 31.682680, mean_squared_error: 11018.967773, mean_q: 147.339996\n",
      "542 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 19000}\n",
      " 19000/100000: episode: 543, duration: 2.046s, episode steps: 200, steps per second: 98, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.053 [-0.725, 0.918], loss: 40.142815, mean_squared_error: 10760.486328, mean_q: 145.519424\n",
      "543 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 19200}\n",
      " 19200/100000: episode: 544, duration: 2.071s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.081 [-0.727, 0.502], loss: 42.714203, mean_squared_error: 10392.428711, mean_q: 142.287125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 19400}\n",
      " 19400/100000: episode: 545, duration: 2.040s, episode steps: 200, steps per second: 98, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.195 [-1.461, 0.476], loss: 34.366581, mean_squared_error: 10005.093750, mean_q: 139.906296\n",
      "545 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 19600}\n",
      " 19600/100000: episode: 546, duration: 2.097s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.172 [-1.231, 0.536], loss: 28.005215, mean_squared_error: 10398.732422, mean_q: 142.809937\n",
      "546 {'episode_reward': 180.0, 'nb_episode_steps': 180, 'nb_steps': 19780}\n",
      " 19780/100000: episode: 547, duration: 1.887s, episode steps: 180, steps per second: 95, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.370 [-2.414, 0.453], loss: 29.151449, mean_squared_error: 10811.241211, mean_q: 145.658112\n",
      "547 {'episode_reward': 191.0, 'nb_episode_steps': 191, 'nb_steps': 19971}\n",
      " 19971/100000: episode: 548, duration: 2.004s, episode steps: 191, steps per second: 95, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.351 [-2.725, 0.834], loss: 33.090359, mean_squared_error: 11051.196289, mean_q: 147.412613\n",
      "548 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 20171}\n",
      " 20171/100000: episode: 549, duration: 2.094s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.307 [-2.184, 0.536], loss: 27.278811, mean_squared_error: 10940.974609, mean_q: 146.219467\n",
      "549 {'episode_reward': 199.0, 'nb_episode_steps': 199, 'nb_steps': 20370}\n",
      " 20370/100000: episode: 550, duration: 2.095s, episode steps: 199, steps per second: 95, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.331 [-2.829, 1.200], loss: 31.168064, mean_squared_error: 10790.434570, mean_q: 145.079422\n",
      "550 {'episode_reward': 166.0, 'nb_episode_steps': 166, 'nb_steps': 20536}\n",
      " 20536/100000: episode: 551, duration: 1.708s, episode steps: 166, steps per second: 97, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.386 [-2.805, 1.059], loss: 26.298216, mean_squared_error: 10875.610352, mean_q: 145.635864\n",
      "551 {'episode_reward': 165.0, 'nb_episode_steps': 165, 'nb_steps': 20701}\n",
      " 20701/100000: episode: 552, duration: 1.709s, episode steps: 165, steps per second: 97, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.461 [0.000, 1.000], mean observation: -0.385 [-2.815, 1.163], loss: 37.745556, mean_squared_error: 10652.555664, mean_q: 143.939606\n",
      "552 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 20901}\n",
      " 20901/100000: episode: 553, duration: 2.080s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.170 [-1.478, 0.495], loss: 27.555805, mean_squared_error: 10538.375000, mean_q: 143.030045\n",
      "553 {'episode_reward': 193.0, 'nb_episode_steps': 193, 'nb_steps': 21094}\n",
      " 21094/100000: episode: 554, duration: 1.952s, episode steps: 193, steps per second: 99, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.326 [-3.173, 1.811], loss: 26.698957, mean_squared_error: 10919.241211, mean_q: 145.443130\n",
      "554 {'episode_reward': 198.0, 'nb_episode_steps': 198, 'nb_steps': 21292}\n",
      " 21292/100000: episode: 555, duration: 2.037s, episode steps: 198, steps per second: 97, episode reward: 198.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.465 [0.000, 1.000], mean observation: -0.318 [-2.818, 1.387], loss: 36.689430, mean_squared_error: 10508.254883, mean_q: 141.929230\n",
      "555 {'episode_reward': 191.0, 'nb_episode_steps': 191, 'nb_steps': 21483}\n",
      " 21483/100000: episode: 556, duration: 1.983s, episode steps: 191, steps per second: 96, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.319 [-2.800, 1.435], loss: 26.889305, mean_squared_error: 10283.838867, mean_q: 139.925110\n",
      "556 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 21683}\n",
      " 21683/100000: episode: 557, duration: 2.066s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.155 [-1.304, 0.575], loss: 29.795706, mean_squared_error: 9944.172852, mean_q: 137.122894\n",
      "557 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 21883}\n",
      " 21883/100000: episode: 558, duration: 2.063s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.191 [-1.483, 0.526], loss: 28.756977, mean_squared_error: 9676.143555, mean_q: 135.149506\n",
      "558 {'episode_reward': 161.0, 'nb_episode_steps': 161, 'nb_steps': 22044}\n",
      " 22044/100000: episode: 559, duration: 1.682s, episode steps: 161, steps per second: 96, episode reward: 161.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.318 [-3.637, 3.211], loss: 33.233707, mean_squared_error: 9516.225586, mean_q: 133.815567\n",
      "559 {'episode_reward': 175.0, 'nb_episode_steps': 175, 'nb_steps': 22219}\n",
      " 22219/100000: episode: 560, duration: 1.823s, episode steps: 175, steps per second: 96, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.451 [0.000, 1.000], mean observation: -0.307 [-3.175, 2.565], loss: 27.079905, mean_squared_error: 9283.792969, mean_q: 131.578430\n",
      "560 {'episode_reward': 154.0, 'nb_episode_steps': 154, 'nb_steps': 22373}\n",
      " 22373/100000: episode: 561, duration: 1.621s, episode steps: 154, steps per second: 95, episode reward: 154.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.448 [0.000, 1.000], mean observation: -0.321 [-3.015, 2.359], loss: 21.729689, mean_squared_error: 9340.261719, mean_q: 132.027283\n",
      "561 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 22573}\n",
      " 22573/100000: episode: 562, duration: 2.068s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.199 [-2.257, 1.230], loss: 25.566130, mean_squared_error: 9103.606445, mean_q: 129.620178\n",
      "562 {'episode_reward': 159.0, 'nb_episode_steps': 159, 'nb_steps': 22732}\n",
      " 22732/100000: episode: 563, duration: 1.649s, episode steps: 159, steps per second: 96, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.297 [-3.200, 2.688], loss: 28.741076, mean_squared_error: 9003.075195, mean_q: 128.599197\n",
      "563 {'episode_reward': 176.0, 'nb_episode_steps': 176, 'nb_steps': 22908}\n",
      " 22908/100000: episode: 564, duration: 1.844s, episode steps: 176, steps per second: 95, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.275 [-3.001, 2.501], loss: 19.953358, mean_squared_error: 8907.299805, mean_q: 128.357361\n",
      "564 {'episode_reward': 175.0, 'nb_episode_steps': 175, 'nb_steps': 23083}\n",
      " 23083/100000: episode: 565, duration: 1.824s, episode steps: 175, steps per second: 96, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.274 [-2.835, 2.205], loss: 25.329618, mean_squared_error: 8863.028320, mean_q: 127.648155\n",
      "565 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 23283}\n",
      " 23283/100000: episode: 566, duration: 2.051s, episode steps: 200, steps per second: 98, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.089 [-0.745, 0.441], loss: 15.543641, mean_squared_error: 8793.130859, mean_q: 126.776047\n",
      "566 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 23483}\n",
      " 23483/100000: episode: 567, duration: 2.057s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.216 [-2.785, 2.043], loss: 17.305979, mean_squared_error: 8983.463867, mean_q: 128.403824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567 {'episode_reward': 177.0, 'nb_episode_steps': 177, 'nb_steps': 23660}\n",
      " 23660/100000: episode: 568, duration: 1.836s, episode steps: 177, steps per second: 96, episode reward: 177.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.309 [-2.453, 2.303], loss: 22.489864, mean_squared_error: 9220.784180, mean_q: 130.251129\n",
      "568 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 23860}\n",
      " 23860/100000: episode: 569, duration: 2.123s, episode steps: 200, steps per second: 94, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.179 [-1.857, 1.234], loss: 19.974297, mean_squared_error: 8473.539062, mean_q: 123.935509\n",
      "569 {'episode_reward': 197.0, 'nb_episode_steps': 197, 'nb_steps': 24057}\n",
      " 24057/100000: episode: 570, duration: 2.059s, episode steps: 197, steps per second: 96, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.194 [-2.205, 1.593], loss: 21.677311, mean_squared_error: 8123.574707, mean_q: 120.197906\n",
      "570 {'episode_reward': 153.0, 'nb_episode_steps': 153, 'nb_steps': 24210}\n",
      " 24210/100000: episode: 571, duration: 1.580s, episode steps: 153, steps per second: 97, episode reward: 153.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.235 [-2.660, 2.202], loss: 21.757355, mean_squared_error: 7922.857910, mean_q: 117.982140\n",
      "571 {'episode_reward': 160.0, 'nb_episode_steps': 160, 'nb_steps': 24370}\n",
      " 24370/100000: episode: 572, duration: 1.634s, episode steps: 160, steps per second: 98, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.221 [-2.434, 1.945], loss: 21.482018, mean_squared_error: 7480.885742, mean_q: 114.442383\n",
      "572 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 24570}\n",
      " 24570/100000: episode: 573, duration: 2.082s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.470 [0.000, 1.000], mean observation: -0.222 [-2.265, 2.210], loss: 20.618219, mean_squared_error: 7107.521484, mean_q: 110.894379\n",
      "573 {'episode_reward': 193.0, 'nb_episode_steps': 193, 'nb_steps': 24763}\n",
      " 24763/100000: episode: 574, duration: 2.012s, episode steps: 193, steps per second: 96, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.158 [-2.439, 2.135], loss: 21.643654, mean_squared_error: 6723.561523, mean_q: 106.986855\n",
      "574 {'episode_reward': 188.0, 'nb_episode_steps': 188, 'nb_steps': 24951}\n",
      " 24951/100000: episode: 575, duration: 1.960s, episode steps: 188, steps per second: 96, episode reward: 188.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.270 [-3.470, 3.610], loss: 12.068601, mean_squared_error: 6314.729980, mean_q: 103.480431\n",
      "575 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 25151}\n",
      " 25151/100000: episode: 576, duration: 2.057s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.104 [-0.775, 0.642], loss: 15.484314, mean_squared_error: 5881.440430, mean_q: 98.563499\n",
      "576 {'episode_reward': 181.0, 'nb_episode_steps': 181, 'nb_steps': 25332}\n",
      " 25332/100000: episode: 577, duration: 1.891s, episode steps: 181, steps per second: 96, episode reward: 181.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.227 [-2.484, 2.905], loss: 16.782570, mean_squared_error: 5448.646973, mean_q: 94.396774\n",
      "577 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 25532}\n",
      " 25532/100000: episode: 578, duration: 2.064s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.532, 0.645], loss: 12.145294, mean_squared_error: 5281.280762, mean_q: 92.200195\n",
      "578 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 25732}\n",
      " 25732/100000: episode: 579, duration: 2.097s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.169 [-1.308, 0.737], loss: 14.482470, mean_squared_error: 5373.685547, mean_q: 93.411652\n",
      "579 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 25932}\n",
      " 25932/100000: episode: 580, duration: 2.084s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.167 [-1.223, 0.650], loss: 16.696922, mean_squared_error: 5156.162109, mean_q: 90.324646\n",
      "580 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 26132}\n",
      " 26132/100000: episode: 581, duration: 2.075s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-0.560, 0.546], loss: 16.572348, mean_squared_error: 5180.453125, mean_q: 90.656174\n",
      "581 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 26332}\n",
      " 26332/100000: episode: 582, duration: 2.109s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.129 [-0.455, 0.949], loss: 15.516040, mean_squared_error: 5280.001953, mean_q: 91.467010\n",
      "582 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 26532}\n",
      " 26532/100000: episode: 583, duration: 2.067s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.123 [-0.425, 0.952], loss: 14.091321, mean_squared_error: 5432.658691, mean_q: 92.602913\n",
      "583 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 26732}\n",
      " 26732/100000: episode: 584, duration: 2.050s, episode steps: 200, steps per second: 98, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-0.632, 0.552], loss: 26.050293, mean_squared_error: 5576.906250, mean_q: 94.429802\n",
      "584 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 26932}\n",
      " 26932/100000: episode: 585, duration: 2.063s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.266 [-0.533, 1.904], loss: 17.849180, mean_squared_error: 6061.309570, mean_q: 99.288910\n",
      "585 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 27132}\n",
      " 27132/100000: episode: 586, duration: 2.107s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.548, 0.650], loss: 13.616247, mean_squared_error: 6771.582520, mean_q: 106.021194\n",
      "586 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 27332}\n",
      " 27332/100000: episode: 587, duration: 2.035s, episode steps: 200, steps per second: 98, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.119 [-0.806, 0.515], loss: 20.735664, mean_squared_error: 7457.141113, mean_q: 112.541618\n",
      "587 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 27532}\n",
      " 27532/100000: episode: 588, duration: 2.087s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.612, 0.638], loss: 19.625549, mean_squared_error: 7850.071289, mean_q: 117.041534\n",
      "588 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 27732}\n",
      " 27732/100000: episode: 589, duration: 2.085s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.042 [-0.672, 0.605], loss: 22.911474, mean_squared_error: 7772.662598, mean_q: 115.734222\n",
      "589 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 27932}\n",
      " 27932/100000: episode: 590, duration: 2.080s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.098 [-0.707, 0.648], loss: 18.718441, mean_squared_error: 7791.001465, mean_q: 116.782562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 28132}\n",
      " 28132/100000: episode: 591, duration: 2.088s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.081 [-0.925, 0.865], loss: 17.628426, mean_squared_error: 7965.663574, mean_q: 117.516731\n",
      "591 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 28332}\n",
      " 28332/100000: episode: 592, duration: 2.063s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.066 [-0.683, 0.596], loss: 12.963902, mean_squared_error: 7737.897949, mean_q: 115.769104\n",
      "592 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 28532}\n",
      " 28532/100000: episode: 593, duration: 2.145s, episode steps: 200, steps per second: 93, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.179 [-1.461, 0.879], loss: 22.100121, mean_squared_error: 7445.215820, mean_q: 113.030319\n",
      "593 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 28732}\n",
      " 28732/100000: episode: 594, duration: 2.087s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.150 [-1.272, 0.689], loss: 15.828997, mean_squared_error: 7464.814453, mean_q: 113.435371\n",
      "594 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 28932}\n",
      " 28932/100000: episode: 595, duration: 2.062s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.052 [-0.666, 0.699], loss: 14.872810, mean_squared_error: 7939.563965, mean_q: 116.806023\n",
      "595 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 29132}\n",
      " 29132/100000: episode: 596, duration: 2.078s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.055 [-0.809, 0.730], loss: 19.418608, mean_squared_error: 7947.571289, mean_q: 115.751526\n",
      "596 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 29332}\n",
      " 29332/100000: episode: 597, duration: 2.086s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.099 [-0.746, 0.593], loss: 18.337929, mean_squared_error: 7709.925781, mean_q: 114.007904\n",
      "597 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 29532}\n",
      " 29532/100000: episode: 598, duration: 2.087s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.294 [-0.727, 2.100], loss: 22.154482, mean_squared_error: 7569.196289, mean_q: 112.748360\n",
      "598 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 29732}\n",
      " 29732/100000: episode: 599, duration: 2.109s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.021 [-0.519, 0.612], loss: 23.272873, mean_squared_error: 7357.958984, mean_q: 110.726036\n",
      "599 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 29932}\n",
      " 29932/100000: episode: 600, duration: 2.057s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.062 [-0.604, 0.538], loss: 17.277185, mean_squared_error: 7717.441406, mean_q: 114.025658\n",
      "600 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 30132}\n",
      " 30132/100000: episode: 601, duration: 2.032s, episode steps: 200, steps per second: 98, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.031 [-0.858, 0.544], loss: 23.489857, mean_squared_error: 7387.331055, mean_q: 111.258614\n",
      "601 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 30332}\n",
      " 30332/100000: episode: 602, duration: 2.064s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.133 [-1.530, 1.278], loss: 10.565007, mean_squared_error: 7120.104980, mean_q: 108.120117\n",
      "602 {'episode_reward': 175.0, 'nb_episode_steps': 175, 'nb_steps': 30507}\n",
      " 30507/100000: episode: 603, duration: 1.803s, episode steps: 175, steps per second: 97, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.318 [-0.649, 2.060], loss: 22.355661, mean_squared_error: 6641.905762, mean_q: 103.633591\n",
      "603 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 30707}\n",
      " 30707/100000: episode: 604, duration: 2.085s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.195 [-0.587, 1.650], loss: 18.513330, mean_squared_error: 6900.971191, mean_q: 105.960846\n",
      "604 {'episode_reward': 176.0, 'nb_episode_steps': 176, 'nb_steps': 30883}\n",
      " 30883/100000: episode: 605, duration: 1.848s, episode steps: 176, steps per second: 95, episode reward: 176.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.272 [-0.684, 1.810], loss: 27.973206, mean_squared_error: 7087.688965, mean_q: 107.953659\n",
      "605 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 31083}\n",
      " 31083/100000: episode: 606, duration: 2.102s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.021 [-0.622, 0.562], loss: 31.059080, mean_squared_error: 6888.624512, mean_q: 106.004242\n",
      "606 {'episode_reward': 160.0, 'nb_episode_steps': 160, 'nb_steps': 31243}\n",
      " 31243/100000: episode: 607, duration: 1.694s, episode steps: 160, steps per second: 94, episode reward: 160.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.287 [-0.477, 1.840], loss: 21.418522, mean_squared_error: 6479.891602, mean_q: 102.721535\n",
      "607 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 31443}\n",
      " 31443/100000: episode: 608, duration: 2.054s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.136 [-1.500, 1.425], loss: 10.678511, mean_squared_error: 6867.596680, mean_q: 106.356621\n",
      "608 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 31643}\n",
      " 31643/100000: episode: 609, duration: 2.337s, episode steps: 200, steps per second: 86, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.107 [-0.693, 0.701], loss: 16.523182, mean_squared_error: 6740.876465, mean_q: 104.873283\n",
      "609 {'episode_reward': 152.0, 'nb_episode_steps': 152, 'nb_steps': 31795}\n",
      " 31795/100000: episode: 610, duration: 1.581s, episode steps: 152, steps per second: 96, episode reward: 152.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.257 [-0.872, 1.679], loss: 17.759119, mean_squared_error: 6678.248047, mean_q: 103.504173\n",
      "610 {'episode_reward': 195.0, 'nb_episode_steps': 195, 'nb_steps': 31990}\n",
      " 31990/100000: episode: 611, duration: 2.054s, episode steps: 195, steps per second: 95, episode reward: 195.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.114 [-1.718, 1.816], loss: 14.235565, mean_squared_error: 6506.365234, mean_q: 101.245529\n",
      "611 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 32190}\n",
      " 32190/100000: episode: 612, duration: 2.065s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.065 [-0.585, 0.729], loss: 16.837095, mean_squared_error: 6291.325195, mean_q: 99.091873\n",
      "612 {'episode_reward': 148.0, 'nb_episode_steps': 148, 'nb_steps': 32338}\n",
      " 32338/100000: episode: 613, duration: 1.561s, episode steps: 148, steps per second: 95, episode reward: 148.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.527 [0.000, 1.000], mean observation: 0.246 [-0.456, 1.624], loss: 14.298851, mean_squared_error: 6201.588379, mean_q: 98.429359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "613 {'episode_reward': 187.0, 'nb_episode_steps': 187, 'nb_steps': 32525}\n",
      " 32525/100000: episode: 614, duration: 1.951s, episode steps: 187, steps per second: 96, episode reward: 187.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.198 [-0.391, 1.508], loss: 11.866465, mean_squared_error: 6631.041504, mean_q: 101.919350\n",
      "614 {'episode_reward': 139.0, 'nb_episode_steps': 139, 'nb_steps': 32664}\n",
      " 32664/100000: episode: 615, duration: 1.458s, episode steps: 139, steps per second: 95, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.254 [-0.397, 1.663], loss: 23.756590, mean_squared_error: 7078.583496, mean_q: 105.172714\n",
      "615 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 32864}\n",
      " 32864/100000: episode: 616, duration: 2.084s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.052 [-0.543, 0.891], loss: 14.004028, mean_squared_error: 7019.560547, mean_q: 105.208595\n",
      "616 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 33064}\n",
      " 33064/100000: episode: 617, duration: 2.109s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.131 [-0.451, 1.330], loss: 13.659297, mean_squared_error: 7195.414551, mean_q: 106.299728\n",
      "617 {'episode_reward': 179.0, 'nb_episode_steps': 179, 'nb_steps': 33243}\n",
      " 33243/100000: episode: 618, duration: 1.871s, episode steps: 179, steps per second: 96, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.177 [-0.752, 1.305], loss: 6.890306, mean_squared_error: 7358.833984, mean_q: 107.470863\n",
      "618 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 33443}\n",
      " 33443/100000: episode: 619, duration: 2.092s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.087 [-0.614, 0.545], loss: 13.172158, mean_squared_error: 7340.608887, mean_q: 107.377174\n",
      "619 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 33643}\n",
      " 33643/100000: episode: 620, duration: 2.056s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.097 [-0.780, 0.732], loss: 17.304720, mean_squared_error: 7567.056152, mean_q: 108.616814\n",
      "620 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 33843}\n",
      " 33843/100000: episode: 621, duration: 2.082s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.354, 0.512], loss: 15.771494, mean_squared_error: 6868.417480, mean_q: 103.251015\n",
      "621 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 34043}\n",
      " 34043/100000: episode: 622, duration: 2.057s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.117 [-0.820, 0.510], loss: 16.052183, mean_squared_error: 6737.380859, mean_q: 101.171837\n",
      "622 {'episode_reward': 141.0, 'nb_episode_steps': 141, 'nb_steps': 34184}\n",
      " 34184/100000: episode: 623, duration: 1.466s, episode steps: 141, steps per second: 96, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.200 [-0.542, 1.235], loss: 20.534939, mean_squared_error: 7173.535645, mean_q: 104.667847\n",
      "623 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 34384}\n",
      " 34384/100000: episode: 624, duration: 2.085s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.076 [-0.533, 0.894], loss: 8.499969, mean_squared_error: 7547.826660, mean_q: 107.889191\n",
      "624 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 34584}\n",
      " 34584/100000: episode: 625, duration: 2.103s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.089 [-0.390, 0.918], loss: 18.399708, mean_squared_error: 7204.598145, mean_q: 103.992096\n",
      "625 {'episode_reward': 197.0, 'nb_episode_steps': 197, 'nb_steps': 34781}\n",
      " 34781/100000: episode: 626, duration: 2.065s, episode steps: 197, steps per second: 95, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.136 [-0.634, 1.259], loss: 12.804999, mean_squared_error: 7180.380859, mean_q: 103.870399\n",
      "626 {'episode_reward': 168.0, 'nb_episode_steps': 168, 'nb_steps': 34949}\n",
      " 34949/100000: episode: 627, duration: 1.742s, episode steps: 168, steps per second: 96, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.148 [-0.547, 1.120], loss: 17.289219, mean_squared_error: 7849.940430, mean_q: 108.994789\n",
      "627 {'episode_reward': 111.0, 'nb_episode_steps': 111, 'nb_steps': 35060}\n",
      " 35060/100000: episode: 628, duration: 1.147s, episode steps: 111, steps per second: 97, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.523 [0.000, 1.000], mean observation: 0.221 [-0.535, 1.120], loss: 14.991551, mean_squared_error: 8845.347656, mean_q: 116.589081\n",
      "628 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 35260}\n",
      " 35260/100000: episode: 629, duration: 2.088s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.070 [-0.689, 0.536], loss: 12.596821, mean_squared_error: 8042.700684, mean_q: 110.135353\n",
      "629 {'episode_reward': 199.0, 'nb_episode_steps': 199, 'nb_steps': 35459}\n",
      " 35459/100000: episode: 630, duration: 2.061s, episode steps: 199, steps per second: 97, episode reward: 199.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.513 [0.000, 1.000], mean observation: 0.125 [-0.559, 1.077], loss: 9.954372, mean_squared_error: 7822.590332, mean_q: 107.532333\n",
      "630 {'episode_reward': 119.0, 'nb_episode_steps': 119, 'nb_steps': 35578}\n",
      " 35578/100000: episode: 631, duration: 1.238s, episode steps: 119, steps per second: 96, episode reward: 119.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.192 [-0.362, 1.140], loss: 15.706331, mean_squared_error: 7228.910645, mean_q: 102.340996\n",
      "631 {'episode_reward': 117.0, 'nb_episode_steps': 117, 'nb_steps': 35695}\n",
      " 35695/100000: episode: 632, duration: 1.221s, episode steps: 117, steps per second: 96, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.199 [-0.686, 1.325], loss: 9.824640, mean_squared_error: 7331.456543, mean_q: 103.176407\n",
      "632 {'episode_reward': 155.0, 'nb_episode_steps': 155, 'nb_steps': 35850}\n",
      " 35850/100000: episode: 633, duration: 1.632s, episode steps: 155, steps per second: 95, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.144 [-0.389, 1.093], loss: 11.609979, mean_squared_error: 7381.191895, mean_q: 103.611565\n",
      "633 {'episode_reward': 117.0, 'nb_episode_steps': 117, 'nb_steps': 35967}\n",
      " 35967/100000: episode: 634, duration: 1.252s, episode steps: 117, steps per second: 93, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.181 [-0.427, 1.073], loss: 15.991822, mean_squared_error: 7659.061523, mean_q: 105.475616\n",
      "634 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 36167}\n",
      " 36167/100000: episode: 635, duration: 2.076s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.087 [-1.065, 0.796], loss: 19.482613, mean_squared_error: 7479.458984, mean_q: 104.111038\n",
      "635 {'episode_reward': 127.0, 'nb_episode_steps': 127, 'nb_steps': 36294}\n",
      " 36294/100000: episode: 636, duration: 1.331s, episode steps: 127, steps per second: 95, episode reward: 127.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.150 [-0.574, 1.057], loss: 11.682011, mean_squared_error: 7189.733398, mean_q: 100.967018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636 {'episode_reward': 97.0, 'nb_episode_steps': 97, 'nb_steps': 36391}\n",
      " 36391/100000: episode: 637, duration: 1.057s, episode steps: 97, steps per second: 92, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.191 [-0.410, 1.150], loss: 6.422316, mean_squared_error: 6991.574707, mean_q: 99.726372\n",
      "637 {'episode_reward': 133.0, 'nb_episode_steps': 133, 'nb_steps': 36524}\n",
      " 36524/100000: episode: 638, duration: 1.398s, episode steps: 133, steps per second: 95, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.142 [-0.450, 0.924], loss: 20.558647, mean_squared_error: 6521.292480, mean_q: 95.723625\n",
      "638 {'episode_reward': 96.0, 'nb_episode_steps': 96, 'nb_steps': 36620}\n",
      " 36620/100000: episode: 639, duration: 1.000s, episode steps: 96, steps per second: 96, episode reward: 96.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.521 [0.000, 1.000], mean observation: 0.186 [-0.525, 1.003], loss: 7.930719, mean_squared_error: 6705.482910, mean_q: 96.614349\n",
      "639 {'episode_reward': 129.0, 'nb_episode_steps': 129, 'nb_steps': 36749}\n",
      " 36749/100000: episode: 640, duration: 1.354s, episode steps: 129, steps per second: 95, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.140 [-0.426, 1.165], loss: 10.202357, mean_squared_error: 7163.436035, mean_q: 100.549324\n",
      "640 {'episode_reward': 129.0, 'nb_episode_steps': 129, 'nb_steps': 36878}\n",
      " 36878/100000: episode: 641, duration: 1.351s, episode steps: 129, steps per second: 95, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: 0.132 [-0.516, 1.139], loss: 9.590219, mean_squared_error: 7072.134277, mean_q: 99.445602\n",
      "641 {'episode_reward': 116.0, 'nb_episode_steps': 116, 'nb_steps': 36994}\n",
      " 36994/100000: episode: 642, duration: 1.215s, episode steps: 116, steps per second: 95, episode reward: 116.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.154 [-0.449, 1.429], loss: 14.815341, mean_squared_error: 6936.203613, mean_q: 98.007057\n",
      "642 {'episode_reward': 168.0, 'nb_episode_steps': 168, 'nb_steps': 37162}\n",
      " 37162/100000: episode: 643, duration: 1.763s, episode steps: 168, steps per second: 95, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.506 [0.000, 1.000], mean observation: 0.100 [-0.510, 1.300], loss: 8.453024, mean_squared_error: 7044.254395, mean_q: 99.437805\n",
      "643 {'episode_reward': 102.0, 'nb_episode_steps': 102, 'nb_steps': 37264}\n",
      " 37264/100000: episode: 644, duration: 1.078s, episode steps: 102, steps per second: 95, episode reward: 102.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.156 [-0.388, 1.620], loss: 5.859938, mean_squared_error: 7292.509766, mean_q: 100.987038\n",
      "644 {'episode_reward': 134.0, 'nb_episode_steps': 134, 'nb_steps': 37398}\n",
      " 37398/100000: episode: 645, duration: 1.408s, episode steps: 134, steps per second: 95, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.507 [0.000, 1.000], mean observation: 0.121 [-0.404, 1.181], loss: 8.726832, mean_squared_error: 7005.069824, mean_q: 98.041946\n",
      "645 {'episode_reward': 179.0, 'nb_episode_steps': 179, 'nb_steps': 37577}\n",
      " 37577/100000: episode: 646, duration: 1.875s, episode steps: 179, steps per second: 95, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.503 [0.000, 1.000], mean observation: 0.079 [-0.483, 1.338], loss: 10.549980, mean_squared_error: 6727.428711, mean_q: 95.895622\n",
      "646 {'episode_reward': 86.0, 'nb_episode_steps': 86, 'nb_steps': 37663}\n",
      " 37663/100000: episode: 647, duration: 0.897s, episode steps: 86, steps per second: 96, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.158 [-0.509, 1.529], loss: 10.387653, mean_squared_error: 6758.081543, mean_q: 95.792221\n",
      "647 {'episode_reward': 93.0, 'nb_episode_steps': 93, 'nb_steps': 37756}\n",
      " 37756/100000: episode: 648, duration: 0.964s, episode steps: 93, steps per second: 96, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.151 [-0.523, 1.724], loss: 7.100169, mean_squared_error: 6431.503906, mean_q: 92.853287\n",
      "648 {'episode_reward': 99.0, 'nb_episode_steps': 99, 'nb_steps': 37855}\n",
      " 37855/100000: episode: 649, duration: 1.022s, episode steps: 99, steps per second: 97, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.134 [-0.489, 1.391], loss: 9.122485, mean_squared_error: 5919.708496, mean_q: 89.055298\n",
      "649 {'episode_reward': 88.0, 'nb_episode_steps': 88, 'nb_steps': 37943}\n",
      " 37943/100000: episode: 650, duration: 0.920s, episode steps: 88, steps per second: 96, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.142 [-0.368, 1.197], loss: 16.309221, mean_squared_error: 5783.644043, mean_q: 87.890541\n",
      "650 {'episode_reward': 121.0, 'nb_episode_steps': 121, 'nb_steps': 38064}\n",
      " 38064/100000: episode: 651, duration: 1.281s, episode steps: 121, steps per second: 94, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: 0.108 [-0.369, 1.179], loss: 13.709732, mean_squared_error: 6023.902344, mean_q: 90.796638\n",
      "651 {'episode_reward': 173.0, 'nb_episode_steps': 173, 'nb_steps': 38237}\n",
      " 38237/100000: episode: 652, duration: 1.845s, episode steps: 173, steps per second: 94, episode reward: 173.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.497 [0.000, 1.000], mean observation: 0.059 [-0.805, 1.672], loss: 16.849989, mean_squared_error: 5672.676270, mean_q: 86.899857\n",
      "652 {'episode_reward': 168.0, 'nb_episode_steps': 168, 'nb_steps': 38405}\n",
      " 38405/100000: episode: 653, duration: 1.736s, episode steps: 168, steps per second: 97, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.067 [-0.536, 1.861], loss: 5.958439, mean_squared_error: 5296.735352, mean_q: 83.108086\n",
      "653 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 38605}\n",
      " 38605/100000: episode: 654, duration: 2.123s, episode steps: 200, steps per second: 94, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.052 [-0.612, 0.712], loss: 6.937834, mean_squared_error: 4870.360840, mean_q: 79.148895\n",
      "654 {'episode_reward': 66.0, 'nb_episode_steps': 66, 'nb_steps': 38671}\n",
      " 38671/100000: episode: 655, duration: 0.678s, episode steps: 66, steps per second: 97, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.141 [-0.648, 1.505], loss: 10.041698, mean_squared_error: 4705.736328, mean_q: 78.085434\n",
      "655 {'episode_reward': 84.0, 'nb_episode_steps': 84, 'nb_steps': 38755}\n",
      " 38755/100000: episode: 656, duration: 0.887s, episode steps: 84, steps per second: 95, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.119 [-0.549, 1.769], loss: 4.238674, mean_squared_error: 5132.816406, mean_q: 82.605721\n",
      "656 {'episode_reward': 133.0, 'nb_episode_steps': 133, 'nb_steps': 38888}\n",
      " 38888/100000: episode: 657, duration: 1.407s, episode steps: 133, steps per second: 95, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.078 [-0.663, 2.042], loss: 6.483161, mean_squared_error: 4761.112793, mean_q: 79.162651\n",
      "657 {'episode_reward': 117.0, 'nb_episode_steps': 117, 'nb_steps': 39005}\n",
      " 39005/100000: episode: 658, duration: 1.236s, episode steps: 117, steps per second: 95, episode reward: 117.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.084 [-0.602, 1.931], loss: 5.565217, mean_squared_error: 4674.193359, mean_q: 78.318573\n",
      "658 {'episode_reward': 97.0, 'nb_episode_steps': 97, 'nb_steps': 39102}\n",
      " 39102/100000: episode: 659, duration: 1.025s, episode steps: 97, steps per second: 95, episode reward: 97.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.104 [-0.620, 2.019], loss: 3.858943, mean_squared_error: 4560.879395, mean_q: 75.774864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659 {'episode_reward': 133.0, 'nb_episode_steps': 133, 'nb_steps': 39235}\n",
      " 39235/100000: episode: 660, duration: 1.426s, episode steps: 133, steps per second: 93, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.077 [-0.582, 1.875], loss: 7.201098, mean_squared_error: 4542.433105, mean_q: 76.437103\n",
      "660 {'episode_reward': 72.0, 'nb_episode_steps': 72, 'nb_steps': 39307}\n",
      " 39307/100000: episode: 661, duration: 0.775s, episode steps: 72, steps per second: 93, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.123 [-0.781, 2.202], loss: 3.864079, mean_squared_error: 4103.341309, mean_q: 72.045975\n",
      "661 {'episode_reward': 83.0, 'nb_episode_steps': 83, 'nb_steps': 39390}\n",
      " 39390/100000: episode: 662, duration: 0.864s, episode steps: 83, steps per second: 96, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: 0.113 [-0.398, 1.534], loss: 3.902164, mean_squared_error: 4150.004395, mean_q: 72.643257\n",
      "662 {'episode_reward': 103.0, 'nb_episode_steps': 103, 'nb_steps': 39493}\n",
      " 39493/100000: episode: 663, duration: 1.061s, episode steps: 103, steps per second: 97, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.087 [-0.644, 1.930], loss: 9.225848, mean_squared_error: 4281.232910, mean_q: 74.169029\n",
      "663 {'episode_reward': 99.0, 'nb_episode_steps': 99, 'nb_steps': 39592}\n",
      " 39592/100000: episode: 664, duration: 1.034s, episode steps: 99, steps per second: 96, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.095 [-0.579, 1.814], loss: 8.456363, mean_squared_error: 3974.649414, mean_q: 70.784424\n",
      "664 {'episode_reward': 81.0, 'nb_episode_steps': 81, 'nb_steps': 39673}\n",
      " 39673/100000: episode: 665, duration: 0.871s, episode steps: 81, steps per second: 93, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.112 [-0.598, 1.842], loss: 5.167052, mean_squared_error: 4216.554688, mean_q: 73.361595\n",
      "665 {'episode_reward': 86.0, 'nb_episode_steps': 86, 'nb_steps': 39759}\n",
      " 39759/100000: episode: 666, duration: 0.906s, episode steps: 86, steps per second: 95, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.107 [-0.415, 1.625], loss: 7.740998, mean_squared_error: 3918.284180, mean_q: 69.753609\n",
      "666 {'episode_reward': 110.0, 'nb_episode_steps': 110, 'nb_steps': 39869}\n",
      " 39869/100000: episode: 667, duration: 1.185s, episode steps: 110, steps per second: 93, episode reward: 110.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.081 [-0.803, 2.075], loss: 3.836937, mean_squared_error: 4453.959473, mean_q: 75.572357\n",
      "667 {'episode_reward': 106.0, 'nb_episode_steps': 106, 'nb_steps': 39975}\n",
      " 39975/100000: episode: 668, duration: 1.152s, episode steps: 106, steps per second: 92, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.090 [-0.770, 2.015], loss: 3.654166, mean_squared_error: 4175.020020, mean_q: 72.517914\n",
      "668 {'episode_reward': 141.0, 'nb_episode_steps': 141, 'nb_steps': 40116}\n",
      " 40116/100000: episode: 669, duration: 1.494s, episode steps: 141, steps per second: 94, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: 0.065 [-0.972, 2.288], loss: 4.752451, mean_squared_error: 3723.028809, mean_q: 68.377434\n",
      "669 {'episode_reward': 89.0, 'nb_episode_steps': 89, 'nb_steps': 40205}\n",
      " 40205/100000: episode: 670, duration: 0.958s, episode steps: 89, steps per second: 93, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.097 [-1.046, 2.387], loss: 5.292443, mean_squared_error: 3794.761230, mean_q: 69.309837\n",
      "670 {'episode_reward': 179.0, 'nb_episode_steps': 179, 'nb_steps': 40384}\n",
      " 40384/100000: episode: 671, duration: 1.871s, episode steps: 179, steps per second: 96, episode reward: 179.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.052 [-1.060, 2.352], loss: 7.417974, mean_squared_error: 3935.957031, mean_q: 71.395302\n",
      "671 {'episode_reward': 120.0, 'nb_episode_steps': 120, 'nb_steps': 40504}\n",
      " 40504/100000: episode: 672, duration: 1.243s, episode steps: 120, steps per second: 97, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.475 [0.000, 1.000], mean observation: 0.077 [-1.179, 2.545], loss: 3.564442, mean_squared_error: 3695.095947, mean_q: 69.894302\n",
      "672 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 40704}\n",
      " 40704/100000: episode: 673, duration: 2.068s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.556, 0.390], loss: 3.759872, mean_squared_error: 4108.394531, mean_q: 73.938942\n",
      "673 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 40904}\n",
      " 40904/100000: episode: 674, duration: 2.057s, episode steps: 200, steps per second: 97, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-0.610, 0.571], loss: 4.796705, mean_squared_error: 4805.211914, mean_q: 81.685677\n",
      "674 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 41104}\n",
      " 41104/100000: episode: 675, duration: 2.100s, episode steps: 200, steps per second: 95, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.528, 0.593], loss: 8.368564, mean_squared_error: 6741.243652, mean_q: 98.541092\n",
      "675 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 41304}\n",
      " 41304/100000: episode: 676, duration: 2.073s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.048 [-0.428, 0.593], loss: 42.653507, mean_squared_error: 15476.790039, mean_q: 156.808640\n",
      "676 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 41504}\n",
      " 41504/100000: episode: 677, duration: 2.082s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-0.552, 0.552], loss: 103.314453, mean_squared_error: 33153.210938, mean_q: 221.278229\n",
      "677 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 41704}\n",
      " 41704/100000: episode: 678, duration: 2.089s, episode steps: 200, steps per second: 96, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.150 [-0.955, 0.692], loss: 365.701324, mean_squared_error: 49932.000000, mean_q: 262.097656\n",
      "678 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 41904}\n",
      " 41904/100000: episode: 679, duration: 2.480s, episode steps: 200, steps per second: 81, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.240 [-1.244, 0.653], loss: 434.902863, mean_squared_error: 48707.558594, mean_q: 252.868561\n",
      "679 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 42104}\n",
      " 42104/100000: episode: 680, duration: 2.381s, episode steps: 200, steps per second: 84, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.180 [-1.274, 0.766], loss: 272.795013, mean_squared_error: 49761.878906, mean_q: 253.284653\n",
      "680 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 42304}\n",
      " 42304/100000: episode: 681, duration: 2.160s, episode steps: 200, steps per second: 93, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.330 [-1.555, 0.614], loss: 559.282898, mean_squared_error: 54696.035156, mean_q: 269.615540\n",
      "681 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 42504}\n",
      " 42504/100000: episode: 682, duration: 2.159s, episode steps: 200, steps per second: 93, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.219 [-1.376, 0.605], loss: 424.679840, mean_squared_error: 54719.164062, mean_q: 272.533020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 42704}\n",
      " 42704/100000: episode: 683, duration: 2.240s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.280 [-1.514, 0.602], loss: 367.820068, mean_squared_error: 54147.960938, mean_q: 267.624969\n",
      "683 {'episode_reward': 55.0, 'nb_episode_steps': 55, 'nb_steps': 42759}\n",
      " 42759/100000: episode: 684, duration: 0.641s, episode steps: 55, steps per second: 86, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.063 [-1.145, 2.189], loss: 292.375885, mean_squared_error: 54592.332031, mean_q: 269.167938\n",
      "684 {'episode_reward': 54.0, 'nb_episode_steps': 54, 'nb_steps': 42813}\n",
      " 42813/100000: episode: 685, duration: 0.669s, episode steps: 54, steps per second: 81, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.068 [-1.191, 2.319], loss: 615.560242, mean_squared_error: 59008.246094, mean_q: 283.789734\n",
      "685 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 43013}\n",
      " 43013/100000: episode: 686, duration: 2.258s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.298 [-1.584, 0.666], loss: 417.956909, mean_squared_error: 54221.058594, mean_q: 271.281250\n",
      "686 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 43213}\n",
      " 43213/100000: episode: 687, duration: 2.201s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.217 [-1.349, 0.590], loss: 456.106079, mean_squared_error: 56788.414062, mean_q: 280.302002\n",
      "687 {'episode_reward': 47.0, 'nb_episode_steps': 47, 'nb_steps': 43260}\n",
      " 43260/100000: episode: 688, duration: 0.538s, episode steps: 47, steps per second: 87, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: 0.075 [-0.990, 2.048], loss: 469.309875, mean_squared_error: 54731.621094, mean_q: 275.500214\n",
      "688 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 43460}\n",
      " 43460/100000: episode: 689, duration: 2.256s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.312 [-1.629, 0.556], loss: 460.432190, mean_squared_error: 53321.101562, mean_q: 273.257538\n",
      "689 {'episode_reward': 62.0, 'nb_episode_steps': 62, 'nb_steps': 43522}\n",
      " 43522/100000: episode: 690, duration: 0.701s, episode steps: 62, steps per second: 88, episode reward: 62.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: 0.064 [-1.174, 2.362], loss: 510.199860, mean_squared_error: 48970.222656, mean_q: 258.190613\n",
      "690 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 43722}\n",
      " 43722/100000: episode: 691, duration: 2.232s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.131 [-1.350, 0.730], loss: 274.845184, mean_squared_error: 49589.824219, mean_q: 265.621765\n",
      "691 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 43922}\n",
      " 43922/100000: episode: 692, duration: 2.257s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.274 [-1.569, 0.820], loss: 302.362274, mean_squared_error: 50691.375000, mean_q: 271.428406\n",
      "692 {'episode_reward': 27.0, 'nb_episode_steps': 27, 'nb_steps': 43949}\n",
      " 43949/100000: episode: 693, duration: 0.324s, episode steps: 27, steps per second: 83, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.110 [-0.567, 1.494], loss: 311.299408, mean_squared_error: 49573.695312, mean_q: 265.639679\n",
      "693 {'episode_reward': 33.0, 'nb_episode_steps': 33, 'nb_steps': 43982}\n",
      " 43982/100000: episode: 694, duration: 0.391s, episode steps: 33, steps per second: 85, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.084 [-0.639, 1.635], loss: 286.715088, mean_squared_error: 46680.253906, mean_q: 255.327744\n",
      "694 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 44182}\n",
      " 44182/100000: episode: 695, duration: 2.207s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.156 [-1.270, 0.634], loss: 384.271301, mean_squared_error: 46353.394531, mean_q: 259.333252\n",
      "695 {'episode_reward': 31.0, 'nb_episode_steps': 31, 'nb_steps': 44213}\n",
      " 44213/100000: episode: 696, duration: 0.370s, episode steps: 31, steps per second: 84, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: 0.103 [-0.979, 2.070], loss: 435.832581, mean_squared_error: 46184.832031, mean_q: 263.258698\n",
      "696 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 44413}\n",
      " 44413/100000: episode: 697, duration: 2.219s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.076 [-1.127, 0.573], loss: 263.730927, mean_squared_error: 43357.953125, mean_q: 253.664322\n",
      "697 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 44613}\n",
      " 44613/100000: episode: 698, duration: 2.209s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.073 [-1.097, 0.590], loss: 242.803360, mean_squared_error: 43910.089844, mean_q: 257.499115\n",
      "698 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 44813}\n",
      " 44813/100000: episode: 699, duration: 2.231s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.304 [-1.868, 0.505], loss: 312.938416, mean_squared_error: 43468.984375, mean_q: 258.486633\n",
      "699 {'episode_reward': 45.0, 'nb_episode_steps': 45, 'nb_steps': 44858}\n",
      " 44858/100000: episode: 700, duration: 0.517s, episode steps: 45, steps per second: 87, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.082 [-0.628, 1.672], loss: 125.391235, mean_squared_error: 41900.203125, mean_q: 253.615692\n",
      "700 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 45058}\n",
      " 45058/100000: episode: 701, duration: 2.192s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.241 [-1.604, 0.618], loss: 122.047340, mean_squared_error: 43280.160156, mean_q: 261.069092\n",
      "701 {'episode_reward': 93.0, 'nb_episode_steps': 93, 'nb_steps': 45151}\n",
      " 45151/100000: episode: 702, duration: 1.040s, episode steps: 93, steps per second: 89, episode reward: 93.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.473 [0.000, 1.000], mean observation: 0.065 [-1.001, 2.158], loss: 283.247192, mean_squared_error: 42010.539062, mean_q: 257.734558\n",
      "702 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 45351}\n",
      " 45351/100000: episode: 703, duration: 2.181s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.206 [-1.424, 0.533], loss: 115.397064, mean_squared_error: 39870.851562, mean_q: 250.809265\n",
      "703 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 45551}\n",
      " 45551/100000: episode: 704, duration: 2.232s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.344 [-1.932, 0.847], loss: 111.173454, mean_squared_error: 39706.542969, mean_q: 251.846603\n",
      "704 {'episode_reward': 64.0, 'nb_episode_steps': 64, 'nb_steps': 45615}\n",
      " 45615/100000: episode: 705, duration: 0.741s, episode steps: 64, steps per second: 86, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.079 [-0.844, 2.003], loss: 143.721344, mean_squared_error: 39523.218750, mean_q: 250.337494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "705 {'episode_reward': 78.0, 'nb_episode_steps': 78, 'nb_steps': 45693}\n",
      " 45693/100000: episode: 706, duration: 0.863s, episode steps: 78, steps per second: 90, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.072 [-0.655, 1.668], loss: 164.331223, mean_squared_error: 38543.710938, mean_q: 246.708817\n",
      "706 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 45893}\n",
      " 45893/100000: episode: 707, duration: 2.210s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.283 [-1.808, 0.521], loss: 147.948471, mean_squared_error: 38092.488281, mean_q: 247.616837\n",
      "707 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 46093}\n",
      " 46093/100000: episode: 708, duration: 2.183s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.426, 0.501], loss: 92.621407, mean_squared_error: 38252.031250, mean_q: 250.088364\n",
      "708 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 46293}\n",
      " 46293/100000: episode: 709, duration: 2.221s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.309 [-1.786, 0.657], loss: 131.514114, mean_squared_error: 36590.765625, mean_q: 243.498596\n",
      "709 {'episode_reward': 180.0, 'nb_episode_steps': 180, 'nb_steps': 46473}\n",
      " 46473/100000: episode: 710, duration: 1.988s, episode steps: 180, steps per second: 91, episode reward: 180.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.456 [0.000, 1.000], mean observation: -0.308 [-3.030, 3.401], loss: 129.003143, mean_squared_error: 35489.503906, mean_q: 240.806030\n",
      "710 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 46673}\n",
      " 46673/100000: episode: 711, duration: 2.223s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.460 [0.000, 1.000], mean observation: -0.247 [-3.033, 3.344], loss: 90.071007, mean_squared_error: 37223.875000, mean_q: 246.164993\n",
      "711 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 46873}\n",
      " 46873/100000: episode: 712, duration: 2.233s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.155 [-1.153, 0.596], loss: 82.521126, mean_squared_error: 34405.164062, mean_q: 234.825119\n",
      "712 {'episode_reward': 184.0, 'nb_episode_steps': 184, 'nb_steps': 47057}\n",
      " 47057/100000: episode: 713, duration: 2.075s, episode steps: 184, steps per second: 89, episode reward: 184.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.188 [-2.651, 2.970], loss: 84.883659, mean_squared_error: 29637.783203, mean_q: 216.750046\n",
      "713 {'episode_reward': 166.0, 'nb_episode_steps': 166, 'nb_steps': 47223}\n",
      " 47223/100000: episode: 714, duration: 1.850s, episode steps: 166, steps per second: 90, episode reward: 166.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.189 [-2.737, 3.183], loss: 27.819689, mean_squared_error: 25872.927734, mean_q: 201.488876\n",
      "714 {'episode_reward': 168.0, 'nb_episode_steps': 168, 'nb_steps': 47391}\n",
      " 47391/100000: episode: 715, duration: 1.881s, episode steps: 168, steps per second: 89, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.173 [-3.086, 3.493], loss: 41.485176, mean_squared_error: 21514.275391, mean_q: 181.230591\n",
      "715 {'episode_reward': 196.0, 'nb_episode_steps': 196, 'nb_steps': 47587}\n",
      " 47587/100000: episode: 716, duration: 2.171s, episode steps: 196, steps per second: 90, episode reward: 196.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.459 [0.000, 1.000], mean observation: -0.120 [-3.070, 3.603], loss: 33.132771, mean_squared_error: 17630.964844, mean_q: 163.347900\n",
      "716 {'episode_reward': 146.0, 'nb_episode_steps': 146, 'nb_steps': 47733}\n",
      " 47733/100000: episode: 717, duration: 1.622s, episode steps: 146, steps per second: 90, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.466 [0.000, 1.000], mean observation: -0.122 [-2.339, 2.677], loss: 38.717827, mean_squared_error: 14619.957031, mean_q: 146.898148\n",
      "717 {'episode_reward': 146.0, 'nb_episode_steps': 146, 'nb_steps': 47879}\n",
      " 47879/100000: episode: 718, duration: 1.601s, episode steps: 146, steps per second: 91, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.089 [-0.608, 1.527], loss: 23.913275, mean_squared_error: 13274.763672, mean_q: 140.073944\n",
      "718 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 48079}\n",
      " 48079/100000: episode: 719, duration: 2.206s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.070 [-0.703, 0.803], loss: 27.757563, mean_squared_error: 10823.829102, mean_q: 125.233246\n",
      "719 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 48279}\n",
      " 48279/100000: episode: 720, duration: 2.214s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.033 [-0.603, 0.596], loss: 19.835711, mean_squared_error: 9988.532227, mean_q: 120.564445\n",
      "720 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 48479}\n",
      " 48479/100000: episode: 721, duration: 2.221s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.665, 0.664], loss: 12.796852, mean_squared_error: 9283.380859, mean_q: 115.743370\n",
      "721 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 48679}\n",
      " 48679/100000: episode: 722, duration: 2.245s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.071 [-0.869, 0.584], loss: 13.434307, mean_squared_error: 8639.153320, mean_q: 110.664177\n",
      "722 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 48879}\n",
      " 48879/100000: episode: 723, duration: 2.210s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.569, 0.842], loss: 11.617602, mean_squared_error: 8449.487305, mean_q: 109.414719\n",
      "723 {'episode_reward': 115.0, 'nb_episode_steps': 115, 'nb_steps': 48994}\n",
      " 48994/100000: episode: 724, duration: 1.292s, episode steps: 115, steps per second: 89, episode reward: 115.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.091 [-0.603, 1.656], loss: 10.753699, mean_squared_error: 8489.097656, mean_q: 109.609428\n",
      "724 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 49194}\n",
      " 49194/100000: episode: 725, duration: 2.216s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-0.665, 0.738], loss: 15.661599, mean_squared_error: 8367.359375, mean_q: 109.297302\n",
      "725 {'episode_reward': 131.0, 'nb_episode_steps': 131, 'nb_steps': 49325}\n",
      " 49325/100000: episode: 726, duration: 1.477s, episode steps: 131, steps per second: 89, episode reward: 131.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.080 [-0.645, 1.759], loss: 17.784830, mean_squared_error: 8721.656250, mean_q: 110.993759\n",
      "726 {'episode_reward': 111.0, 'nb_episode_steps': 111, 'nb_steps': 49436}\n",
      " 49436/100000: episode: 727, duration: 1.260s, episode steps: 111, steps per second: 88, episode reward: 111.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.086 [-0.605, 1.679], loss: 24.480494, mean_squared_error: 8461.140625, mean_q: 110.643723\n",
      "727 {'episode_reward': 167.0, 'nb_episode_steps': 167, 'nb_steps': 49603}\n",
      " 49603/100000: episode: 728, duration: 1.861s, episode steps: 167, steps per second: 90, episode reward: 167.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.061 [-0.675, 1.754], loss: 12.204470, mean_squared_error: 8678.015625, mean_q: 111.979767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728 {'episode_reward': 78.0, 'nb_episode_steps': 78, 'nb_steps': 49681}\n",
      " 49681/100000: episode: 729, duration: 0.881s, episode steps: 78, steps per second: 89, episode reward: 78.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.106 [-0.782, 1.973], loss: 14.339030, mean_squared_error: 9625.545898, mean_q: 118.396049\n",
      "729 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 49881}\n",
      " 49881/100000: episode: 730, duration: 2.226s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.039 [-0.520, 0.701], loss: 16.850414, mean_squared_error: 11890.250977, mean_q: 133.113724\n",
      "730 {'episode_reward': 137.0, 'nb_episode_steps': 137, 'nb_steps': 50018}\n",
      " 50018/100000: episode: 731, duration: 1.535s, episode steps: 137, steps per second: 89, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.064 [-0.645, 1.717], loss: 30.014235, mean_squared_error: 13661.113281, mean_q: 143.698395\n",
      "731 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 50218}\n",
      " 50218/100000: episode: 732, duration: 2.239s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.059 [-0.717, 0.612], loss: 46.920334, mean_squared_error: 19794.695312, mean_q: 178.003571\n",
      "732 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 50418}\n",
      " 50418/100000: episode: 733, duration: 2.239s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.098 [-0.656, 0.532], loss: 150.101837, mean_squared_error: 39938.085938, mean_q: 257.462311\n",
      "733 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 50618}\n",
      " 50618/100000: episode: 734, duration: 2.193s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.216 [-1.210, 0.848], loss: 214.887405, mean_squared_error: 50086.949219, mean_q: 280.896057\n",
      "734 {'episode_reward': 55.0, 'nb_episode_steps': 55, 'nb_steps': 50673}\n",
      " 50673/100000: episode: 735, duration: 0.654s, episode steps: 55, steps per second: 84, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.436 [0.000, 1.000], mean observation: 0.081 [-1.410, 2.700], loss: 175.757156, mean_squared_error: 53039.699219, mean_q: 286.851349\n",
      "735 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 50873}\n",
      " 50873/100000: episode: 736, duration: 2.227s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.295 [-1.461, 0.749], loss: 333.031708, mean_squared_error: 50602.285156, mean_q: 278.358582\n",
      "736 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 51073}\n",
      " 51073/100000: episode: 737, duration: 2.215s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.271 [-1.455, 0.555], loss: 226.927856, mean_squared_error: 48874.628906, mean_q: 270.841125\n",
      "737 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 51273}\n",
      " 51273/100000: episode: 738, duration: 2.217s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.301 [-1.716, 0.598], loss: 279.585327, mean_squared_error: 49429.125000, mean_q: 274.581604\n",
      "738 {'episode_reward': 89.0, 'nb_episode_steps': 89, 'nb_steps': 51362}\n",
      " 51362/100000: episode: 739, duration: 0.974s, episode steps: 89, steps per second: 91, episode reward: 89.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.472 [0.000, 1.000], mean observation: 0.051 [-1.142, 2.140], loss: 293.446381, mean_squared_error: 49285.183594, mean_q: 275.090668\n",
      "739 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 51562}\n",
      " 51562/100000: episode: 740, duration: 2.262s, episode steps: 200, steps per second: 88, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.302 [-1.845, 0.694], loss: 274.529877, mean_squared_error: 46882.480469, mean_q: 267.275421\n",
      "740 {'episode_reward': 135.0, 'nb_episode_steps': 135, 'nb_steps': 51697}\n",
      " 51697/100000: episode: 741, duration: 1.497s, episode steps: 135, steps per second: 90, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.039 [-1.370, 2.534], loss: 301.002563, mean_squared_error: 44634.742188, mean_q: 260.468872\n",
      "741 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 51897}\n",
      " 51897/100000: episode: 742, duration: 2.229s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.381 [-2.059, 0.874], loss: 214.780838, mean_squared_error: 45597.738281, mean_q: 265.397705\n",
      "742 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 52097}\n",
      " 52097/100000: episode: 743, duration: 2.216s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.281 [-1.817, 0.793], loss: 243.248474, mean_squared_error: 44398.261719, mean_q: 262.511841\n",
      "743 {'episode_reward': 29.0, 'nb_episode_steps': 29, 'nb_steps': 52126}\n",
      " 52126/100000: episode: 744, duration: 0.345s, episode steps: 29, steps per second: 84, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: 0.106 [-1.341, 2.547], loss: 333.120087, mean_squared_error: 40596.574219, mean_q: 249.825699\n",
      "744 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 52326}\n",
      " 52326/100000: episode: 745, duration: 2.245s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.329 [-2.116, 0.545], loss: 232.902878, mean_squared_error: 40209.730469, mean_q: 250.142273\n",
      "745 {'episode_reward': 40.0, 'nb_episode_steps': 40, 'nb_steps': 52366}\n",
      " 52366/100000: episode: 746, duration: 0.457s, episode steps: 40, steps per second: 88, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.425 [0.000, 1.000], mean observation: 0.084 [-1.189, 2.318], loss: 256.964508, mean_squared_error: 40488.750000, mean_q: 253.339569\n",
      "746 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 52566}\n",
      " 52566/100000: episode: 747, duration: 2.238s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.245 [-1.785, 0.744], loss: 244.928574, mean_squared_error: 36237.769531, mean_q: 237.095154\n",
      "747 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 52766}\n",
      " 52766/100000: episode: 748, duration: 2.208s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.252 [-1.797, 0.734], loss: 208.912949, mean_squared_error: 34867.039062, mean_q: 232.735611\n",
      "748 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 52966}\n",
      " 52966/100000: episode: 749, duration: 2.218s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.282 [-1.906, 0.716], loss: 119.803261, mean_squared_error: 34209.835938, mean_q: 233.309769\n",
      "749 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 53166}\n",
      " 53166/100000: episode: 750, duration: 2.227s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.112 [-0.940, 0.396], loss: 119.962067, mean_squared_error: 34075.898438, mean_q: 232.920837\n",
      "750 {'episode_reward': 59.0, 'nb_episode_steps': 59, 'nb_steps': 53225}\n",
      " 53225/100000: episode: 751, duration: 0.656s, episode steps: 59, steps per second: 90, episode reward: 59.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.063 [-1.032, 2.124], loss: 262.234131, mean_squared_error: 34333.355469, mean_q: 233.412918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751 {'episode_reward': 36.0, 'nb_episode_steps': 36, 'nb_steps': 53261}\n",
      " 53261/100000: episode: 752, duration: 0.404s, episode steps: 36, steps per second: 89, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.099 [-1.152, 2.347], loss: 93.289078, mean_squared_error: 33048.488281, mean_q: 233.722839\n",
      "752 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 53461}\n",
      " 53461/100000: episode: 753, duration: 2.219s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.136 [-1.092, 0.484], loss: 107.714310, mean_squared_error: 34942.675781, mean_q: 237.817337\n",
      "753 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 53661}\n",
      " 53661/100000: episode: 754, duration: 2.235s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.359 [-2.244, 0.650], loss: 135.234116, mean_squared_error: 34411.066406, mean_q: 237.534805\n",
      "754 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 53861}\n",
      " 53861/100000: episode: 755, duration: 2.251s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.335 [-2.121, 0.612], loss: 116.622833, mean_squared_error: 36155.578125, mean_q: 244.945465\n",
      "755 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 54061}\n",
      " 54061/100000: episode: 756, duration: 2.219s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.143 [-1.125, 0.480], loss: 121.151756, mean_squared_error: 37569.351562, mean_q: 250.276245\n",
      "756 {'episode_reward': 186.0, 'nb_episode_steps': 186, 'nb_steps': 54247}\n",
      " 54247/100000: episode: 757, duration: 2.062s, episode steps: 186, steps per second: 90, episode reward: 186.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.044 [-0.995, 2.008], loss: 127.163559, mean_squared_error: 34335.199219, mean_q: 238.141144\n",
      "757 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 54447}\n",
      " 54447/100000: episode: 758, duration: 2.215s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.294 [-1.979, 0.516], loss: 107.964333, mean_squared_error: 33171.019531, mean_q: 234.446686\n",
      "758 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 54647}\n",
      " 54647/100000: episode: 759, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.332 [-2.092, 0.546], loss: 52.116016, mean_squared_error: 32964.261719, mean_q: 234.411911\n",
      "759 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 54847}\n",
      " 54847/100000: episode: 760, duration: 2.200s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.268 [-1.792, 0.575], loss: 57.482533, mean_squared_error: 32844.945312, mean_q: 234.657852\n",
      "760 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 55047}\n",
      " 55047/100000: episode: 761, duration: 2.248s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.151 [-1.231, 0.575], loss: 94.316444, mean_squared_error: 31669.087891, mean_q: 230.095505\n",
      "761 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 55247}\n",
      " 55247/100000: episode: 762, duration: 2.286s, episode steps: 200, steps per second: 87, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.198 [-1.501, 0.632], loss: 66.027351, mean_squared_error: 31060.732422, mean_q: 227.926483\n",
      "762 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 55447}\n",
      " 55447/100000: episode: 763, duration: 2.193s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.072 [-0.904, 0.534], loss: 56.539043, mean_squared_error: 28225.820312, mean_q: 215.919220\n",
      "763 {'episode_reward': 157.0, 'nb_episode_steps': 157, 'nb_steps': 55604}\n",
      " 55604/100000: episode: 764, duration: 1.731s, episode steps: 157, steps per second: 91, episode reward: 157.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.227 [-2.885, 3.095], loss: 64.583923, mean_squared_error: 24993.640625, mean_q: 203.279938\n",
      "764 {'episode_reward': 158.0, 'nb_episode_steps': 158, 'nb_steps': 55762}\n",
      " 55762/100000: episode: 765, duration: 1.749s, episode steps: 158, steps per second: 90, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.193 [-3.023, 3.419], loss: 60.101624, mean_squared_error: 21680.357422, mean_q: 187.637482\n",
      "765 {'episode_reward': 135.0, 'nb_episode_steps': 135, 'nb_steps': 55897}\n",
      " 55897/100000: episode: 766, duration: 1.551s, episode steps: 135, steps per second: 87, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.203 [-2.886, 3.183], loss: 36.911930, mean_squared_error: 18752.175781, mean_q: 172.579926\n",
      "766 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 56097}\n",
      " 56097/100000: episode: 767, duration: 2.211s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.640, 0.506], loss: 39.190434, mean_squared_error: 16451.203125, mean_q: 160.832718\n",
      "767 {'episode_reward': 159.0, 'nb_episode_steps': 159, 'nb_steps': 56256}\n",
      " 56256/100000: episode: 768, duration: 1.751s, episode steps: 159, steps per second: 91, episode reward: 159.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.453 [0.000, 1.000], mean observation: -0.112 [-2.856, 3.208], loss: 24.032616, mean_squared_error: 13823.669922, mean_q: 145.289017\n",
      "768 {'episode_reward': 141.0, 'nb_episode_steps': 141, 'nb_steps': 56397}\n",
      " 56397/100000: episode: 769, duration: 1.550s, episode steps: 141, steps per second: 91, episode reward: 141.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.109 [-2.814, 3.080], loss: 33.810867, mean_squared_error: 11720.879883, mean_q: 133.171951\n",
      "769 {'episode_reward': 158.0, 'nb_episode_steps': 158, 'nb_steps': 56555}\n",
      " 56555/100000: episode: 770, duration: 1.771s, episode steps: 158, steps per second: 89, episode reward: 158.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.081 [-3.056, 3.473], loss: 28.592411, mean_squared_error: 10234.647461, mean_q: 123.949257\n",
      "770 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 56755}\n",
      " 56755/100000: episode: 771, duration: 2.202s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-0.492, 0.358], loss: 15.095987, mean_squared_error: 8794.610352, mean_q: 114.632561\n",
      "771 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 56955}\n",
      " 56955/100000: episode: 772, duration: 2.206s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.038 [-0.553, 0.448], loss: 18.689886, mean_squared_error: 7392.475098, mean_q: 104.553558\n",
      "772 {'episode_reward': 168.0, 'nb_episode_steps': 168, 'nb_steps': 57123}\n",
      " 57123/100000: episode: 773, duration: 1.881s, episode steps: 168, steps per second: 89, episode reward: 168.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.063 [-2.657, 3.264], loss: 17.857872, mean_squared_error: 6348.062500, mean_q: 96.389839\n",
      "773 {'episode_reward': 83.0, 'nb_episode_steps': 83, 'nb_steps': 57206}\n",
      " 57206/100000: episode: 774, duration: 0.956s, episode steps: 83, steps per second: 87, episode reward: 83.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.422 [0.000, 1.000], mean observation: -0.081 [-2.498, 3.058], loss: 17.378273, mean_squared_error: 5369.646484, mean_q: 88.847107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "774 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 57406}\n",
      " 57406/100000: episode: 775, duration: 2.224s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.615, 0.533], loss: 13.625713, mean_squared_error: 4769.475098, mean_q: 83.868416\n",
      "775 {'episode_reward': 164.0, 'nb_episode_steps': 164, 'nb_steps': 57570}\n",
      " 57570/100000: episode: 776, duration: 1.846s, episode steps: 164, steps per second: 89, episode reward: 164.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.046 [-2.729, 3.480], loss: 11.253200, mean_squared_error: 4520.885742, mean_q: 81.425819\n",
      "776 {'episode_reward': 134.0, 'nb_episode_steps': 134, 'nb_steps': 57704}\n",
      " 57704/100000: episode: 777, duration: 1.607s, episode steps: 134, steps per second: 83, episode reward: 134.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.493 [0.000, 1.000], mean observation: 0.059 [-0.662, 1.363], loss: 8.995956, mean_squared_error: 4422.794922, mean_q: 80.475777\n",
      "777 {'episode_reward': 66.0, 'nb_episode_steps': 66, 'nb_steps': 57770}\n",
      " 57770/100000: episode: 778, duration: 0.754s, episode steps: 66, steps per second: 88, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: -0.075 [-2.718, 3.410], loss: 12.903108, mean_squared_error: 4107.304688, mean_q: 76.451378\n",
      "778 {'episode_reward': 100.0, 'nb_episode_steps': 100, 'nb_steps': 57870}\n",
      " 57870/100000: episode: 779, duration: 1.100s, episode steps: 100, steps per second: 91, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.072 [-0.676, 1.434], loss: 10.663330, mean_squared_error: 4125.318359, mean_q: 77.995834\n",
      "779 {'episode_reward': 155.0, 'nb_episode_steps': 155, 'nb_steps': 58025}\n",
      " 58025/100000: episode: 780, duration: 1.714s, episode steps: 155, steps per second: 90, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.048 [-2.534, 3.232], loss: 14.372732, mean_squared_error: 3949.813232, mean_q: 76.419899\n",
      "780 {'episode_reward': 113.0, 'nb_episode_steps': 113, 'nb_steps': 58138}\n",
      " 58138/100000: episode: 781, duration: 1.266s, episode steps: 113, steps per second: 89, episode reward: 113.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.061 [-0.731, 1.731], loss: 7.810648, mean_squared_error: 3495.746582, mean_q: 71.944740\n",
      "781 {'episode_reward': 124.0, 'nb_episode_steps': 124, 'nb_steps': 58262}\n",
      " 58262/100000: episode: 782, duration: 1.373s, episode steps: 124, steps per second: 90, episode reward: 124.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.051 [-2.742, 3.541], loss: 9.097019, mean_squared_error: 3419.456543, mean_q: 70.872696\n",
      "782 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 58462}\n",
      " 58462/100000: episode: 783, duration: 2.247s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.017 [-0.615, 0.569], loss: 10.253811, mean_squared_error: 3082.798340, mean_q: 67.054718\n",
      "783 {'episode_reward': 146.0, 'nb_episode_steps': 146, 'nb_steps': 58608}\n",
      " 58608/100000: episode: 784, duration: 1.644s, episode steps: 146, steps per second: 89, episode reward: 146.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.056 [-0.826, 1.846], loss: 8.476119, mean_squared_error: 3004.195801, mean_q: 65.885147\n",
      "784 {'episode_reward': 129.0, 'nb_episode_steps': 129, 'nb_steps': 58737}\n",
      " 58737/100000: episode: 785, duration: 1.439s, episode steps: 129, steps per second: 90, episode reward: 129.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.054 [-2.558, 3.488], loss: 8.343966, mean_squared_error: 3140.368408, mean_q: 67.959114\n",
      "785 {'episode_reward': 92.0, 'nb_episode_steps': 92, 'nb_steps': 58829}\n",
      " 58829/100000: episode: 786, duration: 1.026s, episode steps: 92, steps per second: 90, episode reward: 92.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.082 [-0.788, 1.890], loss: 9.006686, mean_squared_error: 3144.220215, mean_q: 67.415520\n",
      "786 {'episode_reward': 88.0, 'nb_episode_steps': 88, 'nb_steps': 58917}\n",
      " 58917/100000: episode: 787, duration: 1.006s, episode steps: 88, steps per second: 87, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.078 [-0.664, 1.447], loss: 11.887783, mean_squared_error: 3130.832275, mean_q: 67.439789\n",
      "787 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 59117}\n",
      " 59117/100000: episode: 788, duration: 2.220s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.008 [-0.581, 0.564], loss: 8.107213, mean_squared_error: 3302.107178, mean_q: 69.628983\n",
      "788 {'episode_reward': 86.0, 'nb_episode_steps': 86, 'nb_steps': 59203}\n",
      " 59203/100000: episode: 789, duration: 1.005s, episode steps: 86, steps per second: 86, episode reward: 86.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.074 [-0.668, 1.672], loss: 16.285313, mean_squared_error: 3733.829834, mean_q: 73.611458\n",
      "789 {'episode_reward': 99.0, 'nb_episode_steps': 99, 'nb_steps': 59302}\n",
      " 59302/100000: episode: 790, duration: 1.134s, episode steps: 99, steps per second: 87, episode reward: 99.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.064 [-0.638, 1.677], loss: 11.356594, mean_squared_error: 3517.796387, mean_q: 71.685768\n",
      "790 {'episode_reward': 82.0, 'nb_episode_steps': 82, 'nb_steps': 59384}\n",
      " 59384/100000: episode: 791, duration: 0.957s, episode steps: 82, steps per second: 86, episode reward: 82.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.080 [-0.819, 1.986], loss: 12.499065, mean_squared_error: 3705.976807, mean_q: 73.913940\n",
      "791 {'episode_reward': 121.0, 'nb_episode_steps': 121, 'nb_steps': 59505}\n",
      " 59505/100000: episode: 792, duration: 1.379s, episode steps: 121, steps per second: 88, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.058 [-0.663, 1.713], loss: 6.758781, mean_squared_error: 3866.668457, mean_q: 75.176682\n",
      "792 {'episode_reward': 165.0, 'nb_episode_steps': 165, 'nb_steps': 59670}\n",
      " 59670/100000: episode: 793, duration: 1.811s, episode steps: 165, steps per second: 91, episode reward: 165.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.049 [-0.623, 1.661], loss: 7.951468, mean_squared_error: 3900.748535, mean_q: 75.343903\n",
      "793 {'episode_reward': 155.0, 'nb_episode_steps': 155, 'nb_steps': 59825}\n",
      " 59825/100000: episode: 794, duration: 1.697s, episode steps: 155, steps per second: 91, episode reward: 155.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.042 [-0.663, 1.738], loss: 5.911032, mean_squared_error: 3836.641113, mean_q: 74.526794\n",
      "794 {'episode_reward': 172.0, 'nb_episode_steps': 172, 'nb_steps': 59997}\n",
      " 59997/100000: episode: 795, duration: 1.906s, episode steps: 172, steps per second: 90, episode reward: 172.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.046 [-0.763, 1.821], loss: 7.510174, mean_squared_error: 3890.254395, mean_q: 75.414177\n",
      "795 {'episode_reward': 140.0, 'nb_episode_steps': 140, 'nb_steps': 60137}\n",
      " 60137/100000: episode: 796, duration: 1.543s, episode steps: 140, steps per second: 91, episode reward: 140.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.054 [-0.849, 1.941], loss: 5.524942, mean_squared_error: 3887.553467, mean_q: 74.730721\n",
      "796 {'episode_reward': 193.0, 'nb_episode_steps': 193, 'nb_steps': 60330}\n",
      " 60330/100000: episode: 797, duration: 2.125s, episode steps: 193, steps per second: 91, episode reward: 193.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.043 [-0.583, 1.527], loss: 9.317275, mean_squared_error: 3757.358887, mean_q: 73.643822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "797 {'episode_reward': 197.0, 'nb_episode_steps': 197, 'nb_steps': 60527}\n",
      " 60527/100000: episode: 798, duration: 2.184s, episode steps: 197, steps per second: 90, episode reward: 197.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: 0.046 [-0.753, 1.585], loss: 6.394783, mean_squared_error: 3731.598877, mean_q: 73.340073\n",
      "798 {'episode_reward': 143.0, 'nb_episode_steps': 143, 'nb_steps': 60670}\n",
      " 60670/100000: episode: 799, duration: 1.609s, episode steps: 143, steps per second: 89, episode reward: 143.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.051 [-0.644, 1.631], loss: 9.784863, mean_squared_error: 4029.873291, mean_q: 75.910927\n",
      "799 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 60870}\n",
      " 60870/100000: episode: 800, duration: 2.224s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.028 [-0.585, 0.560], loss: 4.561215, mean_squared_error: 4260.664062, mean_q: 78.733467\n",
      "800 {'episode_reward': 121.0, 'nb_episode_steps': 121, 'nb_steps': 60991}\n",
      " 60991/100000: episode: 801, duration: 1.356s, episode steps: 121, steps per second: 89, episode reward: 121.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.065 [-0.586, 1.541], loss: 5.637341, mean_squared_error: 4111.839844, mean_q: 77.379097\n",
      "801 {'episode_reward': 120.0, 'nb_episode_steps': 120, 'nb_steps': 61111}\n",
      " 61111/100000: episode: 802, duration: 1.338s, episode steps: 120, steps per second: 90, episode reward: 120.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.060 [-0.814, 1.944], loss: 4.756561, mean_squared_error: 4171.086426, mean_q: 78.023323\n",
      "802 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 61311}\n",
      " 61311/100000: episode: 803, duration: 2.224s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-0.857, 0.574], loss: 6.189338, mean_squared_error: 4042.215576, mean_q: 76.817207\n",
      "803 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 61511}\n",
      " 61511/100000: episode: 804, duration: 2.296s, episode steps: 200, steps per second: 87, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.008 [-0.350, 0.588], loss: 5.790746, mean_squared_error: 3651.712402, mean_q: 72.756813\n",
      "804 {'episode_reward': 76.0, 'nb_episode_steps': 76, 'nb_steps': 61587}\n",
      " 61587/100000: episode: 805, duration: 0.831s, episode steps: 76, steps per second: 91, episode reward: 76.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.073 [-0.721, 1.496], loss: 4.213821, mean_squared_error: 3369.903076, mean_q: 70.144745\n",
      "805 {'episode_reward': 103.0, 'nb_episode_steps': 103, 'nb_steps': 61690}\n",
      " 61690/100000: episode: 806, duration: 1.157s, episode steps: 103, steps per second: 89, episode reward: 103.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.064 [-0.585, 1.481], loss: 6.687940, mean_squared_error: 3024.077881, mean_q: 66.409172\n",
      "806 {'episode_reward': 64.0, 'nb_episode_steps': 64, 'nb_steps': 61754}\n",
      " 61754/100000: episode: 807, duration: 0.729s, episode steps: 64, steps per second: 88, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.089 [-0.789, 1.934], loss: 6.711752, mean_squared_error: 2813.941406, mean_q: 64.338066\n",
      "807 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 61954}\n",
      " 61954/100000: episode: 808, duration: 2.225s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.057 [-1.204, 1.484], loss: 6.981567, mean_squared_error: 2774.298340, mean_q: 63.594242\n",
      "808 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 62154}\n",
      " 62154/100000: episode: 809, duration: 2.232s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.003 [-0.590, 0.860], loss: 7.081742, mean_squared_error: 3050.392578, mean_q: 66.546120\n",
      "809 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 62354}\n",
      " 62354/100000: episode: 810, duration: 2.221s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.589, 0.810], loss: 8.321837, mean_squared_error: 2877.905029, mean_q: 64.806503\n",
      "810 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 62554}\n",
      " 62554/100000: episode: 811, duration: 2.231s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.533, 0.651], loss: 5.821775, mean_squared_error: 2680.792480, mean_q: 62.568100\n",
      "811 {'episode_reward': 137.0, 'nb_episode_steps': 137, 'nb_steps': 62691}\n",
      " 62691/100000: episode: 812, duration: 1.545s, episode steps: 137, steps per second: 89, episode reward: 137.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.057 [-0.814, 1.675], loss: 5.486177, mean_squared_error: 2684.095703, mean_q: 62.585133\n",
      "812 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 62891}\n",
      " 62891/100000: episode: 813, duration: 2.197s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-0.399, 0.602], loss: 5.909320, mean_squared_error: 2385.951904, mean_q: 59.038925\n",
      "813 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 63091}\n",
      " 63091/100000: episode: 814, duration: 2.196s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-0.670, 0.708], loss: 6.611202, mean_squared_error: 2280.871826, mean_q: 57.600075\n",
      "814 {'episode_reward': 84.0, 'nb_episode_steps': 84, 'nb_steps': 63175}\n",
      " 63175/100000: episode: 815, duration: 0.963s, episode steps: 84, steps per second: 87, episode reward: 84.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.085 [-0.951, 1.968], loss: 4.242671, mean_squared_error: 2771.761475, mean_q: 63.875198\n",
      "815 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 63375}\n",
      " 63375/100000: episode: 816, duration: 2.231s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-1.121, 0.788], loss: 5.377648, mean_squared_error: 3098.560059, mean_q: 67.365456\n",
      "816 {'episode_reward': 175.0, 'nb_episode_steps': 175, 'nb_steps': 63550}\n",
      " 63550/100000: episode: 817, duration: 1.931s, episode steps: 175, steps per second: 91, episode reward: 175.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.041 [-0.668, 1.670], loss: 10.468193, mean_squared_error: 3011.457031, mean_q: 66.636673\n",
      "817 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 63750}\n",
      " 63750/100000: episode: 818, duration: 2.218s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.058 [-1.056, 1.290], loss: 4.450977, mean_squared_error: 2685.487305, mean_q: 62.709442\n",
      "818 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 63950}\n",
      " 63950/100000: episode: 819, duration: 2.213s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-1.030, 1.097], loss: 8.868610, mean_squared_error: 2639.409912, mean_q: 62.165024\n",
      "819 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 64150}\n",
      " 64150/100000: episode: 820, duration: 2.211s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.022 [-0.433, 0.778], loss: 6.698096, mean_squared_error: 2707.132568, mean_q: 63.212486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "820 {'episode_reward': 191.0, 'nb_episode_steps': 191, 'nb_steps': 64341}\n",
      " 64341/100000: episode: 821, duration: 2.125s, episode steps: 191, steps per second: 90, episode reward: 191.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.068 [-2.133, 3.209], loss: 7.804585, mean_squared_error: 2524.031006, mean_q: 60.984230\n",
      "821 {'episode_reward': 144.0, 'nb_episode_steps': 144, 'nb_steps': 64485}\n",
      " 64485/100000: episode: 822, duration: 1.831s, episode steps: 144, steps per second: 79, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: 0.053 [-0.768, 1.759], loss: 5.991957, mean_squared_error: 2249.558838, mean_q: 57.600300\n",
      "822 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 64685}\n",
      " 64685/100000: episode: 823, duration: 2.304s, episode steps: 200, steps per second: 87, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.023 [-0.548, 0.592], loss: 5.559332, mean_squared_error: 2275.667480, mean_q: 57.822121\n",
      "823 {'episode_reward': 136.0, 'nb_episode_steps': 136, 'nb_steps': 64821}\n",
      " 64821/100000: episode: 824, duration: 1.530s, episode steps: 136, steps per second: 89, episode reward: 136.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: 0.057 [-0.823, 1.910], loss: 6.442375, mean_squared_error: 2745.927734, mean_q: 63.394848\n",
      "824 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 65021}\n",
      " 65021/100000: episode: 825, duration: 2.235s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.037 [-0.586, 0.632], loss: 6.418736, mean_squared_error: 2938.892578, mean_q: 66.019783\n",
      "825 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 65221}\n",
      " 65221/100000: episode: 826, duration: 2.228s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.014 [-0.464, 0.653], loss: 4.827182, mean_squared_error: 3609.755615, mean_q: 73.039307\n",
      "826 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 65421}\n",
      " 65421/100000: episode: 827, duration: 2.270s, episode steps: 200, steps per second: 88, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-0.801, 1.066], loss: 8.703480, mean_squared_error: 3085.170654, mean_q: 67.463371\n",
      "827 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 65621}\n",
      " 65621/100000: episode: 828, duration: 2.256s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-0.548, 0.594], loss: 7.146142, mean_squared_error: 2449.743408, mean_q: 60.241974\n",
      "828 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 65821}\n",
      " 65821/100000: episode: 829, duration: 2.247s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-0.586, 0.588], loss: 7.225808, mean_squared_error: 2779.114990, mean_q: 64.731819\n",
      "829 {'episode_reward': 77.0, 'nb_episode_steps': 77, 'nb_steps': 65898}\n",
      " 65898/100000: episode: 830, duration: 0.934s, episode steps: 77, steps per second: 82, episode reward: 77.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.085 [-0.791, 1.782], loss: 6.938674, mean_squared_error: 2760.520996, mean_q: 64.325119\n",
      "830 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 66098}\n",
      " 66098/100000: episode: 831, duration: 2.229s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.044 [-0.913, 0.692], loss: 6.924879, mean_squared_error: 2627.192139, mean_q: 62.394897\n",
      "831 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 66298}\n",
      " 66298/100000: episode: 832, duration: 2.218s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-0.797, 0.607], loss: 5.212102, mean_squared_error: 2235.853516, mean_q: 57.671963\n",
      "832 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 66498}\n",
      " 66498/100000: episode: 833, duration: 2.195s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.851, 0.607], loss: 7.433780, mean_squared_error: 2654.800537, mean_q: 63.149017\n",
      "833 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 66698}\n",
      " 66698/100000: episode: 834, duration: 2.195s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.047 [-0.613, 0.562], loss: 4.751256, mean_squared_error: 2640.566162, mean_q: 62.799511\n",
      "834 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 66898}\n",
      " 66898/100000: episode: 835, duration: 2.294s, episode steps: 200, steps per second: 87, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.526, 0.617], loss: 3.812726, mean_squared_error: 2876.004395, mean_q: 65.309593\n",
      "835 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 67098}\n",
      " 67098/100000: episode: 836, duration: 2.255s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-0.607, 0.761], loss: 6.451753, mean_squared_error: 3114.437500, mean_q: 67.971832\n",
      "836 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 67298}\n",
      " 67298/100000: episode: 837, duration: 2.197s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.575, 0.591], loss: 4.763204, mean_squared_error: 2587.308838, mean_q: 61.838345\n",
      "837 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 67498}\n",
      " 67498/100000: episode: 838, duration: 2.225s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.546, 0.567], loss: 6.630124, mean_squared_error: 2603.821289, mean_q: 62.371181\n",
      "838 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 67698}\n",
      " 67698/100000: episode: 839, duration: 2.228s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-0.585, 0.810], loss: 7.058052, mean_squared_error: 2433.684570, mean_q: 60.412148\n",
      "839 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 67898}\n",
      " 67898/100000: episode: 840, duration: 2.291s, episode steps: 200, steps per second: 87, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.044 [-0.610, 0.758], loss: 7.405657, mean_squared_error: 2515.303223, mean_q: 61.735741\n",
      "840 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 68098}\n",
      " 68098/100000: episode: 841, duration: 2.200s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.032 [-0.566, 0.876], loss: 8.535987, mean_squared_error: 2587.811523, mean_q: 62.458691\n",
      "841 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 68298}\n",
      " 68298/100000: episode: 842, duration: 2.206s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-0.373, 0.557], loss: 4.510154, mean_squared_error: 2700.373047, mean_q: 63.640053\n",
      "842 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 68498}\n",
      " 68498/100000: episode: 843, duration: 2.218s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.592, 0.568], loss: 4.120375, mean_squared_error: 2934.991455, mean_q: 66.723015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "843 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 68698}\n",
      " 68698/100000: episode: 844, duration: 2.240s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.003 [-0.578, 0.850], loss: 5.792314, mean_squared_error: 3185.503174, mean_q: 69.348297\n",
      "844 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 68898}\n",
      " 68898/100000: episode: 845, duration: 2.180s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.553, 0.452], loss: 6.087642, mean_squared_error: 3077.166260, mean_q: 68.412666\n",
      "845 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 69098}\n",
      " 69098/100000: episode: 846, duration: 2.252s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.024 [-0.624, 0.555], loss: 8.337878, mean_squared_error: 3138.064453, mean_q: 69.142235\n",
      "846 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 69298}\n",
      " 69298/100000: episode: 847, duration: 2.171s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.571, 0.611], loss: 4.459670, mean_squared_error: 3006.693115, mean_q: 67.140221\n",
      "847 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 69498}\n",
      " 69498/100000: episode: 848, duration: 2.205s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.012 [-0.589, 0.559], loss: 8.644868, mean_squared_error: 3054.530029, mean_q: 67.948639\n",
      "848 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 69698}\n",
      " 69698/100000: episode: 849, duration: 2.232s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-0.661, 0.458], loss: 8.010040, mean_squared_error: 2901.573730, mean_q: 66.672371\n",
      "849 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 69898}\n",
      " 69898/100000: episode: 850, duration: 2.209s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.622, 0.822], loss: 6.458289, mean_squared_error: 2992.829102, mean_q: 67.365852\n",
      "850 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 70098}\n",
      " 70098/100000: episode: 851, duration: 2.170s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-0.624, 0.553], loss: 5.458434, mean_squared_error: 3143.367432, mean_q: 69.150108\n",
      "851 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 70298}\n",
      " 70298/100000: episode: 852, duration: 2.187s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.039 [-0.878, 0.592], loss: 7.742669, mean_squared_error: 2816.545654, mean_q: 65.692795\n",
      "852 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 70498}\n",
      " 70498/100000: episode: 853, duration: 2.196s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.614, 0.876], loss: 7.613824, mean_squared_error: 2727.839355, mean_q: 64.708939\n",
      "853 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 70698}\n",
      " 70698/100000: episode: 854, duration: 2.187s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.048 [-0.849, 1.017], loss: 6.584121, mean_squared_error: 2840.495361, mean_q: 66.273109\n",
      "854 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 70898}\n",
      " 70898/100000: episode: 855, duration: 2.193s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-0.633, 0.682], loss: 9.209026, mean_squared_error: 2787.245605, mean_q: 65.727631\n",
      "855 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 71098}\n",
      " 71098/100000: episode: 856, duration: 2.209s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.047 [-0.584, 0.556], loss: 4.642035, mean_squared_error: 2370.718506, mean_q: 60.467960\n",
      "856 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 71298}\n",
      " 71298/100000: episode: 857, duration: 2.232s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.570, 0.574], loss: 7.498686, mean_squared_error: 2632.126953, mean_q: 63.819775\n",
      "857 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 71498}\n",
      " 71498/100000: episode: 858, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.008 [-0.644, 0.852], loss: 8.622838, mean_squared_error: 2661.255615, mean_q: 64.276047\n",
      "858 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 71698}\n",
      " 71698/100000: episode: 859, duration: 2.188s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-0.557, 0.614], loss: 8.219437, mean_squared_error: 2569.951660, mean_q: 62.782993\n",
      "859 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 71898}\n",
      " 71898/100000: episode: 860, duration: 2.195s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.007 [-0.577, 0.644], loss: 9.969988, mean_squared_error: 2284.912354, mean_q: 59.553188\n",
      "860 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 72098}\n",
      " 72098/100000: episode: 861, duration: 2.228s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.586, 0.611], loss: 6.440817, mean_squared_error: 2299.772217, mean_q: 60.069511\n",
      "861 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 72298}\n",
      " 72298/100000: episode: 862, duration: 2.248s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.002 [-0.887, 0.620], loss: 6.149698, mean_squared_error: 2524.455322, mean_q: 62.670731\n",
      "862 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 72498}\n",
      " 72498/100000: episode: 863, duration: 2.237s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-0.856, 0.819], loss: 6.104320, mean_squared_error: 2720.153809, mean_q: 65.238831\n",
      "863 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 72698}\n",
      " 72698/100000: episode: 864, duration: 2.200s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.004 [-0.605, 0.597], loss: 9.463421, mean_squared_error: 2503.913818, mean_q: 62.774559\n",
      "864 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 72898}\n",
      " 72898/100000: episode: 865, duration: 2.198s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.024 [-0.596, 0.551], loss: 7.203849, mean_squared_error: 2419.549072, mean_q: 61.380405\n",
      "865 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 73098}\n",
      " 73098/100000: episode: 866, duration: 2.189s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.012 [-0.635, 0.525], loss: 7.296817, mean_squared_error: 2702.793213, mean_q: 64.969368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "866 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 73298}\n",
      " 73298/100000: episode: 867, duration: 2.208s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.356, 0.545], loss: 6.391223, mean_squared_error: 2751.778809, mean_q: 65.774261\n",
      "867 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 73498}\n",
      " 73498/100000: episode: 868, duration: 2.224s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.610, 0.587], loss: 11.060844, mean_squared_error: 2759.254395, mean_q: 65.422089\n",
      "868 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 73698}\n",
      " 73698/100000: episode: 869, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-0.595, 0.539], loss: 12.427451, mean_squared_error: 2598.329102, mean_q: 63.562691\n",
      "869 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 73898}\n",
      " 73898/100000: episode: 870, duration: 2.198s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.001 [-0.988, 1.237], loss: 5.358187, mean_squared_error: 2528.880859, mean_q: 63.027966\n",
      "870 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 74098}\n",
      " 74098/100000: episode: 871, duration: 2.205s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.013 [-0.553, 0.572], loss: 7.750244, mean_squared_error: 2894.181885, mean_q: 67.386795\n",
      "871 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 74298}\n",
      " 74298/100000: episode: 872, duration: 2.202s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.007 [-0.498, 0.601], loss: 9.762703, mean_squared_error: 2678.548828, mean_q: 64.859627\n",
      "872 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 74498}\n",
      " 74498/100000: episode: 873, duration: 2.223s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.017 [-0.828, 0.592], loss: 9.048657, mean_squared_error: 2748.834717, mean_q: 66.018814\n",
      "873 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 74698}\n",
      " 74698/100000: episode: 874, duration: 2.185s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.580, 0.574], loss: 9.484986, mean_squared_error: 2725.060059, mean_q: 65.809372\n",
      "874 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 74898}\n",
      " 74898/100000: episode: 875, duration: 2.216s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.000 [-0.547, 0.599], loss: 11.146980, mean_squared_error: 2774.206787, mean_q: 66.172470\n",
      "875 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 75098}\n",
      " 75098/100000: episode: 876, duration: 2.196s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-0.770, 0.727], loss: 9.510057, mean_squared_error: 3270.853027, mean_q: 72.071846\n",
      "876 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 75298}\n",
      " 75298/100000: episode: 877, duration: 2.241s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.010 [-0.799, 0.566], loss: 7.190269, mean_squared_error: 3587.979492, mean_q: 75.349602\n",
      "877 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 75498}\n",
      " 75498/100000: episode: 878, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.025 [-0.620, 0.625], loss: 9.742759, mean_squared_error: 3588.027100, mean_q: 75.146523\n",
      "878 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 75698}\n",
      " 75698/100000: episode: 879, duration: 2.195s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.008 [-0.642, 0.641], loss: 7.896695, mean_squared_error: 3360.491211, mean_q: 72.886482\n",
      "879 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 75898}\n",
      " 75898/100000: episode: 880, duration: 2.181s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-0.771, 0.582], loss: 9.217498, mean_squared_error: 3253.260986, mean_q: 72.010956\n",
      "880 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 76098}\n",
      " 76098/100000: episode: 881, duration: 2.218s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.540, 0.472], loss: 9.977653, mean_squared_error: 3029.464111, mean_q: 69.569778\n",
      "881 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 76298}\n",
      " 76298/100000: episode: 882, duration: 2.232s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.016 [-0.849, 0.590], loss: 9.725191, mean_squared_error: 2875.261230, mean_q: 67.498055\n",
      "882 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 76498}\n",
      " 76498/100000: episode: 883, duration: 2.210s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.607, 0.577], loss: 5.219851, mean_squared_error: 2862.744629, mean_q: 67.097511\n",
      "883 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 76698}\n",
      " 76698/100000: episode: 884, duration: 2.223s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.566, 0.622], loss: 6.357540, mean_squared_error: 2938.340088, mean_q: 68.225876\n",
      "884 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 76898}\n",
      " 76898/100000: episode: 885, duration: 2.217s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.021 [-0.924, 0.733], loss: 10.609422, mean_squared_error: 2957.891846, mean_q: 68.425423\n",
      "885 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 77098}\n",
      " 77098/100000: episode: 886, duration: 2.168s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.002 [-0.502, 0.640], loss: 7.602304, mean_squared_error: 3387.720947, mean_q: 73.618942\n",
      "886 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 77298}\n",
      " 77298/100000: episode: 887, duration: 2.207s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.000 [-0.652, 0.572], loss: 9.067440, mean_squared_error: 3498.996338, mean_q: 74.614098\n",
      "887 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 77498}\n",
      " 77498/100000: episode: 888, duration: 2.210s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.533, 0.669], loss: 13.765742, mean_squared_error: 3354.307617, mean_q: 73.224342\n",
      "888 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 77698}\n",
      " 77698/100000: episode: 889, duration: 2.214s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.026 [-0.612, 0.576], loss: 12.145703, mean_squared_error: 3138.824463, mean_q: 70.941521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 77898}\n",
      " 77898/100000: episode: 890, duration: 2.199s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-0.594, 0.567], loss: 10.395517, mean_squared_error: 3061.744385, mean_q: 70.170158\n",
      "890 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 78098}\n",
      " 78098/100000: episode: 891, duration: 2.219s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.583, 0.824], loss: 12.592057, mean_squared_error: 3088.554443, mean_q: 70.590797\n",
      "891 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 78298}\n",
      " 78298/100000: episode: 892, duration: 2.199s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-0.608, 0.558], loss: 9.836349, mean_squared_error: 3039.171875, mean_q: 70.155739\n",
      "892 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 78498}\n",
      " 78498/100000: episode: 893, duration: 2.233s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.566, 0.646], loss: 8.388844, mean_squared_error: 2932.438477, mean_q: 68.657707\n",
      "893 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 78698}\n",
      " 78698/100000: episode: 894, duration: 2.179s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.552, 0.591], loss: 11.382432, mean_squared_error: 2947.882080, mean_q: 68.760536\n",
      "894 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 78898}\n",
      " 78898/100000: episode: 895, duration: 2.227s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.739, 0.672], loss: 6.865862, mean_squared_error: 3068.016357, mean_q: 70.416435\n",
      "895 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 79098}\n",
      " 79098/100000: episode: 896, duration: 2.182s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.010 [-0.590, 0.556], loss: 13.390461, mean_squared_error: 3433.153809, mean_q: 74.330452\n",
      "896 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 79298}\n",
      " 79298/100000: episode: 897, duration: 2.289s, episode steps: 200, steps per second: 87, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.555, 0.574], loss: 8.544428, mean_squared_error: 3627.147705, mean_q: 76.445580\n",
      "897 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 79498}\n",
      " 79498/100000: episode: 898, duration: 2.583s, episode steps: 200, steps per second: 77, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.009 [-0.594, 0.394], loss: 12.669355, mean_squared_error: 3631.685547, mean_q: 76.835968\n",
      "898 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 79698}\n",
      " 79698/100000: episode: 899, duration: 2.197s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.539, 0.561], loss: 9.480472, mean_squared_error: 3604.797852, mean_q: 76.178108\n",
      "899 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 79898}\n",
      " 79898/100000: episode: 900, duration: 2.188s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.936, 1.140], loss: 11.670815, mean_squared_error: 3388.287109, mean_q: 73.481384\n",
      "900 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 80098}\n",
      " 80098/100000: episode: 901, duration: 2.218s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.019 [-0.807, 0.813], loss: 13.057142, mean_squared_error: 3324.464111, mean_q: 72.870377\n",
      "901 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 80298}\n",
      " 80298/100000: episode: 902, duration: 2.208s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.012 [-0.551, 0.566], loss: 10.629421, mean_squared_error: 3161.885986, mean_q: 71.137878\n",
      "902 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 80498}\n",
      " 80498/100000: episode: 903, duration: 2.199s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.591, 0.590], loss: 12.954725, mean_squared_error: 3242.989990, mean_q: 72.644020\n",
      "903 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 80698}\n",
      " 80698/100000: episode: 904, duration: 2.198s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.017 [-0.659, 0.832], loss: 11.641958, mean_squared_error: 3465.666260, mean_q: 75.359436\n",
      "904 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 80898}\n",
      " 80898/100000: episode: 905, duration: 2.192s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.564, 0.613], loss: 6.342514, mean_squared_error: 3816.575684, mean_q: 78.549522\n",
      "905 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 81098}\n",
      " 81098/100000: episode: 906, duration: 2.211s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.006 [-0.832, 0.598], loss: 13.234275, mean_squared_error: 4402.486328, mean_q: 84.689919\n",
      "906 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 81298}\n",
      " 81298/100000: episode: 907, duration: 2.215s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.008 [-0.623, 0.564], loss: 11.294953, mean_squared_error: 4290.167969, mean_q: 83.745461\n",
      "907 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 81498}\n",
      " 81498/100000: episode: 908, duration: 2.219s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.032 [-0.611, 0.610], loss: 10.658211, mean_squared_error: 3846.155273, mean_q: 78.789040\n",
      "908 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 81698}\n",
      " 81698/100000: episode: 909, duration: 2.203s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.570, 0.561], loss: 11.585684, mean_squared_error: 3585.595947, mean_q: 76.489899\n",
      "909 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 81898}\n",
      " 81898/100000: episode: 910, duration: 2.200s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.600, 0.816], loss: 9.472004, mean_squared_error: 3884.121582, mean_q: 79.661018\n",
      "910 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 82098}\n",
      " 82098/100000: episode: 911, duration: 2.223s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.002 [-0.556, 0.598], loss: 14.234933, mean_squared_error: 3708.239990, mean_q: 77.958908\n",
      "911 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 82298}\n",
      " 82298/100000: episode: 912, duration: 2.235s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.016 [-0.583, 0.582], loss: 11.294853, mean_squared_error: 3901.929443, mean_q: 79.742622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "912 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 82498}\n",
      " 82498/100000: episode: 913, duration: 2.193s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.867, 0.565], loss: 13.235625, mean_squared_error: 3921.357422, mean_q: 80.759865\n",
      "913 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 82698}\n",
      " 82698/100000: episode: 914, duration: 2.218s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.829, 0.585], loss: 9.876902, mean_squared_error: 3757.825195, mean_q: 78.867668\n",
      "914 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 82898}\n",
      " 82898/100000: episode: 915, duration: 2.232s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.002 [-0.623, 0.573], loss: 16.792414, mean_squared_error: 3364.416260, mean_q: 74.603409\n",
      "915 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 83098}\n",
      " 83098/100000: episode: 916, duration: 2.264s, episode steps: 200, steps per second: 88, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.026 [-0.688, 0.598], loss: 9.195868, mean_squared_error: 3237.575928, mean_q: 73.109169\n",
      "916 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 83298}\n",
      " 83298/100000: episode: 917, duration: 2.193s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.466, 0.626], loss: 8.052596, mean_squared_error: 3702.205078, mean_q: 78.334702\n",
      "917 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 83498}\n",
      " 83498/100000: episode: 918, duration: 2.179s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-0.743, 0.925], loss: 14.865702, mean_squared_error: 3855.745117, mean_q: 80.058777\n",
      "918 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 83698}\n",
      " 83698/100000: episode: 919, duration: 2.205s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.664, 0.556], loss: 11.661516, mean_squared_error: 3713.256836, mean_q: 78.715706\n",
      "919 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 83898}\n",
      " 83898/100000: episode: 920, duration: 2.202s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.015 [-0.581, 0.606], loss: 10.337257, mean_squared_error: 3482.197754, mean_q: 76.032959\n",
      "920 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 84098}\n",
      " 84098/100000: episode: 921, duration: 2.206s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.572, 0.614], loss: 13.473140, mean_squared_error: 3216.720703, mean_q: 73.157120\n",
      "921 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 84298}\n",
      " 84298/100000: episode: 922, duration: 2.184s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.010 [-0.526, 0.630], loss: 11.223278, mean_squared_error: 3274.149658, mean_q: 73.960991\n",
      "922 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 84498}\n",
      " 84498/100000: episode: 923, duration: 2.235s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.016 [-0.610, 0.559], loss: 13.516384, mean_squared_error: 3487.956787, mean_q: 76.758026\n",
      "923 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 84698}\n",
      " 84698/100000: episode: 924, duration: 2.198s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.029 [-0.598, 0.556], loss: 10.099905, mean_squared_error: 3322.179932, mean_q: 74.801537\n",
      "924 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 84898}\n",
      " 84898/100000: episode: 925, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.641, 0.582], loss: 12.085353, mean_squared_error: 3416.607178, mean_q: 75.534271\n",
      "925 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 85098}\n",
      " 85098/100000: episode: 926, duration: 2.185s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.012 [-0.507, 0.605], loss: 13.077345, mean_squared_error: 3901.406982, mean_q: 81.342064\n",
      "926 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 85298}\n",
      " 85298/100000: episode: 927, duration: 2.229s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.591, 0.603], loss: 14.292712, mean_squared_error: 4279.604492, mean_q: 85.091873\n",
      "927 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 85498}\n",
      " 85498/100000: episode: 928, duration: 2.193s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.002 [-0.585, 0.596], loss: 11.180957, mean_squared_error: 4324.259277, mean_q: 85.317284\n",
      "928 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 85698}\n",
      " 85698/100000: episode: 929, duration: 2.211s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.662, 0.578], loss: 12.270093, mean_squared_error: 3955.664307, mean_q: 81.920135\n",
      "929 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 85898}\n",
      " 85898/100000: episode: 930, duration: 2.206s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.011 [-0.544, 0.634], loss: 14.925054, mean_squared_error: 4012.746338, mean_q: 82.400658\n",
      "930 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 86098}\n",
      " 86098/100000: episode: 931, duration: 2.228s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-0.702, 0.930], loss: 9.478354, mean_squared_error: 3829.010742, mean_q: 80.395065\n",
      "931 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 86298}\n",
      " 86298/100000: episode: 932, duration: 2.227s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.025 [-0.706, 0.568], loss: 12.363219, mean_squared_error: 3691.779785, mean_q: 79.037643\n",
      "932 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 86498}\n",
      " 86498/100000: episode: 933, duration: 2.238s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.612, 0.609], loss: 13.021097, mean_squared_error: 3863.062256, mean_q: 80.929970\n",
      "933 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 86698}\n",
      " 86698/100000: episode: 934, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.886, 0.728], loss: 12.032795, mean_squared_error: 4233.229492, mean_q: 85.123627\n",
      "934 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 86898}\n",
      " 86898/100000: episode: 935, duration: 2.216s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.013 [-0.887, 0.762], loss: 11.214882, mean_squared_error: 4032.660889, mean_q: 82.269913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 87098}\n",
      " 87098/100000: episode: 936, duration: 2.219s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.002 [-1.085, 1.115], loss: 12.160047, mean_squared_error: 3776.495117, mean_q: 79.858772\n",
      "936 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 87298}\n",
      " 87298/100000: episode: 937, duration: 2.193s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: 0.035 [-0.576, 0.582], loss: 13.695364, mean_squared_error: 3949.820557, mean_q: 82.009590\n",
      "937 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 87498}\n",
      " 87498/100000: episode: 938, duration: 2.183s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-0.568, 0.626], loss: 15.925628, mean_squared_error: 3878.844727, mean_q: 81.623535\n",
      "938 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 87698}\n",
      " 87698/100000: episode: 939, duration: 2.228s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-0.573, 0.651], loss: 8.934803, mean_squared_error: 3645.770020, mean_q: 78.660492\n",
      "939 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 87898}\n",
      " 87898/100000: episode: 940, duration: 2.185s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.783, 0.576], loss: 9.320798, mean_squared_error: 4257.048340, mean_q: 85.194778\n",
      "940 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 88098}\n",
      " 88098/100000: episode: 941, duration: 2.189s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.600, 0.622], loss: 7.242508, mean_squared_error: 4731.497070, mean_q: 89.517334\n",
      "941 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 88298}\n",
      " 88298/100000: episode: 942, duration: 2.189s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.019 [-0.417, 0.615], loss: 20.128670, mean_squared_error: 4372.448242, mean_q: 85.939278\n",
      "942 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 88498}\n",
      " 88498/100000: episode: 943, duration: 2.189s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.037 [-0.618, 0.743], loss: 11.038440, mean_squared_error: 3885.486328, mean_q: 81.522713\n",
      "943 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 88698}\n",
      " 88698/100000: episode: 944, duration: 2.194s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.007 [-0.913, 0.768], loss: 12.712611, mean_squared_error: 4027.810303, mean_q: 82.617615\n",
      "944 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 88898}\n",
      " 88898/100000: episode: 945, duration: 2.225s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.022 [-0.597, 0.823], loss: 12.214548, mean_squared_error: 4049.339355, mean_q: 83.363731\n",
      "945 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 89098}\n",
      " 89098/100000: episode: 946, duration: 2.249s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.002 [-0.517, 0.636], loss: 13.350945, mean_squared_error: 3923.678711, mean_q: 81.509079\n",
      "946 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 89298}\n",
      " 89298/100000: episode: 947, duration: 2.208s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.798, 0.583], loss: 13.104115, mean_squared_error: 3769.364746, mean_q: 79.840836\n",
      "947 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 89498}\n",
      " 89498/100000: episode: 948, duration: 2.201s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.003 [-0.610, 0.611], loss: 14.339375, mean_squared_error: 4013.600586, mean_q: 82.851151\n",
      "948 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 89698}\n",
      " 89698/100000: episode: 949, duration: 2.205s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.541, 0.604], loss: 16.532488, mean_squared_error: 4212.418945, mean_q: 85.021935\n",
      "949 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 89898}\n",
      " 89898/100000: episode: 950, duration: 2.244s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.791, 0.641], loss: 13.273555, mean_squared_error: 4066.209473, mean_q: 83.642815\n",
      "950 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 90098}\n",
      " 90098/100000: episode: 951, duration: 2.190s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-0.587, 0.617], loss: 11.866922, mean_squared_error: 3842.457275, mean_q: 81.171242\n",
      "951 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 90298}\n",
      " 90298/100000: episode: 952, duration: 2.220s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.014 [-0.537, 0.606], loss: 15.200573, mean_squared_error: 4027.520020, mean_q: 83.259491\n",
      "952 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 90498}\n",
      " 90498/100000: episode: 953, duration: 2.187s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.544, 0.652], loss: 13.412924, mean_squared_error: 4078.583496, mean_q: 83.638947\n",
      "953 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 90698}\n",
      " 90698/100000: episode: 954, duration: 2.181s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.006 [-0.595, 0.548], loss: 17.244068, mean_squared_error: 3986.515625, mean_q: 83.090302\n",
      "954 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 90898}\n",
      " 90898/100000: episode: 955, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.015 [-1.099, 0.801], loss: 12.456117, mean_squared_error: 3544.883057, mean_q: 78.382698\n",
      "955 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 91098}\n",
      " 91098/100000: episode: 956, duration: 2.209s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.028 [-0.606, 0.569], loss: 7.095294, mean_squared_error: 3796.680908, mean_q: 80.895760\n",
      "956 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 91298}\n",
      " 91298/100000: episode: 957, duration: 2.196s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.434, 0.636], loss: 12.731416, mean_squared_error: 4231.470703, mean_q: 85.589531\n",
      "957 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 91498}\n",
      " 91498/100000: episode: 958, duration: 2.177s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.005 [-0.876, 0.600], loss: 17.744070, mean_squared_error: 4238.000000, mean_q: 85.212814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "958 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 91698}\n",
      " 91698/100000: episode: 959, duration: 2.192s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.015 [-0.808, 0.808], loss: 13.323051, mean_squared_error: 4309.617676, mean_q: 85.802597\n",
      "959 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 91898}\n",
      " 91898/100000: episode: 960, duration: 2.221s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.804, 0.638], loss: 11.999502, mean_squared_error: 4046.096924, mean_q: 83.078148\n",
      "960 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 92098}\n",
      " 92098/100000: episode: 961, duration: 2.245s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.030 [-0.508, 0.635], loss: 10.797766, mean_squared_error: 4144.831055, mean_q: 84.411942\n",
      "961 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 92298}\n",
      " 92298/100000: episode: 962, duration: 2.193s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.854, 0.644], loss: 15.401668, mean_squared_error: 3968.455566, mean_q: 82.409042\n",
      "962 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 92498}\n",
      " 92498/100000: episode: 963, duration: 2.207s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.529, 0.627], loss: 10.911549, mean_squared_error: 3686.198975, mean_q: 79.588272\n",
      "963 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 92698}\n",
      " 92698/100000: episode: 964, duration: 2.209s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.566, 0.599], loss: 12.407269, mean_squared_error: 3894.064453, mean_q: 81.831108\n",
      "964 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 92898}\n",
      " 92898/100000: episode: 965, duration: 2.208s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.017 [-0.705, 0.644], loss: 17.633615, mean_squared_error: 3847.611816, mean_q: 81.405174\n",
      "965 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 93098}\n",
      " 93098/100000: episode: 966, duration: 2.251s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.001 [-0.686, 0.585], loss: 15.017055, mean_squared_error: 3560.735596, mean_q: 78.389687\n",
      "966 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 93298}\n",
      " 93298/100000: episode: 967, duration: 2.215s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: 0.022 [-0.688, 0.636], loss: 14.129077, mean_squared_error: 3607.028809, mean_q: 78.940804\n",
      "967 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 93498}\n",
      " 93498/100000: episode: 968, duration: 2.224s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.018 [-0.548, 0.390], loss: 15.781436, mean_squared_error: 4012.314941, mean_q: 83.113731\n",
      "968 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 93698}\n",
      " 93698/100000: episode: 969, duration: 2.207s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.004 [-0.590, 0.547], loss: 11.674047, mean_squared_error: 3863.521484, mean_q: 81.804131\n",
      "969 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 93898}\n",
      " 93898/100000: episode: 970, duration: 2.262s, episode steps: 200, steps per second: 88, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.011 [-0.578, 0.421], loss: 11.379258, mean_squared_error: 4259.035156, mean_q: 85.443817\n",
      "970 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 94098}\n",
      " 94098/100000: episode: 971, duration: 2.249s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.029 [-0.571, 0.579], loss: 20.251093, mean_squared_error: 4106.487793, mean_q: 84.187302\n",
      "971 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 94298}\n",
      " 94298/100000: episode: 972, duration: 2.194s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.000 [-1.084, 0.832], loss: 19.565865, mean_squared_error: 3943.673828, mean_q: 83.232735\n",
      "972 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 94498}\n",
      " 94498/100000: episode: 973, duration: 2.227s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.031 [-0.571, 0.607], loss: 13.107749, mean_squared_error: 4436.933105, mean_q: 87.790634\n",
      "973 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 94698}\n",
      " 94698/100000: episode: 974, duration: 2.178s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.020 [-0.562, 0.573], loss: 17.009493, mean_squared_error: 5051.521973, mean_q: 94.228226\n",
      "974 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 94898}\n",
      " 94898/100000: episode: 975, duration: 2.239s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.009 [-0.321, 0.343], loss: 21.208229, mean_squared_error: 5495.319336, mean_q: 98.371155\n",
      "975 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 95098}\n",
      " 95098/100000: episode: 976, duration: 2.223s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.003 [-0.618, 0.839], loss: 21.840830, mean_squared_error: 6250.230469, mean_q: 105.804642\n",
      "976 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 95298}\n",
      " 95298/100000: episode: 977, duration: 2.229s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.005 [-0.618, 0.544], loss: 28.020042, mean_squared_error: 6395.476074, mean_q: 106.939827\n",
      "977 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 95498}\n",
      " 95498/100000: episode: 978, duration: 2.177s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-0.597, 0.578], loss: 20.682867, mean_squared_error: 6755.893555, mean_q: 110.809296\n",
      "978 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 95698}\n",
      " 95698/100000: episode: 979, duration: 2.186s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.020 [-0.410, 0.577], loss: 31.933315, mean_squared_error: 8147.487305, mean_q: 122.079300\n",
      "979 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 95898}\n",
      " 95898/100000: episode: 980, duration: 2.219s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.060 [-0.841, 0.526], loss: 81.157249, mean_squared_error: 17035.099609, mean_q: 178.641678\n",
      "980 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 96098}\n",
      " 96098/100000: episode: 981, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.162 [-0.912, 0.663], loss: 324.642822, mean_squared_error: 44021.371094, mean_q: 291.156403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "981 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 96298}\n",
      " 96298/100000: episode: 982, duration: 2.197s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.495 [0.000, 1.000], mean observation: -0.161 [-0.843, 0.404], loss: 268.513245, mean_squared_error: 56967.738281, mean_q: 327.607330\n",
      "982 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 96498}\n",
      " 96498/100000: episode: 983, duration: 2.221s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.221 [-1.066, 0.466], loss: 381.379791, mean_squared_error: 60434.941406, mean_q: 335.065552\n",
      "983 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 96698}\n",
      " 96698/100000: episode: 984, duration: 2.188s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.241 [-1.052, 0.415], loss: 255.969650, mean_squared_error: 60376.609375, mean_q: 332.544922\n",
      "984 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 96898}\n",
      " 96898/100000: episode: 985, duration: 2.202s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.269 [-1.327, 0.577], loss: 393.467102, mean_squared_error: 59047.718750, mean_q: 327.276978\n",
      "985 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 97098}\n",
      " 97098/100000: episode: 986, duration: 2.196s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.216 [-1.150, 0.657], loss: 225.323792, mean_squared_error: 55598.453125, mean_q: 316.046997\n",
      "986 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 97298}\n",
      " 97298/100000: episode: 987, duration: 2.242s, episode steps: 200, steps per second: 89, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.243 [-1.292, 0.501], loss: 279.074646, mean_squared_error: 51501.566406, mean_q: 301.657410\n",
      "987 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 97498}\n",
      " 97498/100000: episode: 988, duration: 2.214s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.289 [-1.543, 0.755], loss: 272.717896, mean_squared_error: 49273.945312, mean_q: 292.974915\n",
      "988 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 97698}\n",
      " 97698/100000: episode: 989, duration: 2.195s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.204 [-1.386, 0.705], loss: 247.866165, mean_squared_error: 46453.769531, mean_q: 282.691650\n",
      "989 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 97898}\n",
      " 97898/100000: episode: 990, duration: 2.198s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.327 [-1.554, 0.604], loss: 310.559326, mean_squared_error: 45535.230469, mean_q: 280.261597\n",
      "990 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 98098}\n",
      " 98098/100000: episode: 991, duration: 2.230s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.312 [-1.674, 0.575], loss: 248.396698, mean_squared_error: 43250.996094, mean_q: 273.729187\n",
      "991 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 98298}\n",
      " 98298/100000: episode: 992, duration: 2.267s, episode steps: 200, steps per second: 88, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.285 [-1.662, 0.611], loss: 348.274139, mean_squared_error: 44742.019531, mean_q: 279.306549\n",
      "992 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 98498}\n",
      " 98498/100000: episode: 993, duration: 2.204s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.333 [-1.746, 0.865], loss: 318.717896, mean_squared_error: 41843.960938, mean_q: 271.536713\n",
      "993 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 98698}\n",
      " 98698/100000: episode: 994, duration: 2.220s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.316 [-1.801, 0.755], loss: 251.318588, mean_squared_error: 41211.761719, mean_q: 269.517120\n",
      "994 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 98898}\n",
      " 98898/100000: episode: 995, duration: 2.220s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.288 [-1.840, 0.592], loss: 250.155624, mean_squared_error: 39343.445312, mean_q: 263.086273\n",
      "995 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 99098}\n",
      " 99098/100000: episode: 996, duration: 2.228s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.316 [-1.821, 0.953], loss: 201.872574, mean_squared_error: 39886.574219, mean_q: 265.518158\n",
      "996 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 99298}\n",
      " 99298/100000: episode: 997, duration: 2.210s, episode steps: 200, steps per second: 90, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.069 [-1.463, 0.500], loss: 251.576080, mean_squared_error: 37001.328125, mean_q: 256.622498\n",
      "997 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 99498}\n",
      " 99498/100000: episode: 998, duration: 2.175s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.505 [0.000, 1.000], mean observation: -0.371 [-1.886, 0.622], loss: 247.634018, mean_squared_error: 35058.781250, mean_q: 249.642502\n",
      "998 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 99698}\n",
      " 99698/100000: episode: 999, duration: 2.196s, episode steps: 200, steps per second: 91, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.364 [-1.924, 0.685], loss: 180.493393, mean_squared_error: 34175.101562, mean_q: 247.819885\n",
      "999 {'episode_reward': 200.0, 'nb_episode_steps': 200, 'nb_steps': 99898}\n",
      " 99898/100000: episode: 1000, duration: 2.181s, episode steps: 200, steps per second: 92, episode reward: 200.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.325 [-1.810, 0.567], loss: 148.848129, mean_squared_error: 34446.039062, mean_q: 247.525162\n",
      "done, took 1092.091 seconds\n"
     ]
    }
   ],
   "source": [
    "hist = dqn.fit(env, nb_steps=100000, visualize=False, verbose=2,\n",
    "              callbacks=[LambdaCallback(on_epoch_end=lambda epoch, logs:print(epoch, logs))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards.extend(hist.history.get('episode_reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24eafd09ef0>]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n5Dki9IIgCHnO/wc7/aXgzCl82gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24eafc82978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 89.000, steps: 89\n",
      "Episode 3: reward: 119.000, steps: 119\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24ea45ffeb8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "notify_time": "5",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
