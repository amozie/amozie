{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.agents import *\n",
    "from rl.policy import *\n",
    "from rl.memory import *\n",
    "from rl.random import *\n",
    "import gym\n",
    "from gym import Env, Space, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(123)\n",
    "env.reset()\n",
    "assert len(env.action_space.shape)==1\n",
    "nb_actions = env.action_space.shape[0]\n",
    "window_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_x = Input((window_length, ) + env.observation_space.shape)\n",
    "model_y = Flatten()(model_x)\n",
    "model_y = Dense(16, activation='relu')(model_y)\n",
    "model_y = Dense(16, activation='relu')(model_y)\n",
    "model_y = Dense(16, activation='relu')(model_y)\n",
    "model_y = Dense(nb_actions)(model_y)\n",
    "actor = Model(model_x, model_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_x1 = Input((nb_actions,))\n",
    "model_x2 = Input((window_length,) + env.observation_space.shape)\n",
    "model_y2 = Flatten()(model_x2)\n",
    "model_y = Concatenate()([model_x1, model_y2])\n",
    "model_y = Dense(32, activation='relu')(model_y)\n",
    "model_y = Dense(32, activation='relu')(model_y)\n",
    "model_y = Dense(32, activation='relu')(model_y)\n",
    "model_y = Dense(1)(model_y)\n",
    "critic = Model([model_x1, model_x2], model_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=200000, window_length=window_length)\n",
    "random_process = OrnsteinUhlenbeckProcess(size=nb_actions, theta=.15)\n",
    "agent = DDPGAgent(nb_actions=nb_actions, actor=actor, critic=critic,\n",
    "                 critic_action_input=model_x1, memory=memory,\n",
    "                 nb_steps_warmup_critic=100, nb_steps_warmup_actor=100,\n",
    "                 random_process=random_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.compile(Adam(), metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "   200/100000: episode: 1, duration: 1.665s, episode steps: 200, steps per second: 120, episode reward: -9.033, mean reward: -0.045 [-0.200, -0.000], mean action: -0.101 [-1.414, 0.831], mean observation: -0.281 [-1.200, 0.028], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.032732\n",
      "   400/100000: episode: 2, duration: 2.941s, episode steps: 200, steps per second: 68, episode reward: -6.479, mean reward: -0.032 [-0.122, -0.000], mean action: -0.510 [-1.105, 0.073], mean observation: -0.318 [-0.882, 0.019], loss: 0.000001, mean_squared_error: 0.000003, mean_q: -0.034772\n",
      "   600/100000: episode: 3, duration: 3.024s, episode steps: 200, steps per second: 66, episode reward: -31.911, mean reward: -0.160 [-0.391, -0.000], mean action: 1.055 [-0.660, 1.977], mean observation: -0.114 [-0.645, 0.192], loss: 0.000012, mean_squared_error: 0.000025, mean_q: -0.038833\n",
      "   800/100000: episode: 4, duration: 3.115s, episode steps: 200, steps per second: 64, episode reward: -10.504, mean reward: -0.053 [-0.236, -0.000], mean action: -0.586 [-1.537, 0.113], mean observation: -0.343 [-1.122, 0.030], loss: 0.000009, mean_squared_error: 0.000017, mean_q: -0.047236\n",
      "  1000/100000: episode: 5, duration: 3.017s, episode steps: 200, steps per second: 66, episode reward: -20.202, mean reward: -0.101 [-0.329, -0.003], mean action: -0.951 [-1.814, -0.163], mean observation: -0.357 [-0.905, 0.013], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.048121\n",
      "  1200/100000: episode: 6, duration: 3.195s, episode steps: 200, steps per second: 63, episode reward: -11.522, mean reward: -0.058 [-0.227, -0.000], mean action: -0.406 [-1.505, 0.947], mean observation: -0.293 [-0.948, 0.028], loss: 0.000001, mean_squared_error: 0.000003, mean_q: -0.052345\n",
      "  1400/100000: episode: 7, duration: 3.046s, episode steps: 200, steps per second: 66, episode reward: -7.574, mean reward: -0.038 [-0.187, -0.000], mean action: -0.450 [-1.367, 0.391], mean observation: -0.311 [-0.784, 0.005], loss: 0.000001, mean_squared_error: 0.000003, mean_q: -0.050092\n",
      "  1600/100000: episode: 8, duration: 2.987s, episode steps: 200, steps per second: 67, episode reward: -4.761, mean reward: -0.024 [-0.103, -0.000], mean action: 0.375 [-0.334, 1.015], mean observation: -0.198 [-0.818, 0.102], loss: 0.000001, mean_squared_error: 0.000003, mean_q: -0.048001\n",
      "  1800/100000: episode: 9, duration: 3.594s, episode steps: 200, steps per second: 56, episode reward: -15.186, mean reward: -0.076 [-0.289, -0.000], mean action: -0.737 [-1.700, 0.189], mean observation: -0.334 [-0.954, 0.021], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.048097\n",
      "  2000/100000: episode: 10, duration: 3.054s, episode steps: 200, steps per second: 65, episode reward: -15.917, mean reward: -0.080 [-0.342, -0.000], mean action: -0.726 [-1.849, 0.283], mean observation: -0.330 [-0.822, 0.010], loss: 0.000002, mean_squared_error: 0.000003, mean_q: -0.046275\n",
      "  2200/100000: episode: 11, duration: 3.079s, episode steps: 200, steps per second: 65, episode reward: -4.265, mean reward: -0.021 [-0.077, -0.000], mean action: 0.370 [-0.526, 0.876], mean observation: -0.225 [-0.581, 0.012], loss: 0.000001, mean_squared_error: 0.000003, mean_q: -0.047499\n",
      "  2400/100000: episode: 12, duration: 3.057s, episode steps: 200, steps per second: 65, episode reward: -19.667, mean reward: -0.098 [-0.312, -0.000], mean action: 0.806 [-0.573, 1.765], mean observation: -0.171 [-0.688, 0.022], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.047103\n",
      "  2600/100000: episode: 13, duration: 3.268s, episode steps: 200, steps per second: 61, episode reward: -20.607, mean reward: -0.103 [-0.369, -0.000], mean action: 0.770 [-0.381, 1.921], mean observation: -0.188 [-0.557, 0.017], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.049217\n",
      "  2800/100000: episode: 14, duration: 3.535s, episode steps: 200, steps per second: 57, episode reward: -22.977, mean reward: -0.115 [-0.375, -0.000], mean action: -0.976 [-1.936, -0.010], mean observation: -0.365 [-1.158, 0.025], loss: 0.000001, mean_squared_error: 0.000003, mean_q: -0.052516\n",
      "  3000/100000: episode: 15, duration: 3.554s, episode steps: 200, steps per second: 56, episode reward: -5.309, mean reward: -0.027 [-0.196, -0.000], mean action: -0.281 [-1.398, 0.570], mean observation: -0.303 [-1.200, 0.025], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.054033\n",
      "  3200/100000: episode: 16, duration: 3.439s, episode steps: 200, steps per second: 58, episode reward: -15.590, mean reward: -0.078 [-0.481, -0.000], mean action: -0.602 [-2.193, 0.445], mean observation: -0.348 [-1.200, 0.024], loss: 0.000001, mean_squared_error: 0.000002, mean_q: -0.054777\n",
      "  3400/100000: episode: 17, duration: 3.464s, episode steps: 200, steps per second: 58, episode reward: -8.377, mean reward: -0.042 [-0.159, -0.000], mean action: -0.583 [-1.262, -0.005], mean observation: -0.331 [-0.901, 0.022], loss: 0.000005, mean_squared_error: 0.000010, mean_q: -0.052777\n",
      "  3600/100000: episode: 18, duration: 4.228s, episode steps: 200, steps per second: 47, episode reward: -25.309, mean reward: -0.127 [-0.343, -0.000], mean action: 0.990 [-0.114, 1.851], mean observation: -0.175 [-0.502, 0.015], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.051991\n",
      "  3800/100000: episode: 19, duration: 4.819s, episode steps: 200, steps per second: 42, episode reward: -4.578, mean reward: -0.023 [-0.225, -0.000], mean action: 0.280 [-0.360, 1.499], mean observation: -0.240 [-0.722, 0.027], loss: 0.000004, mean_squared_error: 0.000008, mean_q: -0.054256\n",
      "  4000/100000: episode: 20, duration: 3.613s, episode steps: 200, steps per second: 55, episode reward: -33.304, mean reward: -0.167 [-0.499, -0.000], mean action: -1.077 [-2.233, 0.172], mean observation: -0.341 [-0.774, 0.003], loss: 0.000002, mean_squared_error: 0.000003, mean_q: -0.054699\n",
      "  4200/100000: episode: 21, duration: 3.553s, episode steps: 200, steps per second: 56, episode reward: -8.430, mean reward: -0.042 [-0.197, -0.000], mean action: -0.427 [-1.405, 0.900], mean observation: -0.301 [-1.200, 0.039], loss: 0.000002, mean_squared_error: 0.000005, mean_q: -0.056688\n",
      "  4400/100000: episode: 22, duration: 3.467s, episode steps: 200, steps per second: 58, episode reward: -9.348, mean reward: -0.047 [-0.240, -0.000], mean action: 0.530 [-0.374, 1.548], mean observation: -0.213 [-0.595, 0.017], loss: 0.000003, mean_squared_error: 0.000006, mean_q: -0.057006\n",
      "  4600/100000: episode: 23, duration: 3.497s, episode steps: 200, steps per second: 57, episode reward: -6.653, mean reward: -0.033 [-0.253, -0.000], mean action: -0.436 [-1.591, 0.499], mean observation: -0.322 [-1.037, 0.022], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.056506\n",
      "  4800/100000: episode: 24, duration: 3.502s, episode steps: 200, steps per second: 57, episode reward: -35.449, mean reward: -0.177 [-0.650, -0.000], mean action: -1.095 [-2.549, 0.167], mean observation: -0.341 [-0.823, 0.006], loss: 0.000001, mean_squared_error: 0.000003, mean_q: -0.055189\n",
      "  5000/100000: episode: 25, duration: 3.489s, episode steps: 200, steps per second: 57, episode reward: -9.564, mean reward: -0.048 [-0.193, -0.000], mean action: 0.618 [-0.229, 1.390], mean observation: -0.190 [-0.541, 0.019], loss: 0.000002, mean_squared_error: 0.000003, mean_q: -0.059315\n",
      "  5200/100000: episode: 26, duration: 3.525s, episode steps: 200, steps per second: 57, episode reward: -45.174, mean reward: -0.226 [-0.935, -0.000], mean action: 1.203 [-0.186, 3.057], mean observation: -0.180 [-0.583, 0.011], loss: 0.000004, mean_squared_error: 0.000007, mean_q: -0.059669\n",
      "  5400/100000: episode: 27, duration: 3.460s, episode steps: 200, steps per second: 58, episode reward: -7.227, mean reward: -0.036 [-0.144, -0.000], mean action: -0.520 [-1.199, 0.145], mean observation: -0.314 [-0.817, 0.014], loss: 0.000013, mean_squared_error: 0.000025, mean_q: -0.063602\n",
      "  5600/100000: episode: 28, duration: 3.558s, episode steps: 200, steps per second: 56, episode reward: -24.463, mean reward: -0.122 [-0.677, -0.000], mean action: -0.481 [-2.601, 0.800], mean observation: -0.287 [-0.788, 0.006], loss: 0.000002, mean_squared_error: 0.000005, mean_q: -0.062295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5800/100000: episode: 29, duration: 3.529s, episode steps: 200, steps per second: 57, episode reward: -8.862, mean reward: -0.044 [-0.173, -0.000], mean action: -0.480 [-1.316, 0.449], mean observation: -0.326 [-1.200, 0.036], loss: 0.000006, mean_squared_error: 0.000012, mean_q: -0.064843\n",
      "  6000/100000: episode: 30, duration: 3.644s, episode steps: 200, steps per second: 55, episode reward: -18.847, mean reward: -0.094 [-0.259, -0.002], mean action: -0.900 [-1.609, -0.146], mean observation: -0.350 [-0.832, 0.009], loss: 0.000004, mean_squared_error: 0.000008, mean_q: -0.064517\n",
      "  6200/100000: episode: 31, duration: 3.536s, episode steps: 200, steps per second: 57, episode reward: -41.402, mean reward: -0.207 [-0.660, -0.000], mean action: 1.168 [-0.265, 2.569], mean observation: -0.186 [-0.626, 0.017], loss: 0.000007, mean_squared_error: 0.000014, mean_q: -0.065801\n",
      "  6400/100000: episode: 32, duration: 3.537s, episode steps: 200, steps per second: 57, episode reward: -16.255, mean reward: -0.081 [-0.346, -0.000], mean action: -0.802 [-1.860, -0.009], mean observation: -0.343 [-0.945, 0.016], loss: 0.000004, mean_squared_error: 0.000008, mean_q: -0.069770\n",
      "  6555/100000: episode: 33, duration: 2.771s, episode steps: 155, steps per second: 56, episode reward: 93.584, mean reward: 0.604 [-0.167, 99.912], mean action: 0.466 [-0.469, 1.293], mean observation: -0.189 [-0.852, 0.461], loss: 0.000002, mean_squared_error: 0.000004, mean_q: -0.069952\n",
      "  6755/100000: episode: 34, duration: 3.526s, episode steps: 200, steps per second: 57, episode reward: -4.585, mean reward: -0.023 [-0.142, -0.000], mean action: 0.279 [-0.633, 1.192], mean observation: -0.236 [-0.640, 0.014], loss: 0.781443, mean_squared_error: 1.562886, mean_q: -0.065965\n",
      "  6955/100000: episode: 35, duration: 3.490s, episode steps: 200, steps per second: 57, episode reward: -95.244, mean reward: -0.476 [-1.271, -0.004], mean action: -1.954 [-3.565, -0.191], mean observation: -0.358 [-0.837, 0.007], loss: 0.000045, mean_squared_error: 0.000089, mean_q: -0.066323\n",
      "  7155/100000: episode: 36, duration: 3.499s, episode steps: 200, steps per second: 57, episode reward: -17.101, mean reward: -0.086 [-0.333, -0.000], mean action: 0.806 [-0.351, 1.826], mean observation: -0.173 [-0.784, 0.023], loss: 1.560227, mean_squared_error: 3.120454, mean_q: -0.060435\n",
      "  7355/100000: episode: 37, duration: 3.547s, episode steps: 200, steps per second: 56, episode reward: -3.676, mean reward: -0.018 [-0.141, -0.000], mean action: 0.105 [-0.603, 1.187], mean observation: -0.273 [-1.077, 0.044], loss: 0.000008, mean_squared_error: 0.000016, mean_q: -0.064761\n",
      "  7555/100000: episode: 38, duration: 3.512s, episode steps: 200, steps per second: 57, episode reward: -3.443, mean reward: -0.017 [-0.082, -0.000], mean action: -0.305 [-0.907, 0.378], mean observation: -0.287 [-0.706, 0.013], loss: 0.780178, mean_squared_error: 1.560356, mean_q: -0.065302\n",
      "  7755/100000: episode: 39, duration: 3.547s, episode steps: 200, steps per second: 56, episode reward: -13.985, mean reward: -0.070 [-0.414, -0.000], mean action: -0.607 [-2.034, 0.238], mean observation: -0.318 [-0.887, 0.012], loss: 0.779985, mean_squared_error: 1.559971, mean_q: -0.057180\n",
      "  7955/100000: episode: 40, duration: 3.534s, episode steps: 200, steps per second: 57, episode reward: -26.374, mean reward: -0.132 [-0.432, -0.000], mean action: -0.968 [-2.077, 0.281], mean observation: -0.341 [-0.991, 0.017], loss: 0.000178, mean_squared_error: 0.000356, mean_q: -0.052635\n",
      "  8155/100000: episode: 41, duration: 3.488s, episode steps: 200, steps per second: 57, episode reward: -8.873, mean reward: -0.044 [-0.264, -0.000], mean action: -0.423 [-1.623, 0.537], mean observation: -0.301 [-0.969, 0.018], loss: 0.778380, mean_squared_error: 1.556760, mean_q: -0.053191\n",
      "  8355/100000: episode: 42, duration: 3.474s, episode steps: 200, steps per second: 58, episode reward: -4.621, mean reward: -0.023 [-0.133, -0.000], mean action: -0.357 [-1.155, 0.312], mean observation: -0.302 [-1.112, 0.037], loss: 0.000030, mean_squared_error: 0.000060, mean_q: -0.043606\n",
      "  8555/100000: episode: 43, duration: 3.528s, episode steps: 200, steps per second: 57, episode reward: -16.746, mean reward: -0.084 [-0.398, -0.000], mean action: -0.734 [-1.995, 0.197], mean observation: -0.362 [-1.158, 0.026], loss: 0.000015, mean_squared_error: 0.000029, mean_q: -0.043224\n",
      "  8755/100000: episode: 44, duration: 3.498s, episode steps: 200, steps per second: 57, episode reward: -26.981, mean reward: -0.135 [-0.371, -0.000], mean action: -1.033 [-1.926, -0.039], mean observation: -0.346 [-0.907, 0.018], loss: 1.553339, mean_squared_error: 3.106677, mean_q: -0.042491\n",
      "  8955/100000: episode: 45, duration: 3.480s, episode steps: 200, steps per second: 57, episode reward: -18.101, mean reward: -0.091 [-0.351, -0.000], mean action: 0.537 [-0.955, 1.872], mean observation: -0.216 [-0.768, 0.015], loss: 0.000087, mean_squared_error: 0.000175, mean_q: -0.042098\n",
      "  9155/100000: episode: 46, duration: 3.528s, episode steps: 200, steps per second: 57, episode reward: -7.272, mean reward: -0.036 [-0.216, -0.000], mean action: 0.364 [-0.772, 1.469], mean observation: -0.218 [-1.011, 0.035], loss: 0.775077, mean_squared_error: 1.550153, mean_q: -0.045413\n",
      "  9355/100000: episode: 47, duration: 3.530s, episode steps: 200, steps per second: 57, episode reward: -51.712, mean reward: -0.259 [-0.730, -0.000], mean action: 1.391 [-0.386, 2.702], mean observation: -0.163 [-0.620, 0.016], loss: 0.000109, mean_squared_error: 0.000217, mean_q: -0.039665\n",
      "  9555/100000: episode: 48, duration: 3.533s, episode steps: 200, steps per second: 57, episode reward: -4.279, mean reward: -0.021 [-0.153, -0.000], mean action: 0.128 [-0.581, 1.238], mean observation: -0.251 [-0.767, 0.022], loss: 0.000026, mean_squared_error: 0.000051, mean_q: -0.029890\n",
      "  9755/100000: episode: 49, duration: 3.518s, episode steps: 200, steps per second: 57, episode reward: -9.855, mean reward: -0.049 [-0.396, -0.000], mean action: 0.294 [-0.628, 1.989], mean observation: -0.226 [-0.722, 0.080], loss: 0.774162, mean_squared_error: 1.548324, mean_q: -0.019592\n",
      "  9955/100000: episode: 50, duration: 3.493s, episode steps: 200, steps per second: 57, episode reward: -26.932, mean reward: -0.135 [-0.411, -0.000], mean action: -0.780 [-2.028, 0.724], mean observation: -0.330 [-0.940, 0.014], loss: 0.772116, mean_squared_error: 1.544233, mean_q: -0.018080\n",
      " 10155/100000: episode: 51, duration: 3.486s, episode steps: 200, steps per second: 57, episode reward: -25.934, mean reward: -0.130 [-0.404, -0.000], mean action: -0.959 [-2.010, 0.181], mean observation: -0.344 [-0.864, 0.009], loss: 0.000132, mean_squared_error: 0.000264, mean_q: -0.014197\n",
      " 10355/100000: episode: 52, duration: 3.502s, episode steps: 200, steps per second: 57, episode reward: -41.551, mean reward: -0.208 [-0.642, -0.000], mean action: -1.222 [-2.533, 0.105], mean observation: -0.346 [-0.793, 0.005], loss: 0.000047, mean_squared_error: 0.000094, mean_q: -0.014389\n",
      " 10555/100000: episode: 53, duration: 3.520s, episode steps: 200, steps per second: 57, episode reward: -32.755, mean reward: -0.164 [-0.410, -0.000], mean action: 1.072 [-0.610, 2.024], mean observation: -0.156 [-0.596, 0.020], loss: 0.770045, mean_squared_error: 1.540090, mean_q: -0.009754\n",
      " 10755/100000: episode: 54, duration: 3.506s, episode steps: 200, steps per second: 57, episode reward: -14.157, mean reward: -0.071 [-0.451, -0.000], mean action: 0.569 [-0.501, 2.124], mean observation: -0.210 [-0.586, 0.013], loss: 0.000093, mean_squared_error: 0.000185, mean_q: -0.013106\n",
      " 10955/100000: episode: 55, duration: 3.491s, episode steps: 200, steps per second: 57, episode reward: -22.334, mean reward: -0.112 [-0.429, -0.000], mean action: 0.794 [-0.285, 2.071], mean observation: -0.198 [-0.631, 0.015], loss: 0.000043, mean_squared_error: 0.000086, mean_q: -0.009953\n",
      " 11155/100000: episode: 56, duration: 3.509s, episode steps: 200, steps per second: 57, episode reward: -10.751, mean reward: -0.054 [-0.170, -0.000], mean action: -0.657 [-1.303, 0.082], mean observation: -0.335 [-0.876, 0.018], loss: 0.769135, mean_squared_error: 1.538269, mean_q: -0.008123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11355/100000: episode: 57, duration: 3.686s, episode steps: 200, steps per second: 54, episode reward: -2.183, mean reward: -0.011 [-0.066, -0.000], mean action: 0.011 [-0.813, 0.777], mean observation: -0.255 [-0.842, 0.016], loss: 0.000105, mean_squared_error: 0.000211, mean_q: -0.007044\n",
      " 11542/100000: episode: 58, duration: 3.338s, episode steps: 187, steps per second: 56, episode reward: 93.225, mean reward: 0.499 [-0.365, 99.580], mean action: 0.295 [-0.431, 2.049], mean observation: -0.212 [-0.809, 0.458], loss: 0.000038, mean_squared_error: 0.000077, mean_q: -0.005272\n",
      " 11742/100000: episode: 59, duration: 3.548s, episode steps: 200, steps per second: 56, episode reward: -20.005, mean reward: -0.100 [-0.325, -0.000], mean action: 0.831 [-0.452, 1.802], mean observation: -0.183 [-0.684, 0.013], loss: 1.536561, mean_squared_error: 3.073121, mean_q: 0.003040\n",
      " 11942/100000: episode: 60, duration: 3.535s, episode steps: 200, steps per second: 57, episode reward: -1067.672, mean reward: -5.338 [-37.993, -0.000], mean action: 3.207 [-2.032, 19.492], mean observation: -0.248 [-1.200, 0.141], loss: 3.830904, mean_squared_error: 7.661809, mean_q: -0.039583\n",
      " 12142/100000: episode: 61, duration: 3.485s, episode steps: 200, steps per second: 57, episode reward: -14.521, mean reward: -0.073 [-0.433, -0.000], mean action: -0.094 [-2.082, 1.074], mean observation: -0.256 [-0.936, 0.018], loss: 0.070903, mean_squared_error: 0.141805, mean_q: -0.131376\n",
      " 12342/100000: episode: 62, duration: 3.527s, episode steps: 200, steps per second: 57, episode reward: -9.795, mean reward: -0.049 [-0.275, -0.000], mean action: -0.426 [-1.657, 0.344], mean observation: -0.298 [-0.996, 0.023], loss: 0.049182, mean_squared_error: 0.098363, mean_q: -0.081758\n",
      " 12455/100000: episode: 63, duration: 1.969s, episode steps: 113, steps per second: 57, episode reward: 89.959, mean reward: 0.796 [-0.513, 99.468], mean action: 0.019 [-1.173, 2.307], mean observation: -0.237 [-1.003, 0.457], loss: 0.038814, mean_squared_error: 0.077627, mean_q: -0.098533\n",
      " 12655/100000: episode: 64, duration: 3.508s, episode steps: 200, steps per second: 57, episode reward: -4.564, mean reward: -0.023 [-0.180, -0.000], mean action: -0.018 [-1.342, 0.932], mean observation: -0.266 [-1.135, 0.101], loss: 1.524322, mean_squared_error: 3.048644, mean_q: -0.112451\n",
      " 12855/100000: episode: 65, duration: 3.594s, episode steps: 200, steps per second: 56, episode reward: -56.552, mean reward: -0.283 [-0.832, -0.000], mean action: -1.451 [-2.885, 0.339], mean observation: -0.359 [-0.949, 0.015], loss: 1.520151, mean_squared_error: 3.040303, mean_q: -0.105206\n",
      " 13055/100000: episode: 66, duration: 3.548s, episode steps: 200, steps per second: 56, episode reward: -9.674, mean reward: -0.048 [-0.329, -0.000], mean action: -0.310 [-1.814, 1.001], mean observation: -0.285 [-0.822, 0.022], loss: 0.052587, mean_squared_error: 0.105174, mean_q: -0.129506\n",
      " 13255/100000: episode: 67, duration: 3.451s, episode steps: 200, steps per second: 58, episode reward: -15.377, mean reward: -0.077 [-0.432, -0.000], mean action: -0.495 [-2.079, 0.621], mean observation: -0.319 [-1.200, 0.028], loss: 0.029114, mean_squared_error: 0.058228, mean_q: -0.099129\n",
      " 13455/100000: episode: 68, duration: 3.509s, episode steps: 200, steps per second: 57, episode reward: -6.014, mean reward: -0.030 [-0.170, -0.000], mean action: 0.444 [-0.229, 1.302], mean observation: -0.215 [-0.692, 0.015], loss: 0.028376, mean_squared_error: 0.056751, mean_q: -0.094235\n",
      " 13655/100000: episode: 69, duration: 3.503s, episode steps: 200, steps per second: 57, episode reward: -12.917, mean reward: -0.065 [-0.221, -0.000], mean action: -0.623 [-1.486, 0.351], mean observation: -0.327 [-0.879, 0.010], loss: 1.465079, mean_squared_error: 2.930158, mean_q: -0.068348\n",
      " 13855/100000: episode: 70, duration: 3.677s, episode steps: 200, steps per second: 54, episode reward: -50.753, mean reward: -0.254 [-0.715, -0.000], mean action: -1.350 [-2.675, 0.382], mean observation: -0.357 [-0.917, 0.013], loss: 0.026047, mean_squared_error: 0.052093, mean_q: -0.084615\n",
      " 14055/100000: episode: 71, duration: 3.564s, episode steps: 200, steps per second: 56, episode reward: -4.002, mean reward: -0.020 [-0.102, -0.000], mean action: -0.177 [-1.011, 0.765], mean observation: -0.280 [-0.719, 0.013], loss: 0.021931, mean_squared_error: 0.043862, mean_q: -0.078560\n",
      " 14255/100000: episode: 72, duration: 3.476s, episode steps: 200, steps per second: 58, episode reward: -5.222, mean reward: -0.026 [-0.128, -0.000], mean action: -0.303 [-1.130, 0.370], mean observation: -0.294 [-0.762, 0.013], loss: 0.767192, mean_squared_error: 1.534384, mean_q: -0.080890\n",
      " 14455/100000: episode: 73, duration: 3.465s, episode steps: 200, steps per second: 58, episode reward: -6.457, mean reward: -0.032 [-0.288, -0.000], mean action: -0.375 [-1.697, 0.362], mean observation: -0.304 [-1.006, 0.014], loss: 1.417927, mean_squared_error: 2.835855, mean_q: -0.075364\n",
      " 14655/100000: episode: 74, duration: 3.539s, episode steps: 200, steps per second: 57, episode reward: -9.291, mean reward: -0.046 [-0.292, -0.000], mean action: -0.190 [-1.708, 0.781], mean observation: -0.279 [-0.884, 0.022], loss: 0.701931, mean_squared_error: 1.403862, mean_q: -0.058181\n",
      " 14855/100000: episode: 75, duration: 3.516s, episode steps: 200, steps per second: 57, episode reward: -58.539, mean reward: -0.293 [-1.019, -0.000], mean action: 1.515 [-0.157, 3.193], mean observation: -0.151 [-0.585, 0.033], loss: 0.726248, mean_squared_error: 1.452497, mean_q: -0.050119\n",
      " 15055/100000: episode: 76, duration: 3.503s, episode steps: 200, steps per second: 57, episode reward: -53.987, mean reward: -0.270 [-0.599, -0.000], mean action: 1.535 [-0.106, 2.447], mean observation: -0.158 [-0.496, 0.014], loss: 0.726980, mean_squared_error: 1.453961, mean_q: -0.112232\n",
      " 15255/100000: episode: 77, duration: 3.414s, episode steps: 200, steps per second: 59, episode reward: -42.797, mean reward: -0.214 [-1.011, -0.000], mean action: 0.895 [-0.572, 3.179], mean observation: -0.184 [-0.759, 0.149], loss: 0.697406, mean_squared_error: 1.394812, mean_q: -0.068970\n",
      " 15455/100000: episode: 78, duration: 3.455s, episode steps: 200, steps per second: 58, episode reward: -17.845, mean reward: -0.089 [-0.404, -0.000], mean action: -0.582 [-2.009, 0.793], mean observation: -0.338 [-1.200, 0.026], loss: 1.337048, mean_squared_error: 2.674096, mean_q: -0.096542\n",
      " 15655/100000: episode: 79, duration: 3.621s, episode steps: 200, steps per second: 55, episode reward: -8.253, mean reward: -0.041 [-0.188, -0.000], mean action: -0.528 [-1.372, 0.277], mean observation: -0.318 [-0.829, 0.007], loss: 0.031981, mean_squared_error: 0.063962, mean_q: -0.108103\n",
      " 15855/100000: episode: 80, duration: 3.509s, episode steps: 200, steps per second: 57, episode reward: -14.857, mean reward: -0.074 [-0.184, -0.000], mean action: -0.796 [-1.357, 0.158], mean observation: -0.345 [-0.937, 0.015], loss: 0.687709, mean_squared_error: 1.375418, mean_q: -0.104791\n",
      " 16055/100000: episode: 81, duration: 3.403s, episode steps: 200, steps per second: 59, episode reward: -10.139, mean reward: -0.051 [-0.237, -0.000], mean action: 0.333 [-0.999, 1.540], mean observation: -0.224 [-0.832, 0.023], loss: 0.664298, mean_squared_error: 1.328597, mean_q: -0.100631\n",
      " 16255/100000: episode: 82, duration: 3.503s, episode steps: 200, steps per second: 57, episode reward: -2.612, mean reward: -0.013 [-0.098, -0.000], mean action: -0.097 [-0.988, 0.638], mean observation: -0.265 [-0.948, 0.021], loss: 1.239227, mean_squared_error: 2.478454, mean_q: -0.105292\n",
      " 16455/100000: episode: 83, duration: 3.486s, episode steps: 200, steps per second: 57, episode reward: -3.063, mean reward: -0.015 [-0.093, -0.000], mean action: -0.129 [-0.967, 0.791], mean observation: -0.265 [-0.823, 0.023], loss: 0.666191, mean_squared_error: 1.332382, mean_q: -0.114897\n",
      " 16655/100000: episode: 84, duration: 3.549s, episode steps: 200, steps per second: 56, episode reward: -104.358, mean reward: -0.522 [-1.779, -0.000], mean action: 1.944 [-0.521, 4.218], mean observation: -0.142 [-0.630, 0.021], loss: 0.622145, mean_squared_error: 1.244290, mean_q: -0.127953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16855/100000: episode: 85, duration: 3.571s, episode steps: 200, steps per second: 56, episode reward: -35.930, mean reward: -0.180 [-0.347, -0.000], mean action: 1.272 [-0.038, 1.863], mean observation: -0.155 [-0.528, 0.017], loss: 0.022885, mean_squared_error: 0.045770, mean_q: -0.120711\n",
      " 16991/100000: episode: 86, duration: 2.411s, episode steps: 136, steps per second: 56, episode reward: 76.169, mean reward: 0.560 [-1.051, 98.985], mean action: 0.388 [-1.453, 3.241], mean observation: -0.200 [-0.838, 0.467], loss: 0.840005, mean_squared_error: 1.680011, mean_q: -0.132967\n",
      " 17191/100000: episode: 87, duration: 3.509s, episode steps: 200, steps per second: 57, episode reward: -3.355, mean reward: -0.017 [-0.179, -0.000], mean action: -0.102 [-1.339, 0.673], mean observation: -0.268 [-0.830, 0.019], loss: 0.584889, mean_squared_error: 1.169778, mean_q: -0.115593\n",
      " 17391/100000: episode: 88, duration: 3.470s, episode steps: 200, steps per second: 58, episode reward: -8.922, mean reward: -0.045 [-0.233, -0.000], mean action: 0.063 [-0.869, 1.527], mean observation: -0.258 [-0.836, 0.015], loss: 1.616200, mean_squared_error: 3.232400, mean_q: -0.107368\n",
      " 17591/100000: episode: 89, duration: 3.496s, episode steps: 200, steps per second: 57, episode reward: -9.911, mean reward: -0.050 [-0.229, -0.000], mean action: -0.380 [-1.515, 0.848], mean observation: -0.310 [-1.200, 0.035], loss: 0.541795, mean_squared_error: 1.083590, mean_q: -0.156211\n",
      " 17791/100000: episode: 90, duration: 3.469s, episode steps: 200, steps per second: 58, episode reward: -8.671, mean reward: -0.043 [-0.148, -0.000], mean action: -0.515 [-1.216, 0.379], mean observation: -0.345 [-1.132, 0.025], loss: 0.539496, mean_squared_error: 1.078992, mean_q: -0.128517\n",
      " 17991/100000: episode: 91, duration: 3.569s, episode steps: 200, steps per second: 56, episode reward: -65.991, mean reward: -0.330 [-0.850, -0.000], mean action: -1.612 [-2.915, 0.195], mean observation: -0.358 [-0.913, 0.012], loss: 0.972025, mean_squared_error: 1.944049, mean_q: -0.171967\n",
      " 18191/100000: episode: 92, duration: 3.504s, episode steps: 200, steps per second: 57, episode reward: -15.158, mean reward: -0.076 [-0.312, -0.000], mean action: -0.708 [-1.767, 0.434], mean observation: -0.337 [-0.970, 0.011], loss: 0.458923, mean_squared_error: 0.917846, mean_q: -0.163321\n",
      " 18391/100000: episode: 93, duration: 3.555s, episode steps: 200, steps per second: 56, episode reward: -36.487, mean reward: -0.182 [-0.543, -0.000], mean action: -1.067 [-2.331, 0.550], mean observation: -0.355 [-0.977, 0.016], loss: 0.032023, mean_squared_error: 0.064046, mean_q: -0.200163\n",
      " 18591/100000: episode: 94, duration: 3.547s, episode steps: 200, steps per second: 56, episode reward: -24.425, mean reward: -0.122 [-0.373, -0.003], mean action: -1.035 [-1.931, -0.169], mean observation: -0.352 [-0.902, 0.012], loss: 0.017481, mean_squared_error: 0.034963, mean_q: -0.232978\n",
      " 18791/100000: episode: 95, duration: 3.454s, episode steps: 200, steps per second: 58, episode reward: -2.306, mean reward: -0.012 [-0.063, -0.000], mean action: 0.040 [-0.794, 0.770], mean observation: -0.253 [-0.910, 0.033], loss: 1.012114, mean_squared_error: 2.024228, mean_q: -0.146023\n",
      " 18991/100000: episode: 96, duration: 3.437s, episode steps: 200, steps per second: 58, episode reward: -3.121, mean reward: -0.016 [-0.086, -0.000], mean action: -0.063 [-0.839, 0.927], mean observation: -0.269 [-0.795, 0.016], loss: 1.544983, mean_squared_error: 3.089966, mean_q: -0.138398\n",
      " 19191/100000: episode: 97, duration: 3.473s, episode steps: 200, steps per second: 58, episode reward: -19.337, mean reward: -0.097 [-0.303, -0.000], mean action: -0.851 [-1.741, 0.136], mean observation: -0.373 [-1.200, 0.028], loss: 0.075195, mean_squared_error: 0.150389, mean_q: -0.171508\n",
      " 19391/100000: episode: 98, duration: 3.499s, episode steps: 200, steps per second: 57, episode reward: -60.497, mean reward: -0.302 [-1.055, -0.000], mean action: -1.562 [-3.248, -0.023], mean observation: -0.365 [-0.945, 0.014], loss: 1.177087, mean_squared_error: 2.354175, mean_q: -0.178158\n",
      " 19591/100000: episode: 99, duration: 3.501s, episode steps: 200, steps per second: 57, episode reward: -41.675, mean reward: -0.208 [-0.461, -0.000], mean action: -1.353 [-2.147, 0.018], mean observation: -0.360 [-0.872, 0.010], loss: 0.391124, mean_squared_error: 0.782247, mean_q: -0.173779\n",
      " 19791/100000: episode: 100, duration: 3.536s, episode steps: 200, steps per second: 57, episode reward: -8.891, mean reward: -0.044 [-0.267, -0.000], mean action: -0.423 [-1.633, 0.448], mean observation: -0.331 [-1.200, 0.020], loss: 0.662375, mean_squared_error: 1.324749, mean_q: -0.166096\n",
      " 19991/100000: episode: 101, duration: 3.554s, episode steps: 200, steps per second: 56, episode reward: -25.161, mean reward: -0.126 [-0.594, -0.000], mean action: 0.348 [-1.168, 2.437], mean observation: -0.240 [-0.816, 0.023], loss: 0.010723, mean_squared_error: 0.021447, mean_q: -0.197395\n",
      " 20191/100000: episode: 102, duration: 3.531s, episode steps: 200, steps per second: 57, episode reward: -29.507, mean reward: -0.148 [-0.417, -0.000], mean action: -1.121 [-2.043, -0.011], mean observation: -0.388 [-1.182, 0.026], loss: 0.925636, mean_squared_error: 1.851272, mean_q: -0.147035\n",
      " 20391/100000: episode: 103, duration: 3.494s, episode steps: 200, steps per second: 57, episode reward: -19.927, mean reward: -0.100 [-0.339, -0.000], mean action: 0.859 [-0.051, 1.841], mean observation: -0.188 [-0.595, 0.013], loss: 0.318148, mean_squared_error: 0.636297, mean_q: -0.170698\n",
      " 20591/100000: episode: 104, duration: 3.463s, episode steps: 200, steps per second: 58, episode reward: -39.649, mean reward: -0.198 [-0.671, -0.000], mean action: 1.072 [-0.577, 2.590], mean observation: -0.115 [-0.851, 0.269], loss: 0.488181, mean_squared_error: 0.976362, mean_q: -0.174320\n",
      " 20791/100000: episode: 105, duration: 3.524s, episode steps: 200, steps per second: 57, episode reward: -30.044, mean reward: -0.150 [-0.360, -0.001], mean action: -1.159 [-1.898, -0.079], mean observation: -0.364 [-0.951, 0.015], loss: 0.732518, mean_squared_error: 1.465037, mean_q: -0.117706\n",
      " 20991/100000: episode: 106, duration: 3.521s, episode steps: 200, steps per second: 57, episode reward: -35.151, mean reward: -0.176 [-0.965, -0.000], mean action: -1.032 [-3.106, 0.565], mean observation: -0.379 [-1.200, 0.026], loss: 0.285552, mean_squared_error: 0.571105, mean_q: -0.191463\n",
      " 21191/100000: episode: 107, duration: 3.462s, episode steps: 200, steps per second: 58, episode reward: -6.832, mean reward: -0.034 [-0.142, -0.000], mean action: -0.068 [-0.965, 1.190], mean observation: -0.272 [-0.909, 0.013], loss: 0.220100, mean_squared_error: 0.440200, mean_q: -0.136347\n",
      " 21391/100000: episode: 108, duration: 3.526s, episode steps: 200, steps per second: 57, episode reward: -8.647, mean reward: -0.043 [-0.279, -0.000], mean action: -0.009 [-1.672, 0.898], mean observation: -0.261 [-0.814, 0.006], loss: 0.025606, mean_squared_error: 0.051212, mean_q: -0.152947\n",
      " 21591/100000: episode: 109, duration: 3.475s, episode steps: 200, steps per second: 58, episode reward: -15.208, mean reward: -0.076 [-0.326, -0.000], mean action: 0.769 [-0.080, 1.805], mean observation: -0.184 [-0.532, 0.018], loss: 0.190011, mean_squared_error: 0.380022, mean_q: -0.095871\n",
      " 21791/100000: episode: 110, duration: 3.438s, episode steps: 200, steps per second: 58, episode reward: -20.555, mean reward: -0.103 [-0.293, -0.002], mean action: -0.936 [-1.711, -0.134], mean observation: -0.352 [-0.888, 0.012], loss: 0.030203, mean_squared_error: 0.060406, mean_q: -0.115772\n",
      " 21991/100000: episode: 111, duration: 3.418s, episode steps: 200, steps per second: 59, episode reward: -21.847, mean reward: -0.109 [-0.404, -0.000], mean action: -0.510 [-2.010, 0.978], mean observation: -0.308 [-1.200, 0.026], loss: 0.191535, mean_squared_error: 0.383070, mean_q: -0.121888\n",
      " 22191/100000: episode: 112, duration: 3.461s, episode steps: 200, steps per second: 58, episode reward: -13.975, mean reward: -0.070 [-0.257, -0.000], mean action: -0.591 [-1.603, 0.451], mean observation: -0.319 [-1.015, 0.019], loss: 0.303042, mean_squared_error: 0.606083, mean_q: -0.104556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 22391/100000: episode: 113, duration: 3.507s, episode steps: 200, steps per second: 57, episode reward: -25.013, mean reward: -0.125 [-0.615, -0.000], mean action: 0.679 [-0.889, 2.479], mean observation: -0.178 [-0.883, 0.210], loss: 0.316978, mean_squared_error: 0.633956, mean_q: -0.152884\n",
      " 22591/100000: episode: 114, duration: 3.496s, episode steps: 200, steps per second: 57, episode reward: -7.714, mean reward: -0.039 [-0.223, -0.000], mean action: -0.239 [-1.495, 0.727], mean observation: -0.295 [-1.200, 0.031], loss: 0.038679, mean_squared_error: 0.077358, mean_q: -0.158393\n",
      " 22791/100000: episode: 115, duration: 3.533s, episode steps: 200, steps per second: 57, episode reward: -24.697, mean reward: -0.123 [-0.270, -0.000], mean action: 1.050 [-0.064, 1.642], mean observation: -0.164 [-0.513, 0.015], loss: 0.298564, mean_squared_error: 0.597127, mean_q: -0.099035\n",
      " 22991/100000: episode: 116, duration: 3.490s, episode steps: 200, steps per second: 57, episode reward: -23.687, mean reward: -0.118 [-0.241, -0.000], mean action: 1.050 [-0.070, 1.552], mean observation: -0.149 [-0.594, 0.021], loss: 0.024201, mean_squared_error: 0.048402, mean_q: -0.099185\n",
      " 23191/100000: episode: 117, duration: 3.539s, episode steps: 200, steps per second: 57, episode reward: -2.841, mean reward: -0.014 [-0.094, -0.000], mean action: -0.226 [-0.970, 0.610], mean observation: -0.298 [-0.951, 0.018], loss: 0.032137, mean_squared_error: 0.064274, mean_q: -0.105490\n",
      " 23391/100000: episode: 118, duration: 3.512s, episode steps: 200, steps per second: 57, episode reward: -27.012, mean reward: -0.135 [-0.487, -0.000], mean action: 0.572 [-0.945, 2.207], mean observation: -0.227 [-0.878, 0.017], loss: 0.352037, mean_squared_error: 0.704074, mean_q: -0.075530\n",
      " 23591/100000: episode: 119, duration: 3.489s, episode steps: 200, steps per second: 57, episode reward: -11.664, mean reward: -0.058 [-0.230, -0.000], mean action: 0.509 [-0.431, 1.518], mean observation: -0.213 [-0.613, 0.012], loss: 0.032872, mean_squared_error: 0.065744, mean_q: -0.076954\n",
      " 23791/100000: episode: 120, duration: 3.524s, episode steps: 200, steps per second: 57, episode reward: -35.202, mean reward: -0.176 [-0.655, -0.000], mean action: -1.002 [-2.559, 0.336], mean observation: -0.329 [-0.994, 0.017], loss: 0.160898, mean_squared_error: 0.321797, mean_q: -0.071716\n",
      " 23991/100000: episode: 121, duration: 3.487s, episode steps: 200, steps per second: 57, episode reward: -11.407, mean reward: -0.057 [-0.201, -0.000], mean action: -0.564 [-1.418, 0.603], mean observation: -0.323 [-0.826, 0.010], loss: 0.109800, mean_squared_error: 0.219600, mean_q: -0.072349\n",
      " 24191/100000: episode: 122, duration: 3.478s, episode steps: 200, steps per second: 58, episode reward: -63.352, mean reward: -0.317 [-0.600, -0.000], mean action: -1.626 [-2.450, 0.257], mean observation: -0.356 [-0.876, 0.010], loss: 0.082900, mean_squared_error: 0.165799, mean_q: -0.088191\n",
      " 24391/100000: episode: 123, duration: 3.532s, episode steps: 200, steps per second: 57, episode reward: -11.397, mean reward: -0.057 [-0.182, -0.000], mean action: -0.666 [-1.350, 0.146], mean observation: -0.337 [-1.042, 0.013], loss: 0.178096, mean_squared_error: 0.356192, mean_q: -0.037131\n",
      " 24591/100000: episode: 124, duration: 3.508s, episode steps: 200, steps per second: 57, episode reward: -34.904, mean reward: -0.175 [-0.367, -0.000], mean action: -1.260 [-1.916, 0.276], mean observation: -0.370 [-0.980, 0.017], loss: 0.070644, mean_squared_error: 0.141288, mean_q: -0.141915\n",
      " 24791/100000: episode: 125, duration: 3.484s, episode steps: 200, steps per second: 57, episode reward: -44.351, mean reward: -0.222 [-0.668, -0.000], mean action: -1.302 [-2.585, 0.345], mean observation: -0.361 [-0.982, 0.017], loss: 0.058463, mean_squared_error: 0.116926, mean_q: -0.072497\n",
      " 24991/100000: episode: 126, duration: 3.490s, episode steps: 200, steps per second: 57, episode reward: -3.548, mean reward: -0.018 [-0.101, -0.000], mean action: 0.257 [-0.223, 1.006], mean observation: -0.231 [-0.596, 0.016], loss: 0.056058, mean_squared_error: 0.112117, mean_q: -0.076636\n",
      " 25191/100000: episode: 127, duration: 3.482s, episode steps: 200, steps per second: 57, episode reward: -52.452, mean reward: -0.262 [-0.659, -0.002], mean action: -1.529 [-2.566, -0.132], mean observation: -0.365 [-0.899, 0.012], loss: 0.057583, mean_squared_error: 0.115166, mean_q: -0.045504\n",
      " 25391/100000: episode: 128, duration: 3.471s, episode steps: 200, steps per second: 58, episode reward: -6.758, mean reward: -0.034 [-0.160, -0.000], mean action: 0.483 [-0.115, 1.265], mean observation: -0.216 [-0.642, 0.021], loss: 0.082685, mean_squared_error: 0.165370, mean_q: -0.090973\n",
      " 25591/100000: episode: 129, duration: 3.547s, episode steps: 200, steps per second: 56, episode reward: -13.117, mean reward: -0.066 [-0.263, -0.000], mean action: -0.623 [-1.623, 0.554], mean observation: -0.327 [-0.972, 0.016], loss: 0.060071, mean_squared_error: 0.120141, mean_q: -0.083715\n",
      " 25791/100000: episode: 130, duration: 3.475s, episode steps: 200, steps per second: 58, episode reward: -25.264, mean reward: -0.126 [-0.417, -0.005], mean action: 1.056 [0.215, 2.043], mean observation: -0.167 [-0.550, 0.014], loss: 0.021600, mean_squared_error: 0.043200, mean_q: -0.016270\n",
      " 25991/100000: episode: 131, duration: 3.467s, episode steps: 200, steps per second: 58, episode reward: -13.484, mean reward: -0.067 [-0.257, -0.000], mean action: -0.663 [-1.603, 0.323], mean observation: -0.333 [-1.010, 0.021], loss: 0.031670, mean_squared_error: 0.063340, mean_q: -0.057129\n",
      " 26191/100000: episode: 132, duration: 3.449s, episode steps: 200, steps per second: 58, episode reward: -40.864, mean reward: -0.204 [-0.615, -0.001], mean action: 1.316 [0.080, 2.479], mean observation: -0.165 [-0.512, 0.010], loss: 0.060227, mean_squared_error: 0.120454, mean_q: -0.057388\n",
      " 26391/100000: episode: 133, duration: 3.483s, episode steps: 200, steps per second: 57, episode reward: -28.826, mean reward: -0.144 [-0.361, -0.000], mean action: -1.020 [-1.900, 0.173], mean observation: -0.351 [-0.994, 0.017], loss: 0.101472, mean_squared_error: 0.202945, mean_q: -0.010930\n",
      " 26583/100000: episode: 134, duration: 3.333s, episode steps: 192, steps per second: 58, episode reward: 83.355, mean reward: 0.434 [-0.437, 99.709], mean action: 0.641 [-0.492, 2.091], mean observation: -0.170 [-0.834, 0.469], loss: 0.047660, mean_squared_error: 0.095320, mean_q: -0.119329\n",
      " 26783/100000: episode: 135, duration: 3.509s, episode steps: 200, steps per second: 57, episode reward: -34.987, mean reward: -0.175 [-0.431, -0.000], mean action: -1.171 [-2.076, 0.259], mean observation: -0.366 [-0.977, 0.016], loss: 0.059225, mean_squared_error: 0.118449, mean_q: 0.014670\n",
      " 26983/100000: episode: 136, duration: 3.490s, episode steps: 200, steps per second: 57, episode reward: -10.597, mean reward: -0.053 [-0.250, -0.000], mean action: -0.572 [-1.581, 0.100], mean observation: -0.322 [-0.880, 0.014], loss: 0.173711, mean_squared_error: 0.347421, mean_q: 0.001775\n",
      " 27183/100000: episode: 137, duration: 3.461s, episode steps: 200, steps per second: 58, episode reward: -5.250, mean reward: -0.026 [-0.100, -0.000], mean action: -0.163 [-0.975, 1.002], mean observation: -0.270 [-0.864, 0.024], loss: 0.066829, mean_squared_error: 0.133659, mean_q: -0.006702\n",
      " 27383/100000: episode: 138, duration: 3.517s, episode steps: 200, steps per second: 57, episode reward: -4.683, mean reward: -0.023 [-0.246, -0.000], mean action: 0.038 [-0.768, 1.567], mean observation: -0.262 [-0.821, 0.022], loss: 0.073638, mean_squared_error: 0.147276, mean_q: 0.078524\n",
      " 27583/100000: episode: 139, duration: 3.507s, episode steps: 200, steps per second: 57, episode reward: -17.135, mean reward: -0.086 [-0.285, -0.000], mean action: 0.820 [-0.048, 1.687], mean observation: -0.184 [-0.549, 0.011], loss: 0.087936, mean_squared_error: 0.175872, mean_q: -0.021397\n",
      " 27783/100000: episode: 140, duration: 3.418s, episode steps: 200, steps per second: 59, episode reward: -7.587, mean reward: -0.038 [-0.162, -0.000], mean action: 0.088 [-0.969, 1.271], mean observation: -0.268 [-1.103, 0.026], loss: 0.073556, mean_squared_error: 0.147111, mean_q: -0.037119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 27983/100000: episode: 141, duration: 3.536s, episode steps: 200, steps per second: 57, episode reward: -4.310, mean reward: -0.022 [-0.103, -0.000], mean action: -0.271 [-1.015, 0.459], mean observation: -0.286 [-0.734, 0.017], loss: 0.048484, mean_squared_error: 0.096968, mean_q: -0.010516\n",
      " 28183/100000: episode: 142, duration: 3.508s, episode steps: 200, steps per second: 57, episode reward: -13.638, mean reward: -0.068 [-0.209, -0.000], mean action: 0.452 [-0.714, 1.447], mean observation: -0.221 [-0.709, 0.012], loss: 0.042917, mean_squared_error: 0.085835, mean_q: 0.008355\n",
      " 28383/100000: episode: 143, duration: 3.502s, episode steps: 200, steps per second: 57, episode reward: -34.624, mean reward: -0.173 [-0.431, -0.000], mean action: 1.251 [-0.028, 2.075], mean observation: -0.159 [-0.561, 0.014], loss: 0.076820, mean_squared_error: 0.153640, mean_q: 0.026793\n",
      " 28583/100000: episode: 144, duration: 3.454s, episode steps: 200, steps per second: 58, episode reward: -21.402, mean reward: -0.107 [-0.343, -0.000], mean action: -0.800 [-1.851, 0.537], mean observation: -0.354 [-1.053, 0.020], loss: 0.150261, mean_squared_error: 0.300521, mean_q: 0.094462\n",
      " 28783/100000: episode: 145, duration: 3.512s, episode steps: 200, steps per second: 57, episode reward: -3.101, mean reward: -0.016 [-0.134, -0.000], mean action: 0.105 [-0.631, 1.156], mean observation: -0.251 [-0.681, 0.008], loss: 0.037227, mean_squared_error: 0.074453, mean_q: 0.109960\n",
      " 28983/100000: episode: 146, duration: 3.549s, episode steps: 200, steps per second: 56, episode reward: -8.421, mean reward: -0.042 [-0.159, -0.000], mean action: -0.594 [-1.260, -0.032], mean observation: -0.323 [-0.915, 0.018], loss: 0.110049, mean_squared_error: 0.220098, mean_q: 0.021990\n",
      " 29183/100000: episode: 147, duration: 3.466s, episode steps: 200, steps per second: 58, episode reward: -9.096, mean reward: -0.045 [-0.161, -0.000], mean action: -0.145 [-1.269, 1.210], mean observation: -0.282 [-0.901, 0.019], loss: 0.052803, mean_squared_error: 0.105606, mean_q: 0.000586\n",
      " 29383/100000: episode: 148, duration: 3.480s, episode steps: 200, steps per second: 57, episode reward: -20.437, mean reward: -0.102 [-0.422, -0.000], mean action: 0.818 [-0.498, 2.054], mean observation: -0.188 [-0.597, 0.014], loss: 0.054757, mean_squared_error: 0.109514, mean_q: -0.047050\n",
      " 29583/100000: episode: 149, duration: 3.459s, episode steps: 200, steps per second: 58, episode reward: -30.472, mean reward: -0.152 [-0.594, -0.000], mean action: -0.935 [-2.436, 0.487], mean observation: -0.348 [-1.182, 0.026], loss: 0.169759, mean_squared_error: 0.339518, mean_q: 0.012608\n",
      " 29783/100000: episode: 150, duration: 3.554s, episode steps: 200, steps per second: 56, episode reward: -9.631, mean reward: -0.048 [-0.510, -0.000], mean action: 0.455 [-0.572, 2.257], mean observation: -0.211 [-0.847, 0.365], loss: 0.271600, mean_squared_error: 0.543200, mean_q: 0.030025\n",
      " 29983/100000: episode: 151, duration: 3.945s, episode steps: 200, steps per second: 51, episode reward: -8.981, mean reward: -0.045 [-0.153, -0.000], mean action: -0.600 [-1.236, 0.233], mean observation: -0.323 [-0.889, 0.012], loss: 0.877561, mean_squared_error: 1.755121, mean_q: 0.029227\n",
      " 30183/100000: episode: 152, duration: 4.031s, episode steps: 200, steps per second: 50, episode reward: -7.522, mean reward: -0.038 [-0.146, -0.000], mean action: -0.527 [-1.209, 0.156], mean observation: -0.333 [-0.989, 0.025], loss: 0.105428, mean_squared_error: 0.210855, mean_q: 0.040774\n",
      " 30383/100000: episode: 153, duration: 3.748s, episode steps: 200, steps per second: 53, episode reward: -3.692, mean reward: -0.018 [-0.186, -0.000], mean action: -0.234 [-1.364, 0.467], mean observation: -0.282 [-0.798, 0.014], loss: 0.063781, mean_squared_error: 0.127561, mean_q: 0.062272\n",
      " 30583/100000: episode: 154, duration: 3.387s, episode steps: 200, steps per second: 59, episode reward: -2.737, mean reward: -0.014 [-0.081, -0.000], mean action: -0.278 [-0.898, 0.275], mean observation: -0.304 [-1.039, 0.025], loss: 0.046270, mean_squared_error: 0.092541, mean_q: 0.075310\n",
      " 30783/100000: episode: 155, duration: 3.458s, episode steps: 200, steps per second: 58, episode reward: -3.540, mean reward: -0.018 [-0.080, -0.000], mean action: -0.169 [-0.895, 0.816], mean observation: -0.286 [-0.851, 0.016], loss: 0.118017, mean_squared_error: 0.236034, mean_q: 0.152908\n",
      " 30983/100000: episode: 156, duration: 3.474s, episode steps: 200, steps per second: 58, episode reward: -32.849, mean reward: -0.164 [-0.553, -0.000], mean action: 1.088 [-0.320, 2.352], mean observation: -0.180 [-0.596, 0.014], loss: 0.107662, mean_squared_error: 0.215324, mean_q: 0.101715\n",
      " 31183/100000: episode: 157, duration: 3.488s, episode steps: 200, steps per second: 57, episode reward: -9.320, mean reward: -0.047 [-0.221, -0.000], mean action: 0.149 [-0.894, 1.485], mean observation: -0.251 [-0.747, 0.015], loss: 0.081472, mean_squared_error: 0.162944, mean_q: 0.000646\n",
      " 31383/100000: episode: 158, duration: 3.505s, episode steps: 200, steps per second: 57, episode reward: -0.993, mean reward: -0.005 [-0.043, -0.000], mean action: -0.100 [-0.654, 0.308], mean observation: -0.271 [-0.643, 0.006], loss: 0.113854, mean_squared_error: 0.227707, mean_q: 0.151515\n",
      " 31583/100000: episode: 159, duration: 3.533s, episode steps: 200, steps per second: 57, episode reward: -10.557, mean reward: -0.053 [-0.179, -0.000], mean action: -0.616 [-1.339, 0.153], mean observation: -0.335 [-0.952, 0.017], loss: 0.117294, mean_squared_error: 0.234588, mean_q: 0.083812\n",
      " 31783/100000: episode: 160, duration: 3.433s, episode steps: 200, steps per second: 58, episode reward: -21.549, mean reward: -0.108 [-0.232, -0.000], mean action: 0.993 [0.015, 1.524], mean observation: -0.166 [-0.509, 0.011], loss: 0.129511, mean_squared_error: 0.259022, mean_q: 0.130023\n",
      " 31983/100000: episode: 161, duration: 3.494s, episode steps: 200, steps per second: 57, episode reward: -9.568, mean reward: -0.048 [-0.205, -0.000], mean action: -0.606 [-1.431, 0.250], mean observation: -0.328 [-0.864, 0.012], loss: 0.131568, mean_squared_error: 0.263135, mean_q: 0.063922\n",
      " 32183/100000: episode: 162, duration: 3.472s, episode steps: 200, steps per second: 58, episode reward: -14.301, mean reward: -0.072 [-0.295, -0.001], mean action: 0.747 [0.096, 1.717], mean observation: -0.182 [-0.483, 0.014], loss: 0.072301, mean_squared_error: 0.144602, mean_q: 0.061243\n",
      " 32383/100000: episode: 163, duration: 3.518s, episode steps: 200, steps per second: 57, episode reward: -12.525, mean reward: -0.063 [-0.253, -0.000], mean action: -0.377 [-1.592, 0.985], mean observation: -0.304 [-1.093, 0.023], loss: 1.130964, mean_squared_error: 2.261928, mean_q: 0.100479\n",
      " 32583/100000: episode: 164, duration: 3.504s, episode steps: 200, steps per second: 57, episode reward: -5.010, mean reward: -0.025 [-0.134, -0.000], mean action: -0.311 [-1.157, 0.367], mean observation: -0.291 [-0.874, 0.018], loss: 0.090494, mean_squared_error: 0.180989, mean_q: 0.075726\n",
      " 32783/100000: episode: 165, duration: 3.531s, episode steps: 200, steps per second: 57, episode reward: -17.645, mean reward: -0.088 [-0.580, -0.000], mean action: 0.609 [-0.996, 2.408], mean observation: -0.173 [-0.815, 0.398], loss: 0.099482, mean_squared_error: 0.198963, mean_q: 0.092492\n",
      " 32983/100000: episode: 166, duration: 3.486s, episode steps: 200, steps per second: 57, episode reward: -4.313, mean reward: -0.022 [-0.181, -0.000], mean action: 0.135 [-0.681, 1.347], mean observation: -0.250 [-0.901, 0.032], loss: 1.798881, mean_squared_error: 3.597762, mean_q: 0.204272\n",
      " 33183/100000: episode: 167, duration: 3.533s, episode steps: 200, steps per second: 57, episode reward: -8.051, mean reward: -0.040 [-0.148, -0.000], mean action: 0.542 [-0.293, 1.217], mean observation: -0.208 [-0.571, 0.015], loss: 0.898123, mean_squared_error: 1.796246, mean_q: 0.188183\n",
      " 33383/100000: episode: 168, duration: 3.538s, episode steps: 200, steps per second: 57, episode reward: -5.021, mean reward: -0.025 [-0.221, -0.000], mean action: 0.322 [-0.442, 1.487], mean observation: -0.241 [-0.974, 0.045], loss: 0.199614, mean_squared_error: 0.399227, mean_q: 0.247862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33583/100000: episode: 169, duration: 3.467s, episode steps: 200, steps per second: 58, episode reward: -11.698, mean reward: -0.058 [-0.130, -0.000], mean action: -0.056 [-1.141, 1.121], mean observation: -0.263 [-0.868, 0.016], loss: 1.139832, mean_squared_error: 2.279664, mean_q: 0.168897\n",
      " 33739/100000: episode: 170, duration: 2.735s, episode steps: 156, steps per second: 57, episode reward: 88.532, mean reward: 0.568 [-0.888, 99.094], mean action: 0.165 [-0.798, 3.009], mean observation: -0.255 [-0.996, 0.472], loss: 0.106093, mean_squared_error: 0.212186, mean_q: 0.155131\n",
      " 33939/100000: episode: 171, duration: 3.519s, episode steps: 200, steps per second: 57, episode reward: -4.045, mean reward: -0.020 [-0.096, -0.000], mean action: 0.062 [-0.813, 0.981], mean observation: -0.252 [-0.688, 0.020], loss: 0.103308, mean_squared_error: 0.206615, mean_q: 0.268830\n",
      " 34139/100000: episode: 172, duration: 3.470s, episode steps: 200, steps per second: 58, episode reward: -41.856, mean reward: -0.209 [-0.791, -0.000], mean action: -1.064 [-2.812, 0.576], mean observation: -0.335 [-0.869, 0.010], loss: 0.887816, mean_squared_error: 1.775632, mean_q: 0.084094\n",
      " 34339/100000: episode: 173, duration: 3.527s, episode steps: 200, steps per second: 57, episode reward: -2.491, mean reward: -0.012 [-0.107, -0.000], mean action: -0.157 [-1.033, 0.518], mean observation: -0.272 [-0.971, 0.032], loss: 0.150727, mean_squared_error: 0.301454, mean_q: 0.162681\n",
      " 34539/100000: episode: 174, duration: 3.475s, episode steps: 200, steps per second: 58, episode reward: -5.776, mean reward: -0.029 [-0.112, -0.000], mean action: 0.254 [-0.785, 1.060], mean observation: -0.235 [-0.716, 0.018], loss: 0.846921, mean_squared_error: 1.693842, mean_q: 0.203020\n",
      " 34739/100000: episode: 175, duration: 3.478s, episode steps: 200, steps per second: 58, episode reward: -5.772, mean reward: -0.029 [-0.175, -0.000], mean action: -0.014 [-1.324, 1.111], mean observation: -0.274 [-1.041, 0.032], loss: 0.091523, mean_squared_error: 0.183046, mean_q: 0.130637\n",
      " 34939/100000: episode: 176, duration: 3.491s, episode steps: 200, steps per second: 57, episode reward: -5.924, mean reward: -0.030 [-0.102, -0.000], mean action: 0.037 [-1.011, 0.995], mean observation: -0.260 [-0.933, 0.019], loss: 0.849909, mean_squared_error: 1.699818, mean_q: 0.161441\n",
      " 35139/100000: episode: 177, duration: 3.480s, episode steps: 200, steps per second: 57, episode reward: -49.629, mean reward: -0.248 [-0.851, -0.000], mean action: 1.401 [-0.168, 2.917], mean observation: -0.168 [-0.505, 0.009], loss: 0.243462, mean_squared_error: 0.486924, mean_q: 0.071315\n",
      " 35339/100000: episode: 178, duration: 3.500s, episode steps: 200, steps per second: 57, episode reward: -9.932, mean reward: -0.050 [-0.251, -0.000], mean action: -0.567 [-1.583, 0.239], mean observation: -0.318 [-0.965, 0.008], loss: 0.230455, mean_squared_error: 0.460911, mean_q: 0.228255\n",
      " 35539/100000: episode: 179, duration: 3.515s, episode steps: 200, steps per second: 57, episode reward: -2.175, mean reward: -0.011 [-0.056, -0.000], mean action: -0.091 [-0.751, 0.727], mean observation: -0.282 [-0.879, 0.025], loss: 0.193673, mean_squared_error: 0.387346, mean_q: 0.313008\n",
      " 35739/100000: episode: 180, duration: 3.500s, episode steps: 200, steps per second: 57, episode reward: -25.800, mean reward: -0.129 [-0.335, -0.000], mean action: -1.074 [-1.831, 0.349], mean observation: -0.368 [-0.987, 0.019], loss: 0.132872, mean_squared_error: 0.265745, mean_q: 0.245886\n",
      " 35939/100000: episode: 181, duration: 3.442s, episode steps: 200, steps per second: 58, episode reward: -7.503, mean reward: -0.038 [-0.130, -0.000], mean action: 0.502 [-0.359, 1.140], mean observation: -0.205 [-0.663, 0.013], loss: 0.156960, mean_squared_error: 0.313921, mean_q: 0.262812\n",
      " 36139/100000: episode: 182, duration: 3.545s, episode steps: 200, steps per second: 56, episode reward: -13.433, mean reward: -0.067 [-0.221, -0.000], mean action: -0.658 [-1.487, 0.467], mean observation: -0.367 [-1.124, 0.044], loss: 0.180297, mean_squared_error: 0.360595, mean_q: 0.201190\n",
      " 36339/100000: episode: 183, duration: 3.507s, episode steps: 200, steps per second: 57, episode reward: -55.559, mean reward: -0.278 [-0.704, -0.000], mean action: 1.381 [-0.390, 2.653], mean observation: -0.140 [-0.675, 0.145], loss: 1.796224, mean_squared_error: 3.592449, mean_q: 0.317094\n",
      " 36539/100000: episode: 184, duration: 3.477s, episode steps: 200, steps per second: 58, episode reward: -42.540, mean reward: -0.213 [-0.758, -0.000], mean action: 1.219 [-0.056, 2.753], mean observation: -0.178 [-0.502, 0.009], loss: 0.092794, mean_squared_error: 0.185588, mean_q: 0.288973\n",
      " 36739/100000: episode: 185, duration: 3.476s, episode steps: 200, steps per second: 58, episode reward: -13.514, mean reward: -0.068 [-0.347, -0.000], mean action: 0.725 [-0.010, 1.862], mean observation: -0.185 [-0.497, 0.013], loss: 1.168867, mean_squared_error: 2.337734, mean_q: 0.240953\n",
      " 36939/100000: episode: 186, duration: 3.485s, episode steps: 200, steps per second: 57, episode reward: -27.359, mean reward: -0.137 [-0.895, -0.000], mean action: 0.724 [-0.808, 2.992], mean observation: -0.188 [-0.757, 0.118], loss: 1.235404, mean_squared_error: 2.470808, mean_q: 0.455129\n",
      " 37139/100000: episode: 187, duration: 3.528s, episode steps: 200, steps per second: 57, episode reward: -1.867, mean reward: -0.009 [-0.082, -0.000], mean action: -0.242 [-0.906, 0.137], mean observation: -0.287 [-0.722, 0.014], loss: 0.087499, mean_squared_error: 0.174997, mean_q: 0.209297\n",
      " 37339/100000: episode: 188, duration: 3.467s, episode steps: 200, steps per second: 58, episode reward: -16.686, mean reward: -0.083 [-0.468, -0.000], mean action: 0.741 [0.037, 2.163], mean observation: -0.195 [-0.597, 0.019], loss: 0.144622, mean_squared_error: 0.289244, mean_q: 0.290089\n",
      " 37539/100000: episode: 189, duration: 3.463s, episode steps: 200, steps per second: 58, episode reward: -10.470, mean reward: -0.052 [-0.239, -0.000], mean action: 0.597 [-0.358, 1.547], mean observation: -0.198 [-0.639, 0.019], loss: 1.171053, mean_squared_error: 2.342105, mean_q: 0.384236\n",
      " 37739/100000: episode: 190, duration: 3.502s, episode steps: 200, steps per second: 57, episode reward: -8.475, mean reward: -0.042 [-0.251, -0.000], mean action: 0.006 [-1.586, 0.937], mean observation: -0.275 [-1.034, 0.016], loss: 0.184588, mean_squared_error: 0.369176, mean_q: 0.355873\n",
      " 37939/100000: episode: 191, duration: 3.527s, episode steps: 200, steps per second: 57, episode reward: -2.901, mean reward: -0.015 [-0.099, -0.000], mean action: -0.184 [-0.996, 0.495], mean observation: -0.276 [-0.859, 0.025], loss: 0.136251, mean_squared_error: 0.272502, mean_q: 0.276189\n",
      " 38139/100000: episode: 192, duration: 3.569s, episode steps: 200, steps per second: 56, episode reward: -15.642, mean reward: -0.078 [-0.261, -0.000], mean action: 0.743 [-0.222, 1.614], mean observation: -0.182 [-0.649, 0.023], loss: 0.268724, mean_squared_error: 0.537448, mean_q: 0.435182\n",
      " 38339/100000: episode: 193, duration: 3.562s, episode steps: 200, steps per second: 56, episode reward: -5.661, mean reward: -0.028 [-0.108, -0.000], mean action: 0.431 [-0.246, 1.041], mean observation: -0.204 [-0.621, 0.018], loss: 0.303652, mean_squared_error: 0.607304, mean_q: 0.371182\n",
      " 38539/100000: episode: 194, duration: 3.531s, episode steps: 200, steps per second: 57, episode reward: -23.396, mean reward: -0.117 [-0.298, -0.000], mean action: -0.914 [-1.725, 0.366], mean observation: -0.349 [-1.012, 0.018], loss: 0.410771, mean_squared_error: 0.821542, mean_q: 0.289326\n",
      " 38739/100000: episode: 195, duration: 3.528s, episode steps: 200, steps per second: 57, episode reward: -14.747, mean reward: -0.074 [-0.310, -0.000], mean action: 0.548 [-0.415, 1.760], mean observation: -0.218 [-0.603, 0.011], loss: 0.365605, mean_squared_error: 0.731210, mean_q: 0.310032\n",
      " 38939/100000: episode: 196, duration: 3.495s, episode steps: 200, steps per second: 57, episode reward: -9.090, mean reward: -0.045 [-0.251, -0.000], mean action: -0.514 [-1.584, 0.510], mean observation: -0.307 [-0.877, 0.022], loss: 0.108729, mean_squared_error: 0.217458, mean_q: 0.465029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 39139/100000: episode: 197, duration: 3.512s, episode steps: 200, steps per second: 57, episode reward: -2.213, mean reward: -0.011 [-0.057, -0.000], mean action: 0.222 [-0.494, 0.757], mean observation: -0.238 [-0.576, 0.009], loss: 0.175787, mean_squared_error: 0.351573, mean_q: 0.490839\n",
      " 39339/100000: episode: 198, duration: 3.496s, episode steps: 200, steps per second: 57, episode reward: -4.668, mean reward: -0.023 [-0.123, -0.000], mean action: 0.346 [-0.486, 1.111], mean observation: -0.224 [-0.611, 0.018], loss: 0.231782, mean_squared_error: 0.463564, mean_q: 0.408378\n",
      " 39539/100000: episode: 199, duration: 3.500s, episode steps: 200, steps per second: 57, episode reward: -8.035, mean reward: -0.040 [-0.099, -0.000], mean action: 0.591 [-0.132, 0.996], mean observation: -0.202 [-0.547, 0.009], loss: 0.412873, mean_squared_error: 0.825747, mean_q: 0.455720\n",
      " 39739/100000: episode: 200, duration: 3.496s, episode steps: 200, steps per second: 57, episode reward: -14.771, mean reward: -0.074 [-0.226, -0.000], mean action: -0.734 [-1.504, 0.349], mean observation: -0.349 [-1.130, 0.024], loss: 1.051096, mean_squared_error: 2.102192, mean_q: 0.369981\n",
      " 39939/100000: episode: 201, duration: 3.538s, episode steps: 200, steps per second: 57, episode reward: -6.884, mean reward: -0.034 [-0.230, -0.000], mean action: 0.092 [-0.993, 1.516], mean observation: -0.254 [-0.787, 0.018], loss: 0.503398, mean_squared_error: 1.006796, mean_q: 0.392920\n",
      " 40139/100000: episode: 202, duration: 3.589s, episode steps: 200, steps per second: 56, episode reward: -16.598, mean reward: -0.083 [-0.333, -0.000], mean action: -0.774 [-1.824, 0.225], mean observation: -0.349 [-1.137, 0.026], loss: 0.210927, mean_squared_error: 0.421854, mean_q: 0.521947\n",
      " 40339/100000: episode: 203, duration: 3.498s, episode steps: 200, steps per second: 57, episode reward: -16.870, mean reward: -0.084 [-0.355, -0.000], mean action: 0.720 [-0.692, 1.884], mean observation: -0.201 [-0.764, 0.013], loss: 0.719991, mean_squared_error: 1.439982, mean_q: 0.454032\n",
      " 40539/100000: episode: 204, duration: 3.467s, episode steps: 200, steps per second: 58, episode reward: -8.145, mean reward: -0.041 [-0.132, -0.000], mean action: 0.546 [-0.314, 1.151], mean observation: -0.196 [-0.599, 0.013], loss: 0.186474, mean_squared_error: 0.372948, mean_q: 0.365784\n",
      " 40739/100000: episode: 205, duration: 3.508s, episode steps: 200, steps per second: 57, episode reward: -9.684, mean reward: -0.048 [-0.206, -0.000], mean action: 0.511 [-0.360, 1.437], mean observation: -0.209 [-0.687, 0.020], loss: 1.130702, mean_squared_error: 2.261405, mean_q: 0.473807\n",
      " 40939/100000: episode: 206, duration: 3.456s, episode steps: 200, steps per second: 58, episode reward: -1.720, mean reward: -0.009 [-0.060, -0.000], mean action: 0.092 [-0.776, 0.675], mean observation: -0.258 [-0.770, 0.020], loss: 0.300154, mean_squared_error: 0.600308, mean_q: 0.445471\n",
      " 41139/100000: episode: 207, duration: 3.530s, episode steps: 200, steps per second: 57, episode reward: -16.738, mean reward: -0.084 [-0.297, -0.000], mean action: 0.616 [-0.871, 1.722], mean observation: -0.205 [-0.758, 0.028], loss: 0.135214, mean_squared_error: 0.270428, mean_q: 0.470201\n",
      " 41339/100000: episode: 208, duration: 3.513s, episode steps: 200, steps per second: 57, episode reward: -4.257, mean reward: -0.021 [-0.109, -0.000], mean action: 0.227 [-0.696, 1.042], mean observation: -0.223 [-0.802, 0.026], loss: 0.187509, mean_squared_error: 0.375017, mean_q: 0.476074\n",
      " 41539/100000: episode: 209, duration: 3.495s, episode steps: 200, steps per second: 57, episode reward: -14.700, mean reward: -0.073 [-0.271, -0.000], mean action: 0.600 [-0.550, 1.648], mean observation: -0.197 [-0.704, 0.015], loss: 0.325920, mean_squared_error: 0.651840, mean_q: 0.384507\n",
      " 41739/100000: episode: 210, duration: 3.499s, episode steps: 200, steps per second: 57, episode reward: -7.482, mean reward: -0.037 [-0.158, -0.000], mean action: 0.516 [-0.089, 1.258], mean observation: -0.206 [-0.608, 0.015], loss: 1.279717, mean_squared_error: 2.559434, mean_q: 0.332002\n",
      " 41939/100000: episode: 211, duration: 3.448s, episode steps: 200, steps per second: 58, episode reward: -8.390, mean reward: -0.042 [-0.160, -0.000], mean action: 0.520 [-0.272, 1.266], mean observation: -0.206 [-0.682, 0.018], loss: 0.257589, mean_squared_error: 0.515178, mean_q: 0.646589\n",
      " 42139/100000: episode: 212, duration: 3.490s, episode steps: 200, steps per second: 57, episode reward: -14.958, mean reward: -0.075 [-0.335, -0.000], mean action: -0.661 [-1.831, 0.600], mean observation: -0.325 [-0.836, 0.007], loss: 0.080330, mean_squared_error: 0.160660, mean_q: 0.372000\n",
      " 42339/100000: episode: 213, duration: 3.541s, episode steps: 200, steps per second: 56, episode reward: -14.786, mean reward: -0.074 [-0.233, -0.001], mean action: -0.768 [-1.528, -0.096], mean observation: -0.338 [-0.898, 0.015], loss: 1.188406, mean_squared_error: 2.376813, mean_q: 0.357884\n",
      " 42539/100000: episode: 214, duration: 3.504s, episode steps: 200, steps per second: 57, episode reward: -87.178, mean reward: -0.436 [-1.176, -0.000], mean action: 1.837 [-0.212, 3.430], mean observation: -0.159 [-0.521, 0.015], loss: 0.298864, mean_squared_error: 0.597728, mean_q: 0.397076\n",
      " 42739/100000: episode: 215, duration: 3.504s, episode steps: 200, steps per second: 57, episode reward: -65.046, mean reward: -0.325 [-0.851, -0.000], mean action: -1.617 [-2.917, 0.025], mean observation: -0.360 [-0.920, 0.013], loss: 0.271662, mean_squared_error: 0.543324, mean_q: 0.293612\n",
      " 42939/100000: episode: 216, duration: 3.539s, episode steps: 200, steps per second: 57, episode reward: -5.326, mean reward: -0.027 [-0.167, -0.000], mean action: 0.280 [-0.745, 1.294], mean observation: -0.223 [-0.615, 0.021], loss: 0.628206, mean_squared_error: 1.256412, mean_q: 0.513647\n",
      " 43139/100000: episode: 217, duration: 3.513s, episode steps: 200, steps per second: 57, episode reward: -7.399, mean reward: -0.037 [-0.177, -0.000], mean action: 0.521 [-0.098, 1.331], mean observation: -0.209 [-0.580, 0.017], loss: 0.132956, mean_squared_error: 0.265912, mean_q: 0.478047\n",
      " 43339/100000: episode: 218, duration: 3.498s, episode steps: 200, steps per second: 57, episode reward: -27.280, mean reward: -0.136 [-0.438, -0.000], mean action: -1.030 [-2.092, 0.077], mean observation: -0.356 [-1.099, 0.023], loss: 0.111790, mean_squared_error: 0.223579, mean_q: 0.455932\n",
      " 43539/100000: episode: 219, duration: 3.557s, episode steps: 200, steps per second: 56, episode reward: -10.428, mean reward: -0.052 [-0.228, -0.000], mean action: 0.015 [-1.071, 1.509], mean observation: -0.236 [-0.907, 0.078], loss: 0.218953, mean_squared_error: 0.437907, mean_q: 0.534149\n",
      " 43739/100000: episode: 220, duration: 3.543s, episode steps: 200, steps per second: 56, episode reward: -3.133, mean reward: -0.016 [-0.093, -0.000], mean action: -0.167 [-0.964, 0.649], mean observation: -0.280 [-0.715, 0.011], loss: 0.409838, mean_squared_error: 0.819676, mean_q: 0.486561\n",
      " 43939/100000: episode: 221, duration: 3.519s, episode steps: 200, steps per second: 57, episode reward: -25.271, mean reward: -0.126 [-0.638, -0.000], mean action: -0.942 [-2.526, 0.097], mean observation: -0.365 [-1.148, 0.015], loss: 0.074325, mean_squared_error: 0.148651, mean_q: 0.515731\n",
      " 44139/100000: episode: 222, duration: 3.453s, episode steps: 200, steps per second: 58, episode reward: -4.528, mean reward: -0.023 [-0.093, -0.000], mean action: 0.012 [-0.963, 0.893], mean observation: -0.265 [-1.200, 0.071], loss: 0.208737, mean_squared_error: 0.417475, mean_q: 0.634716\n",
      " 44339/100000: episode: 223, duration: 3.485s, episode steps: 200, steps per second: 57, episode reward: -7.950, mean reward: -0.040 [-0.299, -0.000], mean action: -0.462 [-1.729, 0.177], mean observation: -0.301 [-1.132, 0.029], loss: 0.581996, mean_squared_error: 1.163992, mean_q: 0.449344\n",
      " 44539/100000: episode: 224, duration: 3.500s, episode steps: 200, steps per second: 57, episode reward: -1.220, mean reward: -0.006 [-0.026, -0.000], mean action: 0.126 [-0.455, 0.514], mean observation: -0.251 [-0.649, 0.017], loss: 0.147573, mean_squared_error: 0.295146, mean_q: 0.513715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44739/100000: episode: 225, duration: 3.509s, episode steps: 200, steps per second: 57, episode reward: -4.392, mean reward: -0.022 [-0.083, -0.000], mean action: 0.289 [-0.475, 0.911], mean observation: -0.227 [-0.814, 0.027], loss: 0.330012, mean_squared_error: 0.660025, mean_q: 0.494697\n",
      " 44939/100000: episode: 226, duration: 3.462s, episode steps: 200, steps per second: 58, episode reward: -14.297, mean reward: -0.071 [-0.259, -0.000], mean action: 0.404 [-0.909, 1.610], mean observation: -0.226 [-0.715, 0.012], loss: 0.115521, mean_squared_error: 0.231043, mean_q: 0.497920\n",
      " 45139/100000: episode: 227, duration: 3.585s, episode steps: 200, steps per second: 56, episode reward: -6.500, mean reward: -0.033 [-0.150, -0.000], mean action: 0.329 [-0.939, 1.223], mean observation: -0.218 [-0.758, 0.023], loss: 1.234502, mean_squared_error: 2.469003, mean_q: 0.419643\n",
      " 45339/100000: episode: 228, duration: 3.446s, episode steps: 200, steps per second: 58, episode reward: -18.165, mean reward: -0.091 [-0.351, -0.000], mean action: 0.728 [-0.707, 1.873], mean observation: -0.185 [-0.968, 0.041], loss: 0.323791, mean_squared_error: 0.647582, mean_q: 0.403750\n",
      " 45539/100000: episode: 229, duration: 3.640s, episode steps: 200, steps per second: 55, episode reward: -7.104, mean reward: -0.036 [-0.332, -0.000], mean action: 0.397 [-0.417, 1.821], mean observation: -0.218 [-0.579, 0.019], loss: 1.212247, mean_squared_error: 2.424494, mean_q: 0.626865\n",
      " 45739/100000: episode: 230, duration: 3.547s, episode steps: 200, steps per second: 56, episode reward: -27.009, mean reward: -0.135 [-0.461, -0.000], mean action: -0.754 [-2.148, 0.656], mean observation: -0.325 [-1.005, 0.018], loss: 0.935238, mean_squared_error: 1.870477, mean_q: 0.585745\n",
      " 45939/100000: episode: 231, duration: 3.525s, episode steps: 200, steps per second: 57, episode reward: -4.427, mean reward: -0.022 [-0.172, -0.000], mean action: 0.251 [-0.596, 1.313], mean observation: -0.236 [-0.597, 0.013], loss: 0.809602, mean_squared_error: 1.619205, mean_q: 0.543553\n",
      " 46139/100000: episode: 232, duration: 3.482s, episode steps: 200, steps per second: 57, episode reward: -3.766, mean reward: -0.019 [-0.104, -0.000], mean action: 0.307 [-0.358, 1.020], mean observation: -0.232 [-0.622, 0.009], loss: 0.640538, mean_squared_error: 1.281075, mean_q: 0.613865\n",
      " 46339/100000: episode: 233, duration: 3.518s, episode steps: 200, steps per second: 57, episode reward: -40.624, mean reward: -0.203 [-1.037, -0.000], mean action: 0.897 [-0.737, 3.220], mean observation: -0.204 [-0.633, 0.021], loss: 1.205653, mean_squared_error: 2.411305, mean_q: 0.645856\n",
      " 46539/100000: episode: 234, duration: 3.477s, episode steps: 200, steps per second: 58, episode reward: -10.380, mean reward: -0.052 [-0.248, -0.000], mean action: 0.221 [-0.700, 1.574], mean observation: -0.240 [-0.658, 0.008], loss: 1.282739, mean_squared_error: 2.565478, mean_q: 0.489279\n",
      " 46706/100000: episode: 235, duration: 2.966s, episode steps: 167, steps per second: 56, episode reward: 85.436, mean reward: 0.512 [-0.616, 99.526], mean action: 0.658 [-0.311, 2.481], mean observation: -0.191 [-0.869, 0.451], loss: 0.079538, mean_squared_error: 0.159075, mean_q: 0.366524\n",
      " 46906/100000: episode: 236, duration: 3.490s, episode steps: 200, steps per second: 57, episode reward: -3.076, mean reward: -0.015 [-0.131, -0.000], mean action: -0.021 [-1.146, 0.625], mean observation: -0.258 [-0.871, 0.025], loss: 0.097074, mean_squared_error: 0.194148, mean_q: 0.533280\n",
      " 47106/100000: episode: 237, duration: 3.598s, episode steps: 200, steps per second: 56, episode reward: -60.068, mean reward: -0.300 [-0.572, -0.006], mean action: 1.646 [0.242, 2.391], mean observation: -0.160 [-0.544, 0.010], loss: 0.287972, mean_squared_error: 0.575944, mean_q: 0.632562\n",
      " 47306/100000: episode: 238, duration: 3.515s, episode steps: 200, steps per second: 57, episode reward: -7.717, mean reward: -0.039 [-0.165, -0.000], mean action: -0.302 [-1.286, 0.826], mean observation: -0.311 [-1.200, 0.044], loss: 0.256889, mean_squared_error: 0.513777, mean_q: 0.518436\n",
      " 47506/100000: episode: 239, duration: 3.489s, episode steps: 200, steps per second: 57, episode reward: -26.269, mean reward: -0.131 [-0.388, -0.000], mean action: 1.026 [0.036, 1.969], mean observation: -0.174 [-0.553, 0.007], loss: 1.431971, mean_squared_error: 2.863943, mean_q: 0.588857\n",
      " 47642/100000: episode: 240, duration: 2.343s, episode steps: 136, steps per second: 58, episode reward: 83.578, mean reward: 0.615 [-0.360, 99.685], mean action: 0.961 [0.025, 1.896], mean observation: -0.111 [-0.674, 0.453], loss: 0.249614, mean_squared_error: 0.499228, mean_q: 0.495831\n",
      " 47842/100000: episode: 241, duration: 3.430s, episode steps: 200, steps per second: 58, episode reward: -18.846, mean reward: -0.094 [-0.490, -0.000], mean action: 0.805 [-0.393, 2.213], mean observation: -0.179 [-0.643, 0.024], loss: 0.308450, mean_squared_error: 0.616900, mean_q: 0.729387\n",
      " 47961/100000: episode: 242, duration: 2.063s, episode steps: 119, steps per second: 58, episode reward: 83.031, mean reward: 0.698 [-0.701, 99.259], mean action: 0.959 [-0.167, 2.723], mean observation: -0.156 [-0.768, 0.462], loss: 0.566274, mean_squared_error: 1.132547, mean_q: 0.658052\n",
      " 48161/100000: episode: 243, duration: 3.518s, episode steps: 200, steps per second: 57, episode reward: -76.269, mean reward: -0.381 [-1.234, -0.005], mean action: 1.742 [0.212, 3.513], mean observation: -0.159 [-0.542, 0.019], loss: 0.275996, mean_squared_error: 0.551992, mean_q: 0.728645\n",
      " 48361/100000: episode: 244, duration: 3.434s, episode steps: 200, steps per second: 58, episode reward: -4.243, mean reward: -0.021 [-0.241, -0.000], mean action: 0.333 [-0.130, 1.553], mean observation: -0.231 [-0.598, 0.011], loss: 0.200112, mean_squared_error: 0.400223, mean_q: 0.648322\n",
      " 48561/100000: episode: 245, duration: 3.471s, episode steps: 200, steps per second: 58, episode reward: -10.343, mean reward: -0.052 [-0.218, -0.000], mean action: 0.453 [-0.950, 1.476], mean observation: -0.160 [-1.200, 0.237], loss: 0.260933, mean_squared_error: 0.521866, mean_q: 0.809913\n",
      " 48739/100000: episode: 246, duration: 3.073s, episode steps: 178, steps per second: 58, episode reward: 89.992, mean reward: 0.506 [-0.285, 99.968], mean action: -0.220 [-1.689, 1.053], mean observation: -0.287 [-1.200, 0.483], loss: 0.060297, mean_squared_error: 0.120594, mean_q: 0.619837\n",
      " 48939/100000: episode: 247, duration: 3.517s, episode steps: 200, steps per second: 57, episode reward: -34.585, mean reward: -0.173 [-0.575, -0.000], mean action: 1.143 [-0.456, 2.397], mean observation: -0.171 [-1.010, 0.104], loss: 0.129626, mean_squared_error: 0.259251, mean_q: 0.669747\n",
      " 49139/100000: episode: 248, duration: 3.449s, episode steps: 200, steps per second: 58, episode reward: -45.842, mean reward: -0.229 [-0.806, -0.000], mean action: 1.253 [-0.424, 2.839], mean observation: -0.176 [-0.954, 0.037], loss: 0.127246, mean_squared_error: 0.254491, mean_q: 0.836962\n",
      " 49339/100000: episode: 249, duration: 3.449s, episode steps: 200, steps per second: 58, episode reward: -21.613, mean reward: -0.108 [-0.405, -0.000], mean action: 0.855 [-0.398, 2.013], mean observation: -0.123 [-0.810, 0.202], loss: 1.136925, mean_squared_error: 2.273850, mean_q: 0.880286\n",
      " 49481/100000: episode: 250, duration: 2.489s, episode steps: 142, steps per second: 57, episode reward: 93.723, mean reward: 0.660 [-0.237, 99.779], mean action: 0.229 [-0.843, 1.539], mean observation: -0.234 [-1.200, 0.452], loss: 0.608409, mean_squared_error: 1.216818, mean_q: 0.882352\n",
      " 49681/100000: episode: 251, duration: 3.476s, episode steps: 200, steps per second: 58, episode reward: -44.626, mean reward: -0.223 [-0.628, -0.007], mean action: 1.368 [0.270, 2.506], mean observation: -0.158 [-0.563, 0.017], loss: 2.384200, mean_squared_error: 4.768399, mean_q: 0.994173\n",
      " 49805/100000: episode: 252, duration: 2.190s, episode steps: 124, steps per second: 57, episode reward: 83.990, mean reward: 0.677 [-0.642, 99.427], mean action: 0.590 [-0.528, 2.533], mean observation: -0.202 [-0.781, 0.458], loss: 0.305633, mean_squared_error: 0.611266, mean_q: 0.951092\n",
      " 50005/100000: episode: 253, duration: 3.508s, episode steps: 200, steps per second: 57, episode reward: -3.152, mean reward: -0.016 [-0.104, -0.000], mean action: -0.056 [-1.022, 0.684], mean observation: -0.263 [-0.888, 0.030], loss: 0.233721, mean_squared_error: 0.467443, mean_q: 0.890193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 50205/100000: episode: 254, duration: 3.481s, episode steps: 200, steps per second: 57, episode reward: -10.881, mean reward: -0.054 [-0.240, -0.000], mean action: -0.029 [-1.183, 1.550], mean observation: -0.228 [-0.867, 0.275], loss: 0.796180, mean_squared_error: 1.592360, mean_q: 1.046794\n",
      " 50405/100000: episode: 255, duration: 3.461s, episode steps: 200, steps per second: 58, episode reward: -72.808, mean reward: -0.364 [-1.292, -0.000], mean action: 1.604 [-0.094, 3.595], mean observation: -0.161 [-0.554, 0.015], loss: 1.263393, mean_squared_error: 2.526786, mean_q: 0.878263\n",
      " 50605/100000: episode: 256, duration: 3.483s, episode steps: 200, steps per second: 57, episode reward: -34.772, mean reward: -0.174 [-0.686, -0.000], mean action: -1.035 [-2.620, 0.440], mean observation: -0.352 [-0.994, 0.017], loss: 1.884806, mean_squared_error: 3.769612, mean_q: 1.094839\n",
      " 50805/100000: episode: 257, duration: 3.473s, episode steps: 200, steps per second: 58, episode reward: -3.550, mean reward: -0.018 [-0.070, -0.000], mean action: -0.317 [-0.837, 0.392], mean observation: -0.293 [-0.800, 0.016], loss: 0.214752, mean_squared_error: 0.429504, mean_q: 0.994011\n",
      " 51005/100000: episode: 258, duration: 3.555s, episode steps: 200, steps per second: 56, episode reward: -45.868, mean reward: -0.229 [-0.887, -0.000], mean action: 1.271 [-0.541, 2.979], mean observation: -0.182 [-0.991, 0.054], loss: 0.302446, mean_squared_error: 0.604892, mean_q: 1.050693\n",
      " 51107/100000: episode: 259, duration: 1.786s, episode steps: 102, steps per second: 57, episode reward: 93.854, mean reward: 0.920 [-0.201, 99.796], mean action: 0.203 [-1.026, 1.429], mean observation: -0.239 [-0.966, 0.488], loss: 0.055202, mean_squared_error: 0.110405, mean_q: 1.231370\n",
      " 51307/100000: episode: 260, duration: 3.446s, episode steps: 200, steps per second: 58, episode reward: -39.162, mean reward: -0.196 [-0.528, -0.000], mean action: 1.245 [-0.310, 2.297], mean observation: -0.110 [-0.640, 0.212], loss: 0.227216, mean_squared_error: 0.454433, mean_q: 1.073570\n",
      " 51476/100000: episode: 261, duration: 2.943s, episode steps: 169, steps per second: 57, episode reward: 92.439, mean reward: 0.547 [-0.205, 99.785], mean action: -0.130 [-1.320, 1.466], mean observation: -0.296 [-1.200, 0.504], loss: 0.132338, mean_squared_error: 0.264675, mean_q: 1.099622\n",
      " 51676/100000: episode: 262, duration: 3.433s, episode steps: 200, steps per second: 58, episode reward: -22.970, mean reward: -0.115 [-0.333, -0.000], mean action: -0.876 [-1.824, 0.433], mean observation: -0.350 [-0.897, 0.018], loss: 0.604544, mean_squared_error: 1.209087, mean_q: 1.298348\n",
      " 51876/100000: episode: 263, duration: 3.467s, episode steps: 200, steps per second: 58, episode reward: -18.442, mean reward: -0.092 [-0.342, -0.000], mean action: 0.532 [-1.849, 1.571], mean observation: -0.206 [-0.939, 0.036], loss: 0.242788, mean_squared_error: 0.485575, mean_q: 1.126341\n",
      " 52076/100000: episode: 264, duration: 3.454s, episode steps: 200, steps per second: 58, episode reward: -19.867, mean reward: -0.099 [-0.570, -0.000], mean action: 0.690 [-0.711, 2.388], mean observation: -0.189 [-0.745, 0.213], loss: 0.925298, mean_squared_error: 1.850597, mean_q: 1.147191\n",
      " 52276/100000: episode: 265, duration: 3.513s, episode steps: 200, steps per second: 57, episode reward: -8.979, mean reward: -0.045 [-0.302, -0.000], mean action: 0.046 [-1.117, 1.738], mean observation: -0.261 [-1.200, 0.067], loss: 1.582387, mean_squared_error: 3.164773, mean_q: 1.165137\n",
      " 52411/100000: episode: 266, duration: 2.328s, episode steps: 135, steps per second: 58, episode reward: 94.739, mean reward: 0.702 [-0.192, 99.806], mean action: 0.369 [-0.710, 1.393], mean observation: -0.212 [-0.911, 0.465], loss: 0.093283, mean_squared_error: 0.186566, mean_q: 1.145411\n",
      " 52611/100000: episode: 267, duration: 3.476s, episode steps: 200, steps per second: 58, episode reward: -8.673, mean reward: -0.043 [-0.320, -0.000], mean action: -0.418 [-1.789, 0.636], mean observation: -0.303 [-1.101, 0.029], loss: 0.523830, mean_squared_error: 1.047659, mean_q: 1.222126\n",
      " 52811/100000: episode: 268, duration: 3.416s, episode steps: 200, steps per second: 59, episode reward: -43.091, mean reward: -0.215 [-1.080, -0.003], mean action: 1.268 [0.181, 3.286], mean observation: -0.160 [-0.521, 0.017], loss: 1.497520, mean_squared_error: 2.995040, mean_q: 1.433806\n",
      " 52980/100000: episode: 269, duration: 2.943s, episode steps: 169, steps per second: 57, episode reward: 73.867, mean reward: 0.437 [-0.707, 99.803], mean action: 0.731 [-0.897, 2.659], mean observation: -0.209 [-1.200, 0.451], loss: 2.594103, mean_squared_error: 5.188206, mean_q: 1.436657\n",
      " 53135/100000: episode: 270, duration: 2.671s, episode steps: 155, steps per second: 58, episode reward: 82.312, mean reward: 0.531 [-0.607, 99.772], mean action: 0.590 [-1.010, 2.463], mean observation: -0.219 [-1.132, 0.451], loss: 0.449634, mean_squared_error: 0.899269, mean_q: 1.219967\n",
      " 53335/100000: episode: 271, duration: 3.463s, episode steps: 200, steps per second: 58, episode reward: -13.635, mean reward: -0.068 [-0.260, -0.000], mean action: -0.723 [-1.613, 0.181], mean observation: -0.335 [-1.097, 0.033], loss: 1.608083, mean_squared_error: 3.216167, mean_q: 1.383305\n",
      " 53535/100000: episode: 272, duration: 3.527s, episode steps: 200, steps per second: 57, episode reward: -32.411, mean reward: -0.162 [-0.739, -0.000], mean action: 0.852 [-0.928, 2.719], mean observation: -0.203 [-1.200, 0.080], loss: 1.647218, mean_squared_error: 3.294436, mean_q: 1.388886\n",
      " 53735/100000: episode: 273, duration: 3.541s, episode steps: 200, steps per second: 56, episode reward: -27.383, mean reward: -0.137 [-0.449, -0.002], mean action: 1.060 [0.138, 2.120], mean observation: -0.177 [-0.588, 0.021], loss: 0.054778, mean_squared_error: 0.109556, mean_q: 1.093473\n",
      " 53904/100000: episode: 274, duration: 3.041s, episode steps: 169, steps per second: 56, episode reward: 91.763, mean reward: 0.543 [-0.252, 99.889], mean action: 0.164 [-1.588, 1.055], mean observation: -0.268 [-1.181, 0.465], loss: 0.419944, mean_squared_error: 0.839889, mean_q: 1.524373\n",
      " 54104/100000: episode: 275, duration: 3.501s, episode steps: 200, steps per second: 57, episode reward: -23.666, mean reward: -0.118 [-0.425, -0.000], mean action: -0.780 [-2.062, 0.967], mean observation: -0.358 [-1.200, 0.026], loss: 0.468467, mean_squared_error: 0.936934, mean_q: 1.582040\n",
      " 54304/100000: episode: 276, duration: 3.540s, episode steps: 200, steps per second: 56, episode reward: -35.764, mean reward: -0.179 [-0.712, -0.002], mean action: 1.088 [0.144, 2.669], mean observation: -0.184 [-0.516, 0.017], loss: 1.627345, mean_squared_error: 3.254691, mean_q: 1.335401\n",
      " 54504/100000: episode: 277, duration: 3.507s, episode steps: 200, steps per second: 57, episode reward: -84.266, mean reward: -0.421 [-1.529, -0.000], mean action: 1.598 [-0.310, 3.910], mean observation: -0.159 [-0.635, 0.096], loss: 0.989923, mean_squared_error: 1.979846, mean_q: 1.343378\n",
      " 54613/100000: episode: 278, duration: 1.873s, episode steps: 109, steps per second: 58, episode reward: 94.163, mean reward: 0.864 [-0.258, 99.914], mean action: 0.325 [-0.962, 1.607], mean observation: -0.184 [-0.811, 0.450], loss: 1.621765, mean_squared_error: 3.243530, mean_q: 1.722436\n",
      " 54813/100000: episode: 279, duration: 3.457s, episode steps: 200, steps per second: 58, episode reward: -28.921, mean reward: -0.145 [-0.618, -0.000], mean action: 0.589 [-0.998, 2.485], mean observation: -0.205 [-1.200, 0.421], loss: 2.804306, mean_squared_error: 5.608613, mean_q: 1.665469\n",
      " 55013/100000: episode: 280, duration: 3.479s, episode steps: 200, steps per second: 57, episode reward: -46.946, mean reward: -0.235 [-1.078, -0.000], mean action: 1.115 [-0.321, 3.284], mean observation: -0.167 [-0.680, 0.163], loss: 0.224490, mean_squared_error: 0.448980, mean_q: 1.510383\n",
      " 55213/100000: episode: 281, duration: 3.553s, episode steps: 200, steps per second: 56, episode reward: -4.663, mean reward: -0.023 [-0.127, -0.000], mean action: -0.283 [-1.127, 0.679], mean observation: -0.296 [-1.002, 0.027], loss: 0.303567, mean_squared_error: 0.607133, mean_q: 1.741875\n",
      " 55413/100000: episode: 282, duration: 3.459s, episode steps: 200, steps per second: 58, episode reward: -32.638, mean reward: -0.163 [-0.443, -0.000], mean action: 1.102 [-0.307, 2.104], mean observation: -0.189 [-0.901, 0.044], loss: 2.659604, mean_squared_error: 5.319208, mean_q: 1.711612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 55613/100000: episode: 283, duration: 3.511s, episode steps: 200, steps per second: 57, episode reward: -5.967, mean reward: -0.030 [-0.177, -0.000], mean action: 0.320 [-0.395, 1.331], mean observation: -0.223 [-0.674, 0.008], loss: 1.081075, mean_squared_error: 2.162151, mean_q: 1.734318\n",
      " 55813/100000: episode: 284, duration: 3.529s, episode steps: 200, steps per second: 57, episode reward: -22.742, mean reward: -0.114 [-0.709, -0.000], mean action: 0.766 [-0.159, 2.662], mean observation: -0.170 [-0.861, 0.049], loss: 1.540524, mean_squared_error: 3.081049, mean_q: 1.782428\n",
      " 56004/100000: episode: 285, duration: 3.389s, episode steps: 191, steps per second: 56, episode reward: 67.093, mean reward: 0.351 [-0.681, 99.620], mean action: 1.125 [-0.191, 2.609], mean observation: -0.116 [-0.690, 0.461], loss: 1.484334, mean_squared_error: 2.968667, mean_q: 1.842974\n",
      " 56204/100000: episode: 286, duration: 3.475s, episode steps: 200, steps per second: 58, episode reward: -58.949, mean reward: -0.295 [-0.659, -0.000], mean action: 1.603 [0.058, 2.566], mean observation: -0.163 [-0.447, 0.008], loss: 1.060125, mean_squared_error: 2.120250, mean_q: 1.624202\n",
      " 56357/100000: episode: 287, duration: 2.649s, episode steps: 153, steps per second: 58, episode reward: 83.170, mean reward: 0.544 [-0.551, 99.598], mean action: 0.747 [-0.327, 2.348], mean observation: -0.169 [-0.824, 0.451], loss: 0.804961, mean_squared_error: 1.609922, mean_q: 1.864712\n",
      " 56557/100000: episode: 288, duration: 3.476s, episode steps: 200, steps per second: 58, episode reward: -10.005, mean reward: -0.050 [-0.219, -0.000], mean action: 0.098 [-1.480, 1.202], mean observation: -0.267 [-1.200, 0.014], loss: 1.852486, mean_squared_error: 3.704972, mean_q: 1.769393\n",
      " 56757/100000: episode: 289, duration: 3.460s, episode steps: 200, steps per second: 58, episode reward: -12.826, mean reward: -0.064 [-0.381, -0.000], mean action: 0.492 [-0.324, 1.952], mean observation: -0.211 [-0.899, 0.029], loss: 0.568895, mean_squared_error: 1.137790, mean_q: 2.025181\n",
      " 56957/100000: episode: 290, duration: 3.502s, episode steps: 200, steps per second: 57, episode reward: -30.741, mean reward: -0.154 [-0.556, -0.000], mean action: 1.108 [0.036, 2.359], mean observation: -0.173 [-0.498, 0.006], loss: 1.618100, mean_squared_error: 3.236199, mean_q: 1.828306\n",
      " 57157/100000: episode: 291, duration: 3.491s, episode steps: 200, steps per second: 57, episode reward: -20.517, mean reward: -0.103 [-0.582, -0.000], mean action: 0.785 [-0.090, 2.412], mean observation: -0.195 [-0.567, 0.012], loss: 1.155965, mean_squared_error: 2.311931, mean_q: 1.895326\n",
      " 57357/100000: episode: 292, duration: 3.504s, episode steps: 200, steps per second: 57, episode reward: -41.140, mean reward: -0.206 [-0.949, -0.000], mean action: 1.159 [-0.356, 3.080], mean observation: -0.179 [-0.639, 0.013], loss: 1.731491, mean_squared_error: 3.462983, mean_q: 2.090332\n",
      " 57557/100000: episode: 293, duration: 3.526s, episode steps: 200, steps per second: 57, episode reward: -3.925, mean reward: -0.020 [-0.197, -0.000], mean action: -0.024 [-0.947, 1.403], mean observation: -0.269 [-0.874, 0.077], loss: 2.627456, mean_squared_error: 5.254911, mean_q: 1.951439\n",
      " 57757/100000: episode: 294, duration: 3.463s, episode steps: 200, steps per second: 58, episode reward: -24.046, mean reward: -0.120 [-0.461, -0.000], mean action: -0.886 [-2.147, 0.421], mean observation: -0.341 [-0.854, 0.009], loss: 0.375531, mean_squared_error: 0.751061, mean_q: 2.063514\n",
      " 57957/100000: episode: 295, duration: 3.661s, episode steps: 200, steps per second: 55, episode reward: -91.874, mean reward: -0.459 [-1.557, -0.000], mean action: 1.813 [-0.182, 3.946], mean observation: -0.148 [-0.570, 0.019], loss: 0.529132, mean_squared_error: 1.058264, mean_q: 2.069832\n",
      " 58157/100000: episode: 296, duration: 3.663s, episode steps: 200, steps per second: 55, episode reward: -22.104, mean reward: -0.111 [-0.320, -0.000], mean action: -0.939 [-1.789, 0.144], mean observation: -0.349 [-0.801, 0.005], loss: 1.599693, mean_squared_error: 3.199385, mean_q: 2.047633\n",
      " 58357/100000: episode: 297, duration: 3.431s, episode steps: 200, steps per second: 58, episode reward: -21.673, mean reward: -0.108 [-0.557, -0.000], mean action: 0.508 [-1.240, 2.360], mean observation: -0.232 [-1.200, 0.040], loss: 2.224035, mean_squared_error: 4.448071, mean_q: 2.022160\n",
      " 58557/100000: episode: 298, duration: 3.520s, episode steps: 200, steps per second: 57, episode reward: -14.548, mean reward: -0.073 [-0.252, -0.000], mean action: -0.746 [-1.587, 0.331], mean observation: -0.344 [-0.877, 0.021], loss: 2.389751, mean_squared_error: 4.779502, mean_q: 2.048972\n",
      " 58757/100000: episode: 299, duration: 3.279s, episode steps: 200, steps per second: 61, episode reward: -8.965, mean reward: -0.045 [-0.242, -0.000], mean action: -0.467 [-1.554, 0.295], mean observation: -0.320 [-0.975, 0.014], loss: 1.151041, mean_squared_error: 2.302082, mean_q: 1.878915\n",
      " 58957/100000: episode: 300, duration: 3.610s, episode steps: 200, steps per second: 55, episode reward: -7.939, mean reward: -0.040 [-0.136, -0.000], mean action: -0.522 [-1.168, 1.095], mean observation: -0.331 [-0.876, 0.029], loss: 0.485162, mean_squared_error: 0.970324, mean_q: 2.094423\n",
      " 59157/100000: episode: 301, duration: 3.447s, episode steps: 200, steps per second: 58, episode reward: -39.591, mean reward: -0.198 [-1.102, -0.000], mean action: 0.850 [-0.720, 3.319], mean observation: -0.190 [-1.200, 0.204], loss: 2.054463, mean_squared_error: 4.108926, mean_q: 2.096009\n",
      " 59357/100000: episode: 302, duration: 3.412s, episode steps: 200, steps per second: 59, episode reward: -22.214, mean reward: -0.111 [-0.979, -0.000], mean action: 0.714 [-0.374, 3.128], mean observation: -0.210 [-0.606, 0.023], loss: 3.242280, mean_squared_error: 6.484560, mean_q: 2.122122\n",
      " 59557/100000: episode: 303, duration: 3.449s, episode steps: 200, steps per second: 58, episode reward: -13.071, mean reward: -0.065 [-0.228, -0.000], mean action: -0.289 [-1.508, 1.347], mean observation: -0.291 [-1.200, 0.074], loss: 3.376231, mean_squared_error: 6.752462, mean_q: 2.264344\n",
      " 59757/100000: episode: 304, duration: 3.656s, episode steps: 200, steps per second: 55, episode reward: -51.968, mean reward: -0.260 [-0.826, -0.000], mean action: -1.348 [-2.874, 0.085], mean observation: -0.364 [-1.120, 0.023], loss: 0.891565, mean_squared_error: 1.783130, mean_q: 1.989675\n",
      " 59910/100000: episode: 305, duration: 2.596s, episode steps: 153, steps per second: 59, episode reward: 59.032, mean reward: 0.386 [-0.948, 99.086], mean action: 1.326 [-0.332, 3.079], mean observation: -0.145 [-0.726, 0.456], loss: 4.319035, mean_squared_error: 8.638070, mean_q: 2.064149\n",
      " 60099/100000: episode: 306, duration: 3.594s, episode steps: 189, steps per second: 53, episode reward: 67.688, mean reward: 0.358 [-0.681, 99.442], mean action: 0.378 [-1.352, 2.609], mean observation: -0.168 [-0.800, 0.460], loss: 2.430199, mean_squared_error: 4.860398, mean_q: 2.378129\n",
      " 60281/100000: episode: 307, duration: 3.238s, episode steps: 182, steps per second: 56, episode reward: 80.456, mean reward: 0.442 [-0.717, 99.828], mean action: 0.536 [-0.663, 2.678], mean observation: -0.239 [-1.167, 0.489], loss: 1.698175, mean_squared_error: 3.396350, mean_q: 2.056922\n",
      " 60460/100000: episode: 308, duration: 3.215s, episode steps: 179, steps per second: 56, episode reward: 81.050, mean reward: 0.453 [-0.512, 99.958], mean action: -0.599 [-2.262, 0.808], mean observation: -0.295 [-1.200, 0.460], loss: 1.106453, mean_squared_error: 2.212907, mean_q: 2.472618\n",
      " 60658/100000: episode: 309, duration: 3.677s, episode steps: 198, steps per second: 54, episode reward: 84.417, mean reward: 0.426 [-0.356, 99.736], mean action: 0.492 [-1.090, 1.886], mean observation: -0.163 [-1.200, 0.484], loss: 2.876956, mean_squared_error: 5.753912, mean_q: 2.716582\n",
      " 60858/100000: episode: 310, duration: 3.641s, episode steps: 200, steps per second: 55, episode reward: -38.126, mean reward: -0.191 [-0.885, -0.000], mean action: -0.604 [-2.975, 1.509], mean observation: -0.242 [-1.200, 0.188], loss: 3.208234, mean_squared_error: 6.416467, mean_q: 2.167701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 61054/100000: episode: 311, duration: 3.557s, episode steps: 196, steps per second: 55, episode reward: 68.052, mean reward: 0.347 [-0.953, 99.148], mean action: -0.059 [-1.602, 3.087], mean observation: -0.254 [-0.814, 0.468], loss: 1.459585, mean_squared_error: 2.919170, mean_q: 2.652586\n",
      " 61254/100000: episode: 312, duration: 4.046s, episode steps: 200, steps per second: 49, episode reward: -12.513, mean reward: -0.063 [-0.502, -0.000], mean action: 0.037 [-0.959, 2.240], mean observation: -0.265 [-0.747, 0.127], loss: 2.047555, mean_squared_error: 4.095111, mean_q: 2.362161\n",
      " 61454/100000: episode: 313, duration: 3.623s, episode steps: 200, steps per second: 55, episode reward: -9.826, mean reward: -0.049 [-0.224, -0.000], mean action: -0.145 [-1.496, 1.273], mean observation: -0.260 [-1.200, 0.082], loss: 1.474552, mean_squared_error: 2.949103, mean_q: 2.753770\n",
      " 61644/100000: episode: 314, duration: 3.591s, episode steps: 190, steps per second: 53, episode reward: 93.090, mean reward: 0.490 [-0.211, 99.897], mean action: -0.166 [-1.452, 1.158], mean observation: -0.279 [-1.045, 0.482], loss: 2.309469, mean_squared_error: 4.618937, mean_q: 2.734770\n",
      " 61797/100000: episode: 315, duration: 2.943s, episode steps: 153, steps per second: 52, episode reward: 84.955, mean reward: 0.555 [-0.463, 99.557], mean action: 0.670 [-0.439, 2.151], mean observation: -0.205 [-0.936, 0.471], loss: 2.698351, mean_squared_error: 5.396702, mean_q: 2.600749\n",
      " 61997/100000: episode: 316, duration: 4.041s, episode steps: 200, steps per second: 49, episode reward: -37.521, mean reward: -0.188 [-0.869, -0.000], mean action: -0.925 [-2.948, 0.756], mean observation: -0.315 [-1.200, 0.026], loss: 2.802148, mean_squared_error: 5.604297, mean_q: 2.932250\n",
      " 62197/100000: episode: 317, duration: 3.620s, episode steps: 200, steps per second: 55, episode reward: -30.036, mean reward: -0.150 [-0.528, -0.000], mean action: 0.739 [-0.764, 2.297], mean observation: -0.178 [-1.147, 0.249], loss: 0.973438, mean_squared_error: 1.946876, mean_q: 2.805817\n",
      " 62390/100000: episode: 318, duration: 3.587s, episode steps: 193, steps per second: 54, episode reward: 58.515, mean reward: 0.303 [-0.903, 99.312], mean action: 0.945 [-0.760, 3.004], mean observation: -0.163 [-0.855, 0.478], loss: 0.175294, mean_squared_error: 0.350588, mean_q: 2.976569\n",
      " 62552/100000: episode: 319, duration: 3.106s, episode steps: 162, steps per second: 52, episode reward: 93.294, mean reward: 0.576 [-0.285, 99.977], mean action: 0.005 [-1.689, 1.099], mean observation: -0.245 [-1.200, 0.452], loss: 3.749832, mean_squared_error: 7.499663, mean_q: 3.046669\n",
      " 62752/100000: episode: 320, duration: 3.661s, episode steps: 200, steps per second: 55, episode reward: -9.433, mean reward: -0.047 [-0.376, -0.000], mean action: -0.025 [-1.939, 1.239], mean observation: -0.194 [-1.200, 0.447], loss: 1.778957, mean_squared_error: 3.557914, mean_q: 2.872436\n",
      " 62952/100000: episode: 321, duration: 3.746s, episode steps: 200, steps per second: 53, episode reward: -10.681, mean reward: -0.053 [-0.185, -0.000], mean action: -0.595 [-1.361, 0.308], mean observation: -0.318 [-0.826, 0.008], loss: 1.372575, mean_squared_error: 2.745151, mean_q: 2.980295\n",
      " 63152/100000: episode: 322, duration: 3.714s, episode steps: 200, steps per second: 54, episode reward: -19.416, mean reward: -0.097 [-0.636, -0.000], mean action: 0.577 [-0.491, 2.523], mean observation: -0.224 [-0.816, 0.022], loss: 1.689353, mean_squared_error: 3.378707, mean_q: 3.352811\n",
      " 63302/100000: episode: 323, duration: 2.711s, episode steps: 150, steps per second: 55, episode reward: 85.157, mean reward: 0.568 [-0.426, 99.543], mean action: 0.707 [-0.263, 2.137], mean observation: -0.208 [-0.989, 0.462], loss: 0.743630, mean_squared_error: 1.487260, mean_q: 3.227095\n",
      " 63484/100000: episode: 324, duration: 3.423s, episode steps: 182, steps per second: 53, episode reward: 88.558, mean reward: 0.487 [-0.322, 99.678], mean action: 0.299 [-1.097, 1.795], mean observation: -0.235 [-1.200, 0.492], loss: 0.693709, mean_squared_error: 1.387418, mean_q: 3.373169\n",
      " 63684/100000: episode: 325, duration: 3.670s, episode steps: 200, steps per second: 55, episode reward: -63.362, mean reward: -0.317 [-1.229, -0.000], mean action: 1.298 [-0.411, 3.506], mean observation: -0.107 [-0.652, 0.438], loss: 3.740927, mean_squared_error: 7.481853, mean_q: 3.553110\n",
      " 63821/100000: episode: 326, duration: 2.505s, episode steps: 137, steps per second: 55, episode reward: 93.375, mean reward: 0.682 [-0.188, 99.815], mean action: 0.212 [-0.978, 1.371], mean observation: -0.215 [-0.876, 0.464], loss: 5.299708, mean_squared_error: 10.599416, mean_q: 3.242698\n",
      " 64013/100000: episode: 327, duration: 3.551s, episode steps: 192, steps per second: 54, episode reward: 43.538, mean reward: 0.227 [-1.018, 99.501], mean action: 1.416 [-0.279, 3.190], mean observation: -0.119 [-0.682, 0.452], loss: 0.692893, mean_squared_error: 1.385786, mean_q: 3.410701\n",
      " 64164/100000: episode: 328, duration: 2.770s, episode steps: 151, steps per second: 55, episode reward: 66.722, mean reward: 0.442 [-0.972, 99.480], mean action: 1.103 [-0.448, 3.118], mean observation: -0.169 [-0.881, 0.484], loss: 0.745363, mean_squared_error: 1.490726, mean_q: 3.862123\n",
      " 64271/100000: episode: 329, duration: 1.934s, episode steps: 107, steps per second: 55, episode reward: 89.889, mean reward: 0.840 [-0.284, 99.909], mean action: 0.613 [-0.427, 1.684], mean observation: -0.140 [-0.701, 0.458], loss: 0.745187, mean_squared_error: 1.490373, mean_q: 3.593254\n",
      " 64412/100000: episode: 330, duration: 2.481s, episode steps: 141, steps per second: 57, episode reward: 93.780, mean reward: 0.665 [-0.139, 99.934], mean action: 0.222 [-1.117, 1.177], mean observation: -0.189 [-0.814, 0.466], loss: 0.322438, mean_squared_error: 0.644877, mean_q: 3.867794\n",
      " 64532/100000: episode: 331, duration: 2.144s, episode steps: 120, steps per second: 56, episode reward: 84.780, mean reward: 0.707 [-0.508, 99.629], mean action: 0.592 [-0.850, 2.254], mean observation: -0.198 [-0.752, 0.457], loss: 1.655367, mean_squared_error: 3.310733, mean_q: 3.812180\n",
      " 64732/100000: episode: 332, duration: 3.715s, episode steps: 200, steps per second: 54, episode reward: -40.987, mean reward: -0.205 [-0.599, -0.001], mean action: 1.242 [0.091, 2.448], mean observation: -0.143 [-0.804, 0.161], loss: 1.810310, mean_squared_error: 3.620619, mean_q: 3.854553\n",
      " 64845/100000: episode: 333, duration: 2.080s, episode steps: 113, steps per second: 54, episode reward: 93.552, mean reward: 0.828 [-0.238, 99.985], mean action: 0.120 [-1.298, 1.542], mean observation: -0.225 [-1.200, 0.453], loss: 2.909973, mean_squared_error: 5.819945, mean_q: 3.530293\n",
      " 64963/100000: episode: 334, duration: 2.163s, episode steps: 118, steps per second: 55, episode reward: 88.421, mean reward: 0.749 [-0.446, 99.926], mean action: 0.201 [-1.544, 2.113], mean observation: -0.204 [-1.200, 0.458], loss: 1.130681, mean_squared_error: 2.261361, mean_q: 3.950202\n",
      " 65088/100000: episode: 335, duration: 2.337s, episode steps: 125, steps per second: 53, episode reward: 89.456, mean reward: 0.716 [-0.469, 99.929], mean action: 0.609 [-0.503, 2.165], mean observation: -0.213 [-1.200, 0.471], loss: 3.621519, mean_squared_error: 7.243038, mean_q: 4.103079\n",
      " 65288/100000: episode: 336, duration: 3.754s, episode steps: 200, steps per second: 53, episode reward: -23.350, mean reward: -0.117 [-0.467, -0.000], mean action: -0.891 [-2.162, 0.367], mean observation: -0.340 [-1.162, 0.043], loss: 2.048998, mean_squared_error: 4.097996, mean_q: 4.516177\n",
      " 65366/100000: episode: 337, duration: 1.461s, episode steps: 78, steps per second: 53, episode reward: 89.821, mean reward: 1.152 [-0.391, 99.894], mean action: 0.596 [-0.847, 1.978], mean observation: -0.193 [-0.798, 0.467], loss: 0.367800, mean_squared_error: 0.735599, mean_q: 3.832696\n",
      " 65566/100000: episode: 338, duration: 3.621s, episode steps: 200, steps per second: 55, episode reward: -55.482, mean reward: -0.277 [-0.964, -0.000], mean action: 1.354 [-0.226, 3.104], mean observation: -0.152 [-0.859, 0.030], loss: 0.368350, mean_squared_error: 0.736701, mean_q: 4.235631\n",
      " 65766/100000: episode: 339, duration: 3.618s, episode steps: 200, steps per second: 55, episode reward: -20.164, mean reward: -0.101 [-0.612, -0.000], mean action: 0.359 [-1.480, 2.473], mean observation: -0.209 [-1.200, 0.310], loss: 1.193779, mean_squared_error: 2.387558, mean_q: 4.429954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 65935/100000: episode: 340, duration: 3.048s, episode steps: 169, steps per second: 55, episode reward: 66.136, mean reward: 0.391 [-0.772, 99.949], mean action: 0.946 [-0.517, 2.778], mean observation: -0.191 [-1.200, 0.475], loss: 1.835186, mean_squared_error: 3.670372, mean_q: 4.496558\n",
      " 66043/100000: episode: 341, duration: 1.936s, episode steps: 108, steps per second: 56, episode reward: 67.328, mean reward: 0.623 [-0.943, 99.756], mean action: 1.351 [-0.244, 3.070], mean observation: -0.086 [-0.663, 0.464], loss: 1.487110, mean_squared_error: 2.974220, mean_q: 4.379243\n",
      " 66170/100000: episode: 342, duration: 2.272s, episode steps: 127, steps per second: 56, episode reward: 87.825, mean reward: 0.692 [-0.412, 99.884], mean action: 0.655 [-0.677, 2.030], mean observation: -0.199 [-1.200, 0.490], loss: 1.286958, mean_squared_error: 2.573915, mean_q: 4.776258\n",
      " 66320/100000: episode: 343, duration: 2.692s, episode steps: 150, steps per second: 56, episode reward: 71.865, mean reward: 0.479 [-1.189, 99.985], mean action: 0.754 [-0.683, 3.448], mean observation: -0.237 [-1.194, 0.480], loss: 2.442517, mean_squared_error: 4.885033, mean_q: 4.614927\n",
      " 66510/100000: episode: 344, duration: 3.404s, episode steps: 190, steps per second: 56, episode reward: 65.314, mean reward: 0.344 [-0.652, 99.442], mean action: 0.906 [-0.882, 2.553], mean observation: -0.155 [-0.888, 0.457], loss: 0.722575, mean_squared_error: 1.445150, mean_q: 4.927232\n",
      " 66710/100000: episode: 345, duration: 3.548s, episode steps: 200, steps per second: 56, episode reward: -50.485, mean reward: -0.252 [-0.791, -0.000], mean action: 1.277 [-0.597, 2.812], mean observation: -0.104 [-0.643, 0.226], loss: 0.274519, mean_squared_error: 0.549037, mean_q: 5.257953\n",
      " 66860/100000: episode: 346, duration: 2.711s, episode steps: 150, steps per second: 55, episode reward: 83.668, mean reward: 0.558 [-0.377, 99.866], mean action: 0.650 [-0.833, 1.942], mean observation: -0.200 [-1.200, 0.490], loss: 1.985967, mean_squared_error: 3.971934, mean_q: 5.071352\n",
      " 66998/100000: episode: 347, duration: 2.508s, episode steps: 138, steps per second: 55, episode reward: 87.227, mean reward: 0.632 [-0.821, 99.900], mean action: 0.322 [-0.898, 2.865], mean observation: -0.164 [-1.200, 0.454], loss: 4.266684, mean_squared_error: 8.533368, mean_q: 5.626549\n",
      " 67138/100000: episode: 348, duration: 2.531s, episode steps: 140, steps per second: 55, episode reward: 78.812, mean reward: 0.563 [-0.769, 99.716], mean action: 0.886 [-0.497, 2.773], mean observation: -0.180 [-0.868, 0.466], loss: 0.924436, mean_squared_error: 1.848872, mean_q: 5.520927\n",
      " 67319/100000: episode: 349, duration: 3.111s, episode steps: 181, steps per second: 58, episode reward: 35.915, mean reward: 0.198 [-2.218, 99.533], mean action: 1.420 [-0.582, 4.710], mean observation: -0.138 [-0.760, 0.478], loss: 1.296248, mean_squared_error: 2.592496, mean_q: 5.227584\n",
      " 67410/100000: episode: 350, duration: 1.633s, episode steps: 91, steps per second: 56, episode reward: 94.078, mean reward: 1.034 [-0.267, 99.779], mean action: 0.328 [-0.743, 1.635], mean observation: -0.183 [-0.811, 0.455], loss: 1.738993, mean_squared_error: 3.477986, mean_q: 5.467622\n",
      " 67610/100000: episode: 351, duration: 3.567s, episode steps: 200, steps per second: 56, episode reward: -16.041, mean reward: -0.080 [-0.308, -0.000], mean action: 0.091 [-1.755, 1.752], mean observation: -0.180 [-1.200, 0.220], loss: 0.636683, mean_squared_error: 1.273365, mean_q: 5.354061\n",
      " 67701/100000: episode: 352, duration: 1.600s, episode steps: 91, steps per second: 57, episode reward: 88.534, mean reward: 0.973 [-0.501, 99.859], mean action: 0.745 [-0.508, 2.238], mean observation: -0.143 [-0.700, 0.456], loss: 2.567675, mean_squared_error: 5.135349, mean_q: 6.075344\n",
      " 67855/100000: episode: 353, duration: 2.720s, episode steps: 154, steps per second: 57, episode reward: 84.379, mean reward: 0.548 [-0.599, 99.879], mean action: 0.533 [-1.017, 2.448], mean observation: -0.230 [-1.094, 0.474], loss: 1.803839, mean_squared_error: 3.607678, mean_q: 6.014267\n",
      " 68027/100000: episode: 354, duration: 3.053s, episode steps: 172, steps per second: 56, episode reward: 89.597, mean reward: 0.521 [-0.322, 99.990], mean action: 0.105 [-1.157, 1.793], mean observation: -0.213 [-1.200, 0.462], loss: 3.199404, mean_squared_error: 6.398808, mean_q: 5.598298\n",
      " 68150/100000: episode: 355, duration: 2.260s, episode steps: 123, steps per second: 54, episode reward: 71.933, mean reward: 0.585 [-1.125, 99.938], mean action: 1.008 [-0.541, 3.354], mean observation: -0.209 [-1.147, 0.495], loss: 2.203860, mean_squared_error: 4.407719, mean_q: 6.133721\n",
      " 68314/100000: episode: 356, duration: 2.904s, episode steps: 164, steps per second: 56, episode reward: 74.209, mean reward: 0.452 [-0.752, 99.603], mean action: 0.930 [-0.322, 2.742], mean observation: -0.184 [-1.004, 0.470], loss: 2.350725, mean_squared_error: 4.701451, mean_q: 6.251796\n",
      " 68498/100000: episode: 357, duration: 3.280s, episode steps: 184, steps per second: 56, episode reward: 53.157, mean reward: 0.289 [-1.077, 99.934], mean action: 1.249 [-0.480, 3.282], mean observation: -0.131 [-0.708, 0.464], loss: 0.418082, mean_squared_error: 0.836165, mean_q: 5.932325\n",
      " 68614/100000: episode: 358, duration: 2.080s, episode steps: 116, steps per second: 56, episode reward: 90.820, mean reward: 0.783 [-0.384, 99.976], mean action: 0.242 [-1.303, 1.960], mean observation: -0.185 [-1.200, 0.456], loss: 0.532687, mean_squared_error: 1.065374, mean_q: 6.585710\n",
      " 68772/100000: episode: 359, duration: 2.788s, episode steps: 158, steps per second: 57, episode reward: 88.698, mean reward: 0.561 [-0.541, 99.922], mean action: 0.466 [-0.558, 2.326], mean observation: -0.200 [-1.200, 0.480], loss: 5.162337, mean_squared_error: 10.324675, mean_q: 6.195818\n",
      " 68972/100000: episode: 360, duration: 3.585s, episode steps: 200, steps per second: 56, episode reward: -17.898, mean reward: -0.089 [-0.387, -0.000], mean action: -0.384 [-1.966, 1.915], mean observation: -0.259 [-1.200, 0.062], loss: 1.017017, mean_squared_error: 2.034034, mean_q: 6.785252\n",
      " 69107/100000: episode: 361, duration: 2.445s, episode steps: 135, steps per second: 55, episode reward: 88.643, mean reward: 0.657 [-0.346, 99.981], mean action: 0.524 [-0.651, 1.860], mean observation: -0.157 [-1.200, 0.451], loss: 1.218445, mean_squared_error: 2.436889, mean_q: 7.261436\n",
      " 69205/100000: episode: 362, duration: 1.713s, episode steps: 98, steps per second: 57, episode reward: 91.387, mean reward: 0.933 [-0.400, 99.864], mean action: 0.596 [-0.583, 1.999], mean observation: -0.130 [-0.730, 0.452], loss: 0.365687, mean_squared_error: 0.731374, mean_q: 6.978145\n",
      " 69405/100000: episode: 363, duration: 3.553s, episode steps: 200, steps per second: 56, episode reward: -23.876, mean reward: -0.119 [-0.563, -0.000], mean action: 0.295 [-2.069, 2.373], mean observation: -0.214 [-1.200, 0.200], loss: 1.933555, mean_squared_error: 3.867109, mean_q: 6.940086\n",
      " 69584/100000: episode: 364, duration: 3.257s, episode steps: 179, steps per second: 55, episode reward: 67.988, mean reward: 0.380 [-0.593, 99.649], mean action: 1.184 [-0.105, 2.436], mean observation: -0.113 [-0.680, 0.454], loss: 2.505760, mean_squared_error: 5.011520, mean_q: 6.829657\n",
      " 69784/100000: episode: 365, duration: 3.588s, episode steps: 200, steps per second: 56, episode reward: -13.289, mean reward: -0.066 [-0.372, -0.000], mean action: -0.014 [-1.499, 1.930], mean observation: -0.185 [-1.200, 0.243], loss: 2.275629, mean_squared_error: 4.551258, mean_q: 6.975365\n",
      " 69902/100000: episode: 366, duration: 2.075s, episode steps: 118, steps per second: 57, episode reward: 85.359, mean reward: 0.723 [-0.711, 99.891], mean action: 0.738 [-0.521, 2.666], mean observation: -0.211 [-1.139, 0.466], loss: 0.419721, mean_squared_error: 0.839441, mean_q: 7.661906\n",
      " 70024/100000: episode: 367, duration: 2.133s, episode steps: 122, steps per second: 57, episode reward: 78.250, mean reward: 0.641 [-0.615, 99.621], mean action: 1.092 [-0.178, 2.480], mean observation: -0.176 [-0.894, 0.454], loss: 2.011144, mean_squared_error: 4.022288, mean_q: 7.183054\n",
      " 70172/100000: episode: 368, duration: 2.648s, episode steps: 148, steps per second: 56, episode reward: 82.908, mean reward: 0.560 [-0.809, 99.885], mean action: 0.461 [-0.911, 2.844], mean observation: -0.221 [-1.200, 0.461], loss: 6.211252, mean_squared_error: 12.422504, mean_q: 7.920623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 70306/100000: episode: 369, duration: 2.395s, episode steps: 134, steps per second: 56, episode reward: 62.154, mean reward: 0.464 [-1.429, 98.847], mean action: 1.412 [0.143, 3.781], mean observation: -0.120 [-0.761, 0.455], loss: 1.567400, mean_squared_error: 3.134800, mean_q: 7.246260\n",
      " 70454/100000: episode: 370, duration: 2.636s, episode steps: 148, steps per second: 56, episode reward: 83.595, mean reward: 0.565 [-0.668, 99.916], mean action: 0.820 [-0.424, 2.584], mean observation: -0.088 [-0.842, 0.453], loss: 1.355374, mean_squared_error: 2.710748, mean_q: 7.504761\n",
      " 70572/100000: episode: 371, duration: 2.064s, episode steps: 118, steps per second: 57, episode reward: 84.502, mean reward: 0.716 [-0.368, 99.945], mean action: 0.952 [-0.500, 1.917], mean observation: -0.166 [-0.968, 0.478], loss: 2.830022, mean_squared_error: 5.660045, mean_q: 7.924077\n",
      " 70685/100000: episode: 372, duration: 2.033s, episode steps: 113, steps per second: 56, episode reward: 80.979, mean reward: 0.717 [-0.554, 99.806], mean action: 1.002 [-0.294, 2.353], mean observation: -0.182 [-0.948, 0.482], loss: 2.476012, mean_squared_error: 4.952025, mean_q: 8.108544\n",
      " 70812/100000: episode: 373, duration: 2.297s, episode steps: 127, steps per second: 55, episode reward: 88.626, mean reward: 0.698 [-0.371, 99.637], mean action: 0.544 [-0.771, 1.927], mean observation: -0.190 [-1.109, 0.463], loss: 1.152444, mean_squared_error: 2.304887, mean_q: 7.737634\n",
      " 71012/100000: episode: 374, duration: 3.555s, episode steps: 200, steps per second: 56, episode reward: -95.569, mean reward: -0.478 [-1.105, -0.000], mean action: 1.998 [-0.370, 3.324], mean observation: -0.132 [-0.573, 0.035], loss: 3.951688, mean_squared_error: 7.903377, mean_q: 8.549136\n",
      " 71132/100000: episode: 375, duration: 2.100s, episode steps: 120, steps per second: 57, episode reward: 83.404, mean reward: 0.695 [-0.514, 99.839], mean action: 0.627 [-1.219, 2.267], mean observation: -0.174 [-1.200, 0.470], loss: 1.582229, mean_squared_error: 3.164458, mean_q: 8.313411\n",
      " 71332/100000: episode: 376, duration: 3.497s, episode steps: 200, steps per second: 57, episode reward: -10.981, mean reward: -0.055 [-0.262, -0.000], mean action: -0.190 [-1.484, 1.619], mean observation: -0.284 [-1.200, 0.050], loss: 1.753237, mean_squared_error: 3.506474, mean_q: 7.990525\n",
      " 71448/100000: episode: 377, duration: 2.168s, episode steps: 116, steps per second: 53, episode reward: 84.139, mean reward: 0.725 [-0.424, 99.711], mean action: 0.950 [-0.212, 2.058], mean observation: -0.177 [-0.870, 0.476], loss: 2.087512, mean_squared_error: 4.175024, mean_q: 8.833143\n",
      " 71563/100000: episode: 378, duration: 2.079s, episode steps: 115, steps per second: 55, episode reward: 92.146, mean reward: 0.801 [-0.333, 99.923], mean action: 0.394 [-0.800, 1.826], mean observation: -0.190 [-1.028, 0.469], loss: 0.394961, mean_squared_error: 0.789922, mean_q: 8.477357\n",
      " 71681/100000: episode: 379, duration: 2.068s, episode steps: 118, steps per second: 57, episode reward: 80.244, mean reward: 0.680 [-0.621, 99.772], mean action: 1.105 [-0.203, 2.492], mean observation: -0.149 [-0.845, 0.486], loss: 1.694661, mean_squared_error: 3.389323, mean_q: 9.678306\n",
      " 71781/100000: episode: 380, duration: 1.848s, episode steps: 100, steps per second: 54, episode reward: 88.917, mean reward: 0.889 [-0.313, 99.926], mean action: 0.447 [-1.167, 1.770], mean observation: -0.218 [-1.200, 0.487], loss: 5.315573, mean_squared_error: 10.631145, mean_q: 9.299851\n",
      " 71888/100000: episode: 381, duration: 1.862s, episode steps: 107, steps per second: 57, episode reward: 92.094, mean reward: 0.861 [-0.215, 99.840], mean action: 0.552 [-0.578, 1.467], mean observation: -0.209 [-0.976, 0.453], loss: 1.854922, mean_squared_error: 3.709845, mean_q: 9.064105\n",
      " 71999/100000: episode: 382, duration: 1.987s, episode steps: 111, steps per second: 56, episode reward: 84.221, mean reward: 0.759 [-0.653, 99.608], mean action: 0.849 [-0.426, 2.555], mean observation: -0.186 [-0.943, 0.464], loss: 2.978101, mean_squared_error: 5.956202, mean_q: 9.052732\n",
      " 72114/100000: episode: 383, duration: 2.041s, episode steps: 115, steps per second: 56, episode reward: 74.988, mean reward: 0.652 [-0.671, 99.797], mean action: 1.228 [-0.122, 2.590], mean observation: -0.168 [-0.923, 0.487], loss: 1.060542, mean_squared_error: 2.121084, mean_q: 9.639451\n",
      " 72238/100000: episode: 384, duration: 2.201s, episode steps: 124, steps per second: 56, episode reward: 71.658, mean reward: 0.578 [-1.009, 99.703], mean action: 1.324 [0.139, 3.176], mean observation: -0.133 [-0.750, 0.472], loss: 4.410036, mean_squared_error: 8.820072, mean_q: 9.962428\n",
      " 72438/100000: episode: 385, duration: 3.482s, episode steps: 200, steps per second: 57, episode reward: -55.651, mean reward: -0.278 [-1.379, -0.000], mean action: 1.428 [-0.017, 3.713], mean observation: -0.143 [-0.983, 0.210], loss: 2.520736, mean_squared_error: 5.041472, mean_q: 9.765575\n",
      " 72545/100000: episode: 386, duration: 1.930s, episode steps: 107, steps per second: 55, episode reward: 71.226, mean reward: 0.666 [-1.046, 99.040], mean action: 1.209 [-0.446, 3.234], mean observation: -0.194 [-0.999, 0.464], loss: 3.151406, mean_squared_error: 6.302811, mean_q: 9.312489\n",
      " 72663/100000: episode: 387, duration: 2.092s, episode steps: 118, steps per second: 56, episode reward: 80.084, mean reward: 0.679 [-0.612, 99.948], mean action: 1.048 [-0.007, 2.474], mean observation: -0.159 [-0.880, 0.452], loss: 3.347882, mean_squared_error: 6.695765, mean_q: 9.672619\n",
      " 72807/100000: episode: 388, duration: 2.549s, episode steps: 144, steps per second: 56, episode reward: 83.224, mean reward: 0.578 [-0.351, 99.934], mean action: 0.959 [0.018, 1.872], mean observation: -0.093 [-0.711, 0.453], loss: 0.786961, mean_squared_error: 1.573922, mean_q: 10.373406\n",
      " 72913/100000: episode: 389, duration: 1.897s, episode steps: 106, steps per second: 56, episode reward: 90.298, mean reward: 0.852 [-0.347, 99.925], mean action: 0.688 [-0.945, 1.863], mean observation: -0.167 [-0.840, 0.460], loss: 1.938097, mean_squared_error: 3.876195, mean_q: 10.083376\n",
      " 73028/100000: episode: 390, duration: 2.053s, episode steps: 115, steps per second: 56, episode reward: 75.605, mean reward: 0.657 [-0.687, 99.555], mean action: 1.234 [-0.060, 2.620], mean observation: -0.164 [-0.889, 0.458], loss: 2.118650, mean_squared_error: 4.237299, mean_q: 10.442445\n",
      " 73145/100000: episode: 391, duration: 2.065s, episode steps: 117, steps per second: 57, episode reward: 80.825, mean reward: 0.691 [-0.505, 99.827], mean action: 1.122 [-0.264, 2.247], mean observation: -0.138 [-0.731, 0.451], loss: 4.004593, mean_squared_error: 8.009187, mean_q: 10.246797\n",
      " 73345/100000: episode: 392, duration: 3.576s, episode steps: 200, steps per second: 56, episode reward: -15.253, mean reward: -0.076 [-0.360, -0.000], mean action: 0.260 [-1.340, 1.898], mean observation: -0.214 [-1.200, 0.385], loss: 0.626640, mean_squared_error: 1.253280, mean_q: 10.588565\n",
      " 73545/100000: episode: 393, duration: 3.549s, episode steps: 200, steps per second: 56, episode reward: -17.257, mean reward: -0.086 [-0.308, -0.000], mean action: -0.045 [-1.601, 1.754], mean observation: -0.222 [-1.200, 0.208], loss: 1.599206, mean_squared_error: 3.198412, mean_q: 10.643959\n",
      " 73712/100000: episode: 394, duration: 2.998s, episode steps: 167, steps per second: 56, episode reward: 92.147, mean reward: 0.552 [-0.229, 99.989], mean action: -0.008 [-1.513, 1.288], mean observation: -0.218 [-1.200, 0.453], loss: 2.222205, mean_squared_error: 4.444410, mean_q: 11.105645\n",
      " 73853/100000: episode: 395, duration: 2.499s, episode steps: 141, steps per second: 56, episode reward: 59.181, mean reward: 0.420 [-0.692, 99.656], mean action: 1.585 [0.374, 2.630], mean observation: -0.098 [-0.708, 0.456], loss: 2.413788, mean_squared_error: 4.827576, mean_q: 11.322645\n",
      " 73966/100000: episode: 396, duration: 2.022s, episode steps: 113, steps per second: 56, episode reward: 90.672, mean reward: 0.802 [-0.281, 99.967], mean action: 0.534 [-0.811, 1.677], mean observation: -0.208 [-1.066, 0.454], loss: 1.481275, mean_squared_error: 2.962550, mean_q: 10.946439\n",
      " 74079/100000: episode: 397, duration: 1.987s, episode steps: 113, steps per second: 57, episode reward: 92.093, mean reward: 0.815 [-0.223, 99.983], mean action: 0.562 [-0.586, 1.492], mean observation: -0.199 [-1.036, 0.477], loss: 4.725532, mean_squared_error: 9.451064, mean_q: 10.874172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 74279/100000: episode: 398, duration: 3.535s, episode steps: 200, steps per second: 57, episode reward: -12.752, mean reward: -0.064 [-0.320, -0.000], mean action: 0.107 [-1.311, 1.789], mean observation: -0.229 [-1.200, 0.136], loss: 1.955056, mean_squared_error: 3.910113, mean_q: 11.599468\n",
      " 74388/100000: episode: 399, duration: 1.951s, episode steps: 109, steps per second: 56, episode reward: 90.264, mean reward: 0.828 [-0.293, 99.966], mean action: 0.642 [-0.667, 1.711], mean observation: -0.204 [-1.051, 0.453], loss: 1.763092, mean_squared_error: 3.526183, mean_q: 11.310525\n",
      " 74494/100000: episode: 400, duration: 1.904s, episode steps: 106, steps per second: 56, episode reward: 88.711, mean reward: 0.837 [-0.288, 99.789], mean action: 0.702 [-0.749, 1.698], mean observation: -0.205 [-1.011, 0.464], loss: 7.089410, mean_squared_error: 14.178821, mean_q: 12.462871\n",
      " 74610/100000: episode: 401, duration: 2.113s, episode steps: 116, steps per second: 55, episode reward: 91.496, mean reward: 0.789 [-0.276, 99.778], mean action: 0.407 [-0.798, 1.661], mean observation: -0.217 [-1.200, 0.476], loss: 1.853114, mean_squared_error: 3.706229, mean_q: 11.539643\n",
      " 74719/100000: episode: 402, duration: 1.952s, episode steps: 109, steps per second: 56, episode reward: 75.157, mean reward: 0.690 [-0.627, 99.591], mean action: 1.222 [-0.201, 2.503], mean observation: -0.170 [-0.876, 0.478], loss: 3.940870, mean_squared_error: 7.881741, mean_q: 13.083537\n",
      " 74919/100000: episode: 403, duration: 3.689s, episode steps: 200, steps per second: 54, episode reward: -70.763, mean reward: -0.354 [-1.240, -0.000], mean action: 1.635 [0.071, 3.521], mean observation: -0.129 [-0.685, 0.193], loss: 3.623923, mean_squared_error: 7.247847, mean_q: 12.175127\n",
      " 75035/100000: episode: 404, duration: 2.014s, episode steps: 116, steps per second: 58, episode reward: 80.479, mean reward: 0.694 [-0.498, 99.901], mean action: 1.100 [-0.069, 2.231], mean observation: -0.162 [-0.888, 0.451], loss: 1.865115, mean_squared_error: 3.730230, mean_q: 12.910372\n",
      " 75152/100000: episode: 405, duration: 2.075s, episode steps: 117, steps per second: 56, episode reward: 82.222, mean reward: 0.703 [-0.451, 99.780], mean action: 1.035 [-0.116, 2.123], mean observation: -0.141 [-0.733, 0.471], loss: 2.422609, mean_squared_error: 4.845218, mean_q: 13.010552\n",
      " 75318/100000: episode: 406, duration: 2.940s, episode steps: 166, steps per second: 56, episode reward: 61.602, mean reward: 0.371 [-0.781, 99.774], mean action: 1.297 [-0.269, 2.794], mean observation: -0.130 [-0.736, 0.473], loss: 2.531300, mean_squared_error: 5.062600, mean_q: 12.714762\n",
      " 75515/100000: episode: 407, duration: 3.436s, episode steps: 197, steps per second: 57, episode reward: 32.343, mean reward: 0.164 [-1.393, 98.687], mean action: 1.536 [-0.295, 3.732], mean observation: -0.141 [-0.849, 0.452], loss: 3.121262, mean_squared_error: 6.242525, mean_q: 13.257871\n",
      " 75715/100000: episode: 408, duration: 3.507s, episode steps: 200, steps per second: 57, episode reward: -90.152, mean reward: -0.451 [-1.565, -0.000], mean action: 1.955 [0.058, 3.956], mean observation: -0.155 [-0.413, 0.008], loss: 1.986416, mean_squared_error: 3.972831, mean_q: 12.537895\n",
      " 75827/100000: episode: 409, duration: 1.970s, episode steps: 112, steps per second: 57, episode reward: 83.429, mean reward: 0.745 [-0.428, 99.797], mean action: 0.969 [-0.238, 2.070], mean observation: -0.163 [-0.842, 0.454], loss: 3.110887, mean_squared_error: 6.221774, mean_q: 13.496604\n",
      " 75930/100000: episode: 410, duration: 1.818s, episode steps: 103, steps per second: 57, episode reward: 86.088, mean reward: 0.836 [-0.331, 99.754], mean action: 0.897 [-0.589, 1.818], mean observation: -0.174 [-0.852, 0.477], loss: 0.636521, mean_squared_error: 1.273041, mean_q: 13.774416\n",
      " 76047/100000: episode: 411, duration: 2.039s, episode steps: 117, steps per second: 57, episode reward: 90.452, mean reward: 0.773 [-0.255, 99.949], mean action: 0.452 [-1.010, 1.598], mean observation: -0.194 [-0.999, 0.455], loss: 3.155548, mean_squared_error: 6.311096, mean_q: 13.506365\n",
      " 76247/100000: episode: 412, duration: 3.553s, episode steps: 200, steps per second: 56, episode reward: -13.378, mean reward: -0.067 [-0.228, -0.000], mean action: -0.392 [-1.415, 1.509], mean observation: -0.302 [-1.200, 0.035], loss: 2.302933, mean_squared_error: 4.605866, mean_q: 13.759012\n",
      " 76366/100000: episode: 413, duration: 2.043s, episode steps: 119, steps per second: 58, episode reward: 94.694, mean reward: 0.796 [-0.208, 99.946], mean action: 0.239 [-1.011, 1.441], mean observation: -0.199 [-1.075, 0.450], loss: 4.628364, mean_squared_error: 9.256728, mean_q: 14.081784\n",
      " 76520/100000: episode: 414, duration: 2.682s, episode steps: 154, steps per second: 57, episode reward: 20.899, mean reward: 0.136 [-1.491, 99.298], mean action: 2.057 [0.225, 3.862], mean observation: -0.072 [-0.667, 0.463], loss: 3.035348, mean_squared_error: 6.070695, mean_q: 14.317706\n",
      " 76637/100000: episode: 415, duration: 2.056s, episode steps: 117, steps per second: 57, episode reward: 90.792, mean reward: 0.776 [-0.357, 99.990], mean action: 0.523 [-0.682, 1.889], mean observation: -0.176 [-0.995, 0.459], loss: 0.802285, mean_squared_error: 1.604570, mean_q: 14.314168\n",
      " 76837/100000: episode: 416, duration: 3.558s, episode steps: 200, steps per second: 56, episode reward: -17.169, mean reward: -0.086 [-0.339, -0.000], mean action: -0.361 [-1.842, 1.468], mean observation: -0.257 [-1.200, 0.125], loss: 4.992901, mean_squared_error: 9.985802, mean_q: 14.572353\n",
      " 76947/100000: episode: 417, duration: 1.947s, episode steps: 110, steps per second: 57, episode reward: 74.196, mean reward: 0.675 [-0.681, 99.364], mean action: 1.195 [-0.327, 2.609], mean observation: -0.193 [-1.026, 0.464], loss: 5.929250, mean_squared_error: 11.858500, mean_q: 15.175435\n",
      " 77147/100000: episode: 418, duration: 3.549s, episode steps: 200, steps per second: 56, episode reward: -17.338, mean reward: -0.087 [-0.424, -0.000], mean action: 0.206 [-1.732, 2.058], mean observation: -0.233 [-1.200, 0.107], loss: 4.639299, mean_squared_error: 9.278599, mean_q: 14.563059\n",
      " 77275/100000: episode: 419, duration: 2.264s, episode steps: 128, steps per second: 57, episode reward: 89.393, mean reward: 0.698 [-0.361, 99.949], mean action: 0.595 [-0.753, 1.900], mean observation: -0.196 [-1.129, 0.467], loss: 2.005368, mean_squared_error: 4.010736, mean_q: 15.457811\n",
      " 77471/100000: episode: 420, duration: 3.453s, episode steps: 196, steps per second: 57, episode reward: 59.420, mean reward: 0.303 [-0.968, 99.935], mean action: 1.186 [-0.109, 3.111], mean observation: -0.151 [-0.843, 0.467], loss: 1.744056, mean_squared_error: 3.488111, mean_q: 15.157682\n",
      " 77615/100000: episode: 421, duration: 2.541s, episode steps: 144, steps per second: 57, episode reward: 87.358, mean reward: 0.607 [-0.407, 99.918], mean action: 0.535 [-0.579, 2.016], mean observation: -0.203 [-1.049, 0.454], loss: 4.150643, mean_squared_error: 8.301286, mean_q: 15.603160\n",
      " 77728/100000: episode: 422, duration: 2.019s, episode steps: 113, steps per second: 56, episode reward: 89.711, mean reward: 0.794 [-0.335, 99.919], mean action: 0.494 [-0.876, 1.829], mean observation: -0.232 [-1.160, 0.453], loss: 1.046802, mean_squared_error: 2.093605, mean_q: 15.306986\n",
      " 77828/100000: episode: 423, duration: 1.772s, episode steps: 100, steps per second: 56, episode reward: 89.086, mean reward: 0.891 [-0.364, 99.666], mean action: 0.414 [-1.287, 1.907], mean observation: -0.234 [-1.200, 0.465], loss: 2.627962, mean_squared_error: 5.255924, mean_q: 15.892170\n",
      " 77932/100000: episode: 424, duration: 1.849s, episode steps: 104, steps per second: 56, episode reward: 84.834, mean reward: 0.816 [-0.557, 99.738], mean action: 0.951 [-0.453, 2.359], mean observation: -0.164 [-0.795, 0.459], loss: 2.875685, mean_squared_error: 5.751370, mean_q: 15.926719\n",
      " 78040/100000: episode: 425, duration: 1.931s, episode steps: 108, steps per second: 56, episode reward: 90.539, mean reward: 0.838 [-0.230, 99.948], mean action: 0.590 [-0.773, 1.516], mean observation: -0.182 [-0.948, 0.455], loss: 7.937799, mean_squared_error: 15.875597, mean_q: 15.750231\n",
      " 78146/100000: episode: 426, duration: 1.926s, episode steps: 106, steps per second: 55, episode reward: 85.039, mean reward: 0.802 [-0.572, 99.981], mean action: 0.889 [-0.540, 2.391], mean observation: -0.166 [-0.860, 0.471], loss: 3.001223, mean_squared_error: 6.002445, mean_q: 16.030478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78346/100000: episode: 427, duration: 3.555s, episode steps: 200, steps per second: 56, episode reward: -26.509, mean reward: -0.133 [-0.540, -0.000], mean action: -0.594 [-2.324, 1.570], mean observation: -0.296 [-1.200, 0.038], loss: 2.387594, mean_squared_error: 4.775188, mean_q: 16.591700\n",
      " 78546/100000: episode: 428, duration: 3.540s, episode steps: 200, steps per second: 56, episode reward: -40.011, mean reward: -0.200 [-0.664, -0.000], mean action: -0.441 [-2.576, 1.812], mean observation: -0.230 [-1.200, 0.329], loss: 2.941988, mean_squared_error: 5.883977, mean_q: 16.856617\n",
      " 78739/100000: episode: 429, duration: 3.414s, episode steps: 193, steps per second: 57, episode reward: 89.617, mean reward: 0.464 [-0.267, 99.984], mean action: 0.206 [-1.370, 1.635], mean observation: -0.207 [-1.200, 0.452], loss: 4.273331, mean_squared_error: 8.546661, mean_q: 16.880869\n",
      " 78939/100000: episode: 430, duration: 3.546s, episode steps: 200, steps per second: 56, episode reward: -8.352, mean reward: -0.042 [-0.203, -0.000], mean action: 0.048 [-1.426, 1.300], mean observation: -0.186 [-1.200, 0.422], loss: 2.047738, mean_squared_error: 4.095476, mean_q: 17.463205\n",
      " 79139/100000: episode: 431, duration: 3.508s, episode steps: 200, steps per second: 57, episode reward: -21.595, mean reward: -0.108 [-0.378, -0.000], mean action: -0.420 [-1.944, 1.546], mean observation: -0.292 [-1.200, 0.035], loss: 4.841764, mean_squared_error: 9.683529, mean_q: 17.234209\n",
      " 79339/100000: episode: 432, duration: 3.533s, episode steps: 200, steps per second: 57, episode reward: -37.014, mean reward: -0.185 [-0.988, -0.000], mean action: -0.827 [-3.143, 1.467], mean observation: -0.324 [-1.200, 0.038], loss: 4.531449, mean_squared_error: 9.062898, mean_q: 17.891003\n",
      " 79513/100000: episode: 433, duration: 3.035s, episode steps: 174, steps per second: 57, episode reward: 77.660, mean reward: 0.446 [-0.452, 99.872], mean action: 0.908 [-0.501, 2.125], mean observation: -0.179 [-0.902, 0.488], loss: 3.412735, mean_squared_error: 6.825469, mean_q: 18.650831\n",
      " 79713/100000: episode: 434, duration: 3.514s, episode steps: 200, steps per second: 57, episode reward: -43.408, mean reward: -0.217 [-0.779, -0.020], mean action: 1.347 [0.447, 2.791], mean observation: -0.152 [-0.551, 0.020], loss: 4.748802, mean_squared_error: 9.497603, mean_q: 18.277000\n",
      " 79913/100000: episode: 435, duration: 3.603s, episode steps: 200, steps per second: 56, episode reward: -45.082, mean reward: -0.225 [-0.839, -0.000], mean action: 1.281 [0.058, 2.896], mean observation: -0.163 [-0.755, 0.075], loss: 2.910704, mean_squared_error: 5.821408, mean_q: 18.503773\n",
      " 80025/100000: episode: 436, duration: 1.983s, episode steps: 112, steps per second: 56, episode reward: 79.950, mean reward: 0.714 [-0.692, 99.521], mean action: 0.925 [-0.638, 2.630], mean observation: -0.157 [-0.836, 0.464], loss: 3.107029, mean_squared_error: 6.214059, mean_q: 18.748606\n",
      " 80225/100000: episode: 437, duration: 3.488s, episode steps: 200, steps per second: 57, episode reward: -34.728, mean reward: -0.174 [-0.575, -0.000], mean action: 1.047 [-0.648, 2.399], mean observation: -0.199 [-0.767, 0.038], loss: 2.209955, mean_squared_error: 4.419910, mean_q: 18.908649\n",
      " 80347/100000: episode: 438, duration: 2.123s, episode steps: 122, steps per second: 57, episode reward: 58.232, mean reward: 0.477 [-1.211, 99.074], mean action: 1.453 [-0.433, 3.480], mean observation: -0.119 [-0.727, 0.460], loss: 4.247813, mean_squared_error: 8.495626, mean_q: 18.599367\n",
      " 80547/100000: episode: 439, duration: 3.479s, episode steps: 200, steps per second: 57, episode reward: -40.101, mean reward: -0.201 [-1.220, -0.004], mean action: 1.243 [0.207, 3.493], mean observation: -0.143 [-0.733, 0.188], loss: 1.414290, mean_squared_error: 2.828579, mean_q: 19.172827\n",
      " 80747/100000: episode: 440, duration: 3.566s, episode steps: 200, steps per second: 56, episode reward: -16.008, mean reward: -0.080 [-0.464, -0.000], mean action: 0.098 [-1.527, 2.154], mean observation: -0.227 [-1.200, 0.301], loss: 3.468826, mean_squared_error: 6.937653, mean_q: 19.550125\n",
      " 80947/100000: episode: 441, duration: 3.489s, episode steps: 200, steps per second: 57, episode reward: -7.781, mean reward: -0.039 [-0.304, -0.000], mean action: 0.160 [-1.459, 1.744], mean observation: -0.247 [-0.816, 0.070], loss: 4.509840, mean_squared_error: 9.019681, mean_q: 19.768312\n",
      " 81147/100000: episode: 442, duration: 3.537s, episode steps: 200, steps per second: 57, episode reward: -29.026, mean reward: -0.145 [-0.483, -0.000], mean action: -0.791 [-2.198, 1.312], mean observation: -0.330 [-1.200, 0.036], loss: 5.426659, mean_squared_error: 10.853318, mean_q: 19.621845\n",
      " 81281/100000: episode: 443, duration: 2.429s, episode steps: 134, steps per second: 55, episode reward: 81.351, mean reward: 0.607 [-0.830, 99.062], mean action: 0.844 [-0.788, 3.063], mean observation: -0.116 [-0.868, 0.459], loss: 4.384728, mean_squared_error: 8.769457, mean_q: 20.774363\n",
      " 81388/100000: episode: 444, duration: 1.898s, episode steps: 107, steps per second: 56, episode reward: 55.239, mean reward: 0.516 [-2.959, 96.994], mean action: 1.399 [-0.325, 5.482], mean observation: -0.165 [-0.804, 0.451], loss: 5.459756, mean_squared_error: 10.919513, mean_q: 21.037981\n",
      " 81588/100000: episode: 445, duration: 3.636s, episode steps: 200, steps per second: 55, episode reward: -118.828, mean reward: -0.594 [-1.994, -0.006], mean action: 2.162 [0.248, 4.466], mean observation: -0.147 [-0.526, 0.018], loss: 5.463064, mean_squared_error: 10.926128, mean_q: 20.651949\n",
      " 81788/100000: episode: 446, duration: 3.504s, episode steps: 200, steps per second: 57, episode reward: -10.849, mean reward: -0.054 [-0.387, -0.000], mean action: 0.004 [-1.967, 1.635], mean observation: -0.275 [-0.955, 0.046], loss: 3.047212, mean_squared_error: 6.094424, mean_q: 20.578648\n",
      " 81988/100000: episode: 447, duration: 3.575s, episode steps: 200, steps per second: 56, episode reward: -16.437, mean reward: -0.082 [-0.520, -0.000], mean action: -0.202 [-2.279, 1.349], mean observation: -0.236 [-1.200, 0.078], loss: 4.422335, mean_squared_error: 8.844670, mean_q: 20.848801\n",
      " 82185/100000: episode: 448, duration: 3.538s, episode steps: 197, steps per second: 56, episode reward: 69.291, mean reward: 0.352 [-2.528, 97.224], mean action: 0.674 [-1.005, 5.269], mean observation: -0.221 [-1.175, 0.471], loss: 2.980883, mean_squared_error: 5.961767, mean_q: 21.189642\n",
      " 82385/100000: episode: 449, duration: 3.558s, episode steps: 200, steps per second: 56, episode reward: -59.085, mean reward: -0.295 [-0.908, -0.019], mean action: 1.573 [0.434, 3.014], mean observation: -0.105 [-0.727, 0.180], loss: 3.387452, mean_squared_error: 6.774905, mean_q: 21.397854\n",
      " 82542/100000: episode: 450, duration: 2.808s, episode steps: 157, steps per second: 56, episode reward: 77.909, mean reward: 0.496 [-1.680, 98.286], mean action: 0.331 [-1.716, 4.140], mean observation: -0.238 [-1.083, 0.457], loss: 7.248566, mean_squared_error: 14.497131, mean_q: 21.249075\n",
      " 82742/100000: episode: 451, duration: 3.554s, episode steps: 200, steps per second: 56, episode reward: -43.624, mean reward: -0.218 [-1.128, -0.000], mean action: 1.321 [-0.095, 3.359], mean observation: -0.135 [-0.728, 0.157], loss: 2.894263, mean_squared_error: 5.788526, mean_q: 22.269104\n",
      " 82880/100000: episode: 452, duration: 2.467s, episode steps: 138, steps per second: 56, episode reward: 36.978, mean reward: 0.268 [-2.160, 97.523], mean action: 1.765 [0.053, 4.977], mean observation: -0.090 [-0.672, 0.453], loss: 7.643251, mean_squared_error: 15.286503, mean_q: 21.261532\n",
      " 83011/100000: episode: 453, duration: 2.430s, episode steps: 131, steps per second: 54, episode reward: 79.178, mean reward: 0.604 [-0.812, 99.059], mean action: 0.876 [-0.575, 3.067], mean observation: -0.150 [-0.943, 0.460], loss: 4.620996, mean_squared_error: 9.241992, mean_q: 21.579145\n",
      " 83133/100000: episode: 454, duration: 2.222s, episode steps: 122, steps per second: 55, episode reward: 88.600, mean reward: 0.726 [-0.510, 99.492], mean action: 0.517 [-0.895, 2.259], mean observation: -0.190 [-1.105, 0.468], loss: 8.066434, mean_squared_error: 16.132868, mean_q: 21.760342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 83254/100000: episode: 455, duration: 2.195s, episode steps: 121, steps per second: 55, episode reward: 86.591, mean reward: 0.716 [-0.332, 99.753], mean action: 0.885 [-0.233, 1.823], mean observation: -0.155 [-0.959, 0.482], loss: 6.355563, mean_squared_error: 12.711126, mean_q: 23.054089\n",
      " 83447/100000: episode: 456, duration: 3.447s, episode steps: 193, steps per second: 56, episode reward: 46.928, mean reward: 0.243 [-1.726, 98.190], mean action: 1.343 [-0.106, 4.255], mean observation: -0.143 [-0.770, 0.457], loss: 5.025947, mean_squared_error: 10.051894, mean_q: 22.757732\n",
      " 83556/100000: episode: 457, duration: 1.921s, episode steps: 109, steps per second: 57, episode reward: 64.451, mean reward: 0.591 [-2.636, 97.236], mean action: 0.609 [-1.972, 5.257], mean observation: -0.218 [-1.064, 0.480], loss: 4.471798, mean_squared_error: 8.943597, mean_q: 23.381218\n",
      " 83683/100000: episode: 458, duration: 2.236s, episode steps: 127, steps per second: 57, episode reward: 24.854, mean reward: 0.196 [-4.461, 95.310], mean action: 1.942 [0.253, 6.849], mean observation: -0.121 [-0.762, 0.476], loss: 2.933332, mean_squared_error: 5.866664, mean_q: 22.171383\n",
      " 83855/100000: episode: 459, duration: 2.993s, episode steps: 172, steps per second: 57, episode reward: 72.150, mean reward: 0.419 [-2.351, 97.453], mean action: 0.544 [-1.531, 5.047], mean observation: -0.230 [-1.105, 0.450], loss: 5.043218, mean_squared_error: 10.086435, mean_q: 23.036509\n",
      " 83979/100000: episode: 460, duration: 2.152s, episode steps: 124, steps per second: 58, episode reward: 78.370, mean reward: 0.632 [-0.886, 98.965], mean action: 1.018 [-0.543, 3.218], mean observation: -0.123 [-0.795, 0.462], loss: 4.746893, mean_squared_error: 9.493787, mean_q: 23.512882\n",
      " 84179/100000: episode: 461, duration: 3.564s, episode steps: 200, steps per second: 56, episode reward: -137.448, mean reward: -0.687 [-2.165, -0.000], mean action: 2.341 [0.012, 4.653], mean observation: -0.134 [-0.625, 0.157], loss: 6.967833, mean_squared_error: 13.935666, mean_q: 24.271574\n",
      " 84283/100000: episode: 462, duration: 1.848s, episode steps: 104, steps per second: 56, episode reward: 69.027, mean reward: 0.664 [-1.494, 98.380], mean action: 1.302 [-0.778, 4.025], mean observation: -0.145 [-0.747, 0.473], loss: 4.445279, mean_squared_error: 8.890557, mean_q: 24.000286\n",
      " 84382/100000: episode: 463, duration: 1.744s, episode steps: 99, steps per second: 57, episode reward: 78.133, mean reward: 0.789 [-1.150, 98.787], mean action: 1.032 [-0.879, 3.482], mean observation: -0.186 [-0.904, 0.453], loss: 4.477155, mean_squared_error: 8.954309, mean_q: 23.431858\n",
      " 84569/100000: episode: 464, duration: 3.283s, episode steps: 187, steps per second: 57, episode reward: 83.663, mean reward: 0.447 [-0.836, 99.092], mean action: 0.474 [-1.159, 3.014], mean observation: -0.205 [-1.200, 0.478], loss: 5.664328, mean_squared_error: 11.328655, mean_q: 23.867426\n",
      " 84769/100000: episode: 465, duration: 3.559s, episode steps: 200, steps per second: 56, episode reward: -34.863, mean reward: -0.174 [-0.832, -0.000], mean action: -0.435 [-2.885, 1.954], mean observation: -0.271 [-1.200, 0.037], loss: 4.607423, mean_squared_error: 9.214847, mean_q: 23.886772\n",
      " 84969/100000: episode: 466, duration: 3.593s, episode steps: 200, steps per second: 56, episode reward: -23.802, mean reward: -0.119 [-0.416, -0.000], mean action: 0.297 [-1.655, 2.040], mean observation: -0.187 [-1.200, 0.264], loss: 4.804031, mean_squared_error: 9.608063, mean_q: 23.737194\n",
      " 85103/100000: episode: 467, duration: 2.399s, episode steps: 134, steps per second: 56, episode reward: 38.200, mean reward: 0.285 [-3.276, 96.410], mean action: 1.794 [0.296, 5.992], mean observation: -0.110 [-0.763, 0.472], loss: 5.449919, mean_squared_error: 10.899837, mean_q: 24.402704\n",
      " 85303/100000: episode: 468, duration: 3.579s, episode steps: 200, steps per second: 56, episode reward: -129.988, mean reward: -0.650 [-2.324, -0.000], mean action: 2.226 [-0.223, 4.821], mean observation: -0.129 [-0.591, 0.070], loss: 6.100019, mean_squared_error: 12.200039, mean_q: 24.404869\n",
      " 85503/100000: episode: 469, duration: 3.546s, episode steps: 200, steps per second: 56, episode reward: -26.332, mean reward: -0.132 [-0.718, -0.000], mean action: 0.842 [-0.546, 2.680], mean observation: -0.175 [-0.753, 0.026], loss: 5.981439, mean_squared_error: 11.962878, mean_q: 24.764107\n",
      " 85617/100000: episode: 470, duration: 2.056s, episode steps: 114, steps per second: 55, episode reward: 75.251, mean reward: 0.660 [-1.323, 98.817], mean action: 0.837 [-1.416, 3.638], mean observation: -0.155 [-0.881, 0.460], loss: 4.628719, mean_squared_error: 9.257439, mean_q: 24.647966\n",
      " 85725/100000: episode: 471, duration: 1.911s, episode steps: 108, steps per second: 57, episode reward: 73.865, mean reward: 0.684 [-1.739, 98.088], mean action: 0.979 [-1.073, 4.372], mean observation: -0.172 [-0.878, 0.476], loss: 7.366465, mean_squared_error: 14.732930, mean_q: 24.536770\n",
      " 85891/100000: episode: 472, duration: 2.944s, episode steps: 166, steps per second: 56, episode reward: 40.955, mean reward: 0.247 [-1.934, 97.876], mean action: 1.422 [-0.604, 4.609], mean observation: -0.160 [-0.734, 0.455], loss: 4.549486, mean_squared_error: 9.098972, mean_q: 24.684038\n",
      " 86053/100000: episode: 473, duration: 2.855s, episode steps: 162, steps per second: 57, episode reward: 71.336, mean reward: 0.440 [-2.317, 96.950], mean action: 0.777 [-1.265, 5.522], mean observation: -0.199 [-1.200, 0.502], loss: 5.714815, mean_squared_error: 11.429629, mean_q: 24.431564\n",
      " 86253/100000: episode: 474, duration: 3.488s, episode steps: 200, steps per second: 57, episode reward: -74.468, mean reward: -0.372 [-0.973, -0.009], mean action: 1.709 [0.297, 3.119], mean observation: -0.083 [-0.781, 0.264], loss: 2.238243, mean_squared_error: 4.476486, mean_q: 25.320522\n",
      " 86453/100000: episode: 475, duration: 3.539s, episode steps: 200, steps per second: 57, episode reward: -21.498, mean reward: -0.107 [-0.256, -0.000], mean action: -0.784 [-1.601, 1.313], mean observation: -0.348 [-1.057, 0.022], loss: 4.040346, mean_squared_error: 8.080692, mean_q: 25.863819\n",
      " 86653/100000: episode: 476, duration: 3.529s, episode steps: 200, steps per second: 57, episode reward: -20.135, mean reward: -0.101 [-0.450, -0.000], mean action: -0.099 [-2.121, 1.702], mean observation: -0.257 [-1.200, 0.105], loss: 3.942428, mean_squared_error: 7.884857, mean_q: 25.091070\n",
      " 86826/100000: episode: 477, duration: 3.038s, episode steps: 173, steps per second: 57, episode reward: 73.593, mean reward: 0.425 [-1.855, 97.980], mean action: 0.421 [-1.173, 4.495], mean observation: -0.226 [-0.930, 0.472], loss: 4.641422, mean_squared_error: 9.282844, mean_q: 25.914829\n",
      " 87000/100000: episode: 478, duration: 3.052s, episode steps: 174, steps per second: 57, episode reward: 13.943, mean reward: 0.080 [-3.099, 96.869], mean action: 1.694 [-0.271, 5.596], mean observation: -0.147 [-0.697, 0.451], loss: 6.982956, mean_squared_error: 13.965913, mean_q: 25.677641\n",
      " 87200/100000: episode: 479, duration: 3.570s, episode steps: 200, steps per second: 56, episode reward: -21.370, mean reward: -0.107 [-0.491, -0.000], mean action: -0.167 [-2.216, 1.695], mean observation: -0.254 [-1.200, 0.042], loss: 2.889732, mean_squared_error: 5.779464, mean_q: 25.907585\n",
      " 87400/100000: episode: 480, duration: 3.631s, episode steps: 200, steps per second: 55, episode reward: -75.322, mean reward: -0.377 [-1.165, -0.000], mean action: 1.619 [-0.386, 3.413], mean observation: -0.127 [-0.651, 0.285], loss: 5.462450, mean_squared_error: 10.924900, mean_q: 25.301821\n",
      " 87558/100000: episode: 481, duration: 2.754s, episode steps: 158, steps per second: 57, episode reward: 57.729, mean reward: 0.365 [-2.584, 97.020], mean action: 1.170 [-0.477, 5.459], mean observation: -0.170 [-0.797, 0.454], loss: 2.530180, mean_squared_error: 5.060359, mean_q: 25.804922\n",
      " 87707/100000: episode: 482, duration: 2.607s, episode steps: 149, steps per second: 57, episode reward: 63.996, mean reward: 0.430 [-2.055, 97.813], mean action: 0.949 [-0.896, 4.677], mean observation: -0.189 [-0.816, 0.455], loss: 3.573541, mean_squared_error: 7.147082, mean_q: 26.535671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87907/100000: episode: 483, duration: 3.527s, episode steps: 200, steps per second: 57, episode reward: -69.457, mean reward: -0.347 [-1.237, -0.005], mean action: 1.733 [0.229, 3.518], mean observation: -0.150 [-0.519, 0.017], loss: 4.093544, mean_squared_error: 8.187089, mean_q: 25.508669\n",
      " 88069/100000: episode: 484, duration: 2.849s, episode steps: 162, steps per second: 57, episode reward: 51.747, mean reward: 0.319 [-3.240, 96.435], mean action: 1.247 [-0.429, 5.971], mean observation: -0.172 [-0.886, 0.453], loss: 6.773367, mean_squared_error: 13.546735, mean_q: 25.847158\n",
      " 88258/100000: episode: 485, duration: 3.373s, episode steps: 189, steps per second: 56, episode reward: 57.853, mean reward: 0.306 [-2.220, 97.641], mean action: 1.133 [-0.189, 4.857], mean observation: -0.159 [-0.759, 0.476], loss: 5.122997, mean_squared_error: 10.245995, mean_q: 27.013046\n",
      " 88425/100000: episode: 486, duration: 3.005s, episode steps: 167, steps per second: 56, episode reward: 62.349, mean reward: 0.373 [-1.655, 98.004], mean action: 1.069 [-0.603, 4.468], mean observation: -0.171 [-0.797, 0.459], loss: 2.985836, mean_squared_error: 5.971672, mean_q: 26.623541\n",
      " 88625/100000: episode: 487, duration: 3.545s, episode steps: 200, steps per second: 56, episode reward: -18.833, mean reward: -0.094 [-0.419, -0.000], mean action: 0.016 [-2.046, 1.852], mean observation: -0.269 [-1.069, 0.041], loss: 4.595900, mean_squared_error: 9.191800, mean_q: 26.384024\n",
      " 88785/100000: episode: 488, duration: 2.791s, episode steps: 160, steps per second: 57, episode reward: 73.207, mean reward: 0.458 [-0.544, 99.604], mean action: 0.999 [-0.468, 2.332], mean observation: -0.163 [-0.722, 0.455], loss: 4.313364, mean_squared_error: 8.626727, mean_q: 27.226593\n",
      " 88985/100000: episode: 489, duration: 3.564s, episode steps: 200, steps per second: 56, episode reward: -13.025, mean reward: -0.065 [-0.360, -0.000], mean action: -0.204 [-1.898, 1.751], mean observation: -0.280 [-0.897, 0.026], loss: 5.942435, mean_squared_error: 11.884871, mean_q: 26.024229\n",
      " 89185/100000: episode: 490, duration: 3.509s, episode steps: 200, steps per second: 57, episode reward: -51.022, mean reward: -0.255 [-1.166, -0.000], mean action: 1.414 [-0.045, 3.414], mean observation: -0.149 [-0.523, 0.018], loss: 6.195837, mean_squared_error: 12.391674, mean_q: 26.750908\n",
      " 89383/100000: episode: 491, duration: 3.509s, episode steps: 198, steps per second: 56, episode reward: 70.191, mean reward: 0.355 [-0.462, 99.536], mean action: 0.248 [-2.131, 2.153], mean observation: -0.210 [-1.048, 0.455], loss: 4.029715, mean_squared_error: 8.059429, mean_q: 26.714281\n",
      " 89583/100000: episode: 492, duration: 3.438s, episode steps: 200, steps per second: 58, episode reward: -92.042, mean reward: -0.460 [-1.353, -0.000], mean action: 1.987 [0.055, 3.679], mean observation: -0.151 [-0.570, 0.021], loss: 3.125480, mean_squared_error: 6.250961, mean_q: 26.964024\n",
      " 89783/100000: episode: 493, duration: 3.533s, episode steps: 200, steps per second: 57, episode reward: -100.110, mean reward: -0.501 [-1.629, -0.013], mean action: 2.030 [0.358, 4.037], mean observation: -0.135 [-0.623, 0.151], loss: 5.865281, mean_squared_error: 11.730561, mean_q: 26.990210\n",
      " 89978/100000: episode: 494, duration: 3.436s, episode steps: 195, steps per second: 57, episode reward: 47.828, mean reward: 0.245 [-1.026, 99.405], mean action: 1.407 [0.050, 3.203], mean observation: -0.144 [-0.720, 0.467], loss: 4.177694, mean_squared_error: 8.355389, mean_q: 26.834867\n",
      " 90178/100000: episode: 495, duration: 3.501s, episode steps: 200, steps per second: 57, episode reward: -112.173, mean reward: -0.561 [-2.243, -0.009], mean action: 2.047 [0.304, 4.736], mean observation: -0.150 [-0.508, 0.016], loss: 3.846067, mean_squared_error: 7.692134, mean_q: 26.114317\n",
      " 90344/100000: episode: 496, duration: 2.972s, episode steps: 166, steps per second: 56, episode reward: 64.776, mean reward: 0.390 [-0.948, 99.277], mean action: 1.062 [-0.481, 3.079], mean observation: -0.176 [-0.797, 0.451], loss: 3.274599, mean_squared_error: 6.549198, mean_q: 26.356970\n",
      " 90507/100000: episode: 497, duration: 2.843s, episode steps: 163, steps per second: 57, episode reward: 19.890, mean reward: 0.122 [-1.896, 98.103], mean action: 1.776 [-0.491, 4.356], mean observation: -0.096 [-0.656, 0.462], loss: 3.926520, mean_squared_error: 7.853039, mean_q: 26.638184\n",
      " 90707/100000: episode: 498, duration: 3.519s, episode steps: 200, steps per second: 57, episode reward: -23.611, mean reward: -0.118 [-0.659, -0.000], mean action: 0.532 [-1.215, 2.566], mean observation: -0.242 [-1.019, 0.133], loss: 4.560344, mean_squared_error: 9.120688, mean_q: 26.719072\n",
      " 90907/100000: episode: 499, duration: 3.601s, episode steps: 200, steps per second: 56, episode reward: -152.096, mean reward: -0.760 [-1.934, -0.001], mean action: 2.604 [0.102, 4.397], mean observation: -0.158 [-0.429, 0.010], loss: 4.294802, mean_squared_error: 8.589603, mean_q: 26.767420\n",
      " 91060/100000: episode: 500, duration: 2.706s, episode steps: 153, steps per second: 57, episode reward: 87.867, mean reward: 0.574 [-0.625, 100.000], mean action: 0.462 [-0.854, 2.499], mean observation: -0.173 [-0.953, 0.453], loss: 7.685807, mean_squared_error: 15.371614, mean_q: 26.959770\n",
      " 91260/100000: episode: 501, duration: 3.513s, episode steps: 200, steps per second: 57, episode reward: -23.902, mean reward: -0.120 [-0.763, -0.000], mean action: -0.120 [-2.542, 2.763], mean observation: -0.252 [-1.200, 0.047], loss: 4.402641, mean_squared_error: 8.805282, mean_q: 27.186470\n",
      " 91404/100000: episode: 502, duration: 2.553s, episode steps: 144, steps per second: 56, episode reward: 69.022, mean reward: 0.479 [-0.968, 99.779], mean action: 1.126 [-0.424, 3.112], mean observation: -0.158 [-0.740, 0.463], loss: 3.677609, mean_squared_error: 7.355218, mean_q: 26.417372\n",
      " 91558/100000: episode: 503, duration: 2.650s, episode steps: 154, steps per second: 58, episode reward: 32.372, mean reward: 0.210 [-1.340, 99.381], mean action: 1.909 [0.424, 3.660], mean observation: -0.090 [-0.688, 0.466], loss: 5.709861, mean_squared_error: 11.419723, mean_q: 26.924767\n",
      " 91750/100000: episode: 504, duration: 3.423s, episode steps: 192, steps per second: 56, episode reward: 64.286, mean reward: 0.335 [-0.766, 99.687], mean action: 1.032 [-0.470, 2.768], mean observation: -0.131 [-0.668, 0.460], loss: 6.780754, mean_squared_error: 13.561508, mean_q: 26.907747\n",
      " 91898/100000: episode: 505, duration: 2.606s, episode steps: 148, steps per second: 57, episode reward: 86.855, mean reward: 0.587 [-0.449, 99.982], mean action: -0.345 [-2.118, 1.207], mean observation: -0.295 [-1.105, 0.469], loss: 5.118805, mean_squared_error: 10.237610, mean_q: 27.368536\n",
      " 91980/100000: episode: 506, duration: 1.445s, episode steps: 82, steps per second: 57, episode reward: 92.368, mean reward: 1.126 [-0.417, 99.778], mean action: 0.459 [-0.911, 2.042], mean observation: -0.198 [-0.806, 0.452], loss: 5.943182, mean_squared_error: 11.886364, mean_q: 27.826427\n",
      " 92180/100000: episode: 507, duration: 3.526s, episode steps: 200, steps per second: 57, episode reward: -8.272, mean reward: -0.041 [-0.264, -0.000], mean action: -0.023 [-1.196, 1.625], mean observation: -0.224 [-1.200, 0.325], loss: 3.907506, mean_squared_error: 7.815012, mean_q: 27.837257\n",
      " 92255/100000: episode: 508, duration: 1.329s, episode steps: 75, steps per second: 56, episode reward: 90.975, mean reward: 1.213 [-0.449, 99.765], mean action: 0.394 [-1.082, 2.119], mean observation: -0.224 [-0.887, 0.466], loss: 4.482313, mean_squared_error: 8.964625, mean_q: 27.193487\n",
      " 92333/100000: episode: 509, duration: 1.406s, episode steps: 78, steps per second: 55, episode reward: 88.682, mean reward: 1.137 [-0.402, 99.682], mean action: 0.581 [-0.844, 2.006], mean observation: -0.211 [-0.833, 0.454], loss: 7.203191, mean_squared_error: 14.406382, mean_q: 27.621141\n",
      " 92424/100000: episode: 510, duration: 1.605s, episode steps: 91, steps per second: 57, episode reward: 94.681, mean reward: 1.040 [-0.162, 99.997], mean action: 0.053 [-1.047, 1.274], mean observation: -0.192 [-0.828, 0.450], loss: 4.336893, mean_squared_error: 8.673786, mean_q: 27.397455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92521/100000: episode: 511, duration: 1.748s, episode steps: 97, steps per second: 56, episode reward: 94.085, mean reward: 0.970 [-0.266, 99.999], mean action: 0.141 [-1.021, 1.631], mean observation: -0.188 [-0.837, 0.451], loss: 4.533645, mean_squared_error: 9.067290, mean_q: 26.872526\n",
      " 92606/100000: episode: 512, duration: 1.485s, episode steps: 85, steps per second: 57, episode reward: 86.171, mean reward: 1.014 [-0.549, 99.965], mean action: 0.580 [-1.248, 2.343], mean observation: -0.181 [-0.750, 0.471], loss: 5.981432, mean_squared_error: 11.962865, mean_q: 27.291800\n",
      " 92760/100000: episode: 513, duration: 2.771s, episode steps: 154, steps per second: 56, episode reward: 90.269, mean reward: 0.586 [-0.298, 99.933], mean action: -0.085 [-1.725, 1.438], mean observation: -0.226 [-1.200, 0.480], loss: 8.368183, mean_squared_error: 16.736366, mean_q: 28.003386\n",
      " 92918/100000: episode: 514, duration: 2.816s, episode steps: 158, steps per second: 56, episode reward: 91.303, mean reward: 0.578 [-0.343, 99.995], mean action: 0.130 [-1.354, 1.852], mean observation: -0.201 [-1.200, 0.475], loss: 5.755382, mean_squared_error: 11.510764, mean_q: 26.757553\n",
      " 92994/100000: episode: 515, duration: 1.365s, episode steps: 76, steps per second: 56, episode reward: 91.086, mean reward: 1.199 [-0.338, 99.772], mean action: 0.408 [-1.105, 1.840], mean observation: -0.225 [-0.866, 0.462], loss: 7.316132, mean_squared_error: 14.632263, mean_q: 27.898451\n",
      " 93194/100000: episode: 516, duration: 3.519s, episode steps: 200, steps per second: 57, episode reward: -56.427, mean reward: -0.282 [-1.228, -0.000], mean action: -1.249 [-3.504, 0.770], mean observation: -0.357 [-1.200, 0.036], loss: 4.964076, mean_squared_error: 9.928151, mean_q: 27.359020\n",
      " 93394/100000: episode: 517, duration: 3.553s, episode steps: 200, steps per second: 56, episode reward: -14.757, mean reward: -0.074 [-1.523, -0.000], mean action: -0.098 [-1.921, 3.902], mean observation: -0.226 [-1.200, 0.305], loss: 7.845725, mean_squared_error: 15.691450, mean_q: 27.752186\n",
      " 93581/100000: episode: 518, duration: 3.310s, episode steps: 187, steps per second: 56, episode reward: 57.761, mean reward: 0.309 [-0.949, 99.974], mean action: 1.228 [-0.511, 3.081], mean observation: -0.119 [-0.773, 0.468], loss: 6.029365, mean_squared_error: 12.058729, mean_q: 27.972155\n",
      " 93651/100000: episode: 519, duration: 1.242s, episode steps: 70, steps per second: 56, episode reward: 93.325, mean reward: 1.333 [-0.208, 99.989], mean action: 0.191 [-1.215, 1.443], mean observation: -0.228 [-0.938, 0.477], loss: 3.909617, mean_squared_error: 7.819234, mean_q: 27.925861\n",
      " 93723/100000: episode: 520, duration: 1.297s, episode steps: 72, steps per second: 55, episode reward: 88.202, mean reward: 1.225 [-0.485, 99.961], mean action: 0.696 [-0.995, 2.201], mean observation: -0.185 [-0.797, 0.462], loss: 4.646785, mean_squared_error: 9.293571, mean_q: 28.703796\n",
      " 93800/100000: episode: 521, duration: 1.395s, episode steps: 77, steps per second: 55, episode reward: 95.213, mean reward: 1.237 [-0.193, 100.000], mean action: 0.101 [-1.051, 1.391], mean observation: -0.231 [-0.937, 0.468], loss: 1.968277, mean_squared_error: 3.936554, mean_q: 27.994518\n",
      " 93872/100000: episode: 522, duration: 1.272s, episode steps: 72, steps per second: 57, episode reward: 86.329, mean reward: 1.199 [-0.437, 99.851], mean action: 0.448 [-1.412, 2.090], mean observation: -0.225 [-0.862, 0.477], loss: 7.017244, mean_squared_error: 14.034489, mean_q: 28.858530\n",
      " 93947/100000: episode: 523, duration: 1.326s, episode steps: 75, steps per second: 57, episode reward: 91.729, mean reward: 1.223 [-0.268, 99.999], mean action: 0.022 [-1.636, 1.530], mean observation: -0.202 [-0.853, 0.452], loss: 3.896656, mean_squared_error: 7.793312, mean_q: 28.367270\n",
      " 94021/100000: episode: 524, duration: 1.316s, episode steps: 74, steps per second: 56, episode reward: 93.445, mean reward: 1.263 [-0.291, 99.975], mean action: 0.248 [-1.050, 1.707], mean observation: -0.200 [-0.863, 0.476], loss: 5.307111, mean_squared_error: 10.614222, mean_q: 28.296291\n",
      " 94094/100000: episode: 525, duration: 1.288s, episode steps: 73, steps per second: 57, episode reward: 80.222, mean reward: 1.099 [-0.843, 99.653], mean action: 0.678 [-1.119, 2.904], mean observation: -0.235 [-0.872, 0.462], loss: 5.273466, mean_squared_error: 10.546932, mean_q: 27.373920\n",
      " 94169/100000: episode: 526, duration: 1.375s, episode steps: 75, steps per second: 55, episode reward: 94.866, mean reward: 1.265 [-0.155, 99.987], mean action: -0.042 [-1.243, 1.120], mean observation: -0.219 [-0.951, 0.453], loss: 9.059406, mean_squared_error: 18.118813, mean_q: 28.588028\n",
      " 94369/100000: episode: 527, duration: 3.550s, episode steps: 200, steps per second: 56, episode reward: -27.255, mean reward: -0.136 [-0.723, -0.000], mean action: -0.724 [-2.689, 1.306], mean observation: -0.316 [-1.200, 0.049], loss: 4.941707, mean_squared_error: 9.883414, mean_q: 29.076088\n",
      " 94459/100000: episode: 528, duration: 1.577s, episode steps: 90, steps per second: 57, episode reward: 92.834, mean reward: 1.031 [-0.359, 99.979], mean action: -0.120 [-1.893, 1.339], mean observation: -0.209 [-1.031, 0.458], loss: 5.265482, mean_squared_error: 10.530965, mean_q: 29.079084\n",
      " 94532/100000: episode: 529, duration: 1.325s, episode steps: 73, steps per second: 55, episode reward: 83.780, mean reward: 1.148 [-0.818, 99.917], mean action: 0.898 [-0.899, 2.861], mean observation: -0.179 [-0.791, 0.469], loss: 5.244217, mean_squared_error: 10.488435, mean_q: 28.025331\n",
      " 94610/100000: episode: 530, duration: 1.390s, episode steps: 78, steps per second: 56, episode reward: 94.279, mean reward: 1.209 [-0.153, 99.984], mean action: -0.021 [-1.236, 1.218], mean observation: -0.227 [-0.901, 0.466], loss: 4.272355, mean_squared_error: 8.544710, mean_q: 27.835270\n",
      " 94681/100000: episode: 531, duration: 1.288s, episode steps: 71, steps per second: 55, episode reward: 89.519, mean reward: 1.261 [-0.384, 99.956], mean action: 0.104 [-1.294, 1.959], mean observation: -0.263 [-1.033, 0.473], loss: 4.507249, mean_squared_error: 9.014498, mean_q: 28.213539\n",
      " 94754/100000: episode: 532, duration: 1.348s, episode steps: 73, steps per second: 54, episode reward: 91.975, mean reward: 1.260 [-0.550, 99.999], mean action: 0.432 [-0.978, 2.345], mean observation: -0.190 [-0.853, 0.461], loss: 4.957655, mean_squared_error: 9.915310, mean_q: 27.662285\n",
      " 94830/100000: episode: 533, duration: 1.411s, episode steps: 76, steps per second: 54, episode reward: 90.692, mean reward: 1.193 [-0.333, 99.978], mean action: 0.393 [-0.917, 1.824], mean observation: -0.232 [-0.878, 0.452], loss: 2.387013, mean_squared_error: 4.774026, mean_q: 28.603901\n",
      " 94908/100000: episode: 534, duration: 1.406s, episode steps: 78, steps per second: 55, episode reward: 82.015, mean reward: 1.051 [-0.665, 99.771], mean action: 0.784 [-0.955, 2.578], mean observation: -0.192 [-0.783, 0.469], loss: 5.640979, mean_squared_error: 11.281958, mean_q: 28.157129\n",
      " 94986/100000: episode: 535, duration: 1.421s, episode steps: 78, steps per second: 55, episode reward: 90.843, mean reward: 1.165 [-0.463, 99.997], mean action: 0.224 [-0.982, 2.151], mean observation: -0.225 [-0.860, 0.462], loss: 4.876635, mean_squared_error: 9.753269, mean_q: 28.693281\n",
      " 95096/100000: episode: 536, duration: 2.044s, episode steps: 110, steps per second: 54, episode reward: 71.370, mean reward: 0.649 [-1.101, 99.659], mean action: 0.659 [-1.109, 3.319], mean observation: -0.224 [-0.787, 0.466], loss: 7.492885, mean_squared_error: 14.985769, mean_q: 28.603971\n",
      " 95163/100000: episode: 537, duration: 1.228s, episode steps: 67, steps per second: 55, episode reward: 87.724, mean reward: 1.309 [-0.612, 99.991], mean action: 0.503 [-1.205, 2.475], mean observation: -0.241 [-0.945, 0.450], loss: 5.451146, mean_squared_error: 10.902292, mean_q: 28.922045\n",
      " 95363/100000: episode: 538, duration: 3.512s, episode steps: 200, steps per second: 57, episode reward: -42.911, mean reward: -0.215 [-1.362, -0.000], mean action: -0.790 [-3.691, 1.513], mean observation: -0.233 [-1.200, 0.354], loss: 3.939744, mean_squared_error: 7.879488, mean_q: 29.001123\n",
      " 95563/100000: episode: 539, duration: 3.536s, episode steps: 200, steps per second: 57, episode reward: -58.331, mean reward: -0.292 [-1.337, -0.000], mean action: -1.261 [-3.657, 0.703], mean observation: -0.330 [-1.200, 0.048], loss: 4.058243, mean_squared_error: 8.116486, mean_q: 28.275597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95652/100000: episode: 540, duration: 1.594s, episode steps: 89, steps per second: 56, episode reward: 94.247, mean reward: 1.059 [-0.192, 99.955], mean action: -0.071 [-1.387, 1.241], mean observation: -0.212 [-0.892, 0.463], loss: 4.922810, mean_squared_error: 9.845620, mean_q: 28.858088\n",
      " 95725/100000: episode: 541, duration: 1.319s, episode steps: 73, steps per second: 55, episode reward: 70.450, mean reward: 0.965 [-1.059, 99.268], mean action: 1.270 [-1.154, 3.255], mean observation: -0.179 [-0.784, 0.476], loss: 4.905738, mean_squared_error: 9.811476, mean_q: 29.725376\n",
      " 95807/100000: episode: 542, duration: 1.494s, episode steps: 82, steps per second: 55, episode reward: 87.749, mean reward: 1.070 [-0.482, 99.982], mean action: 0.224 [-1.198, 2.195], mean observation: -0.241 [-0.858, 0.458], loss: 4.487129, mean_squared_error: 8.974258, mean_q: 28.869936\n",
      " 95880/100000: episode: 543, duration: 1.354s, episode steps: 73, steps per second: 54, episode reward: 92.441, mean reward: 1.266 [-0.240, 100.000], mean action: -0.032 [-1.399, 1.549], mean observation: -0.283 [-1.068, 0.480], loss: 4.010113, mean_squared_error: 8.020226, mean_q: 29.414745\n",
      " 95958/100000: episode: 544, duration: 1.388s, episode steps: 78, steps per second: 56, episode reward: 87.517, mean reward: 1.122 [-0.442, 99.937], mean action: 0.043 [-1.483, 2.103], mean observation: -0.242 [-0.897, 0.481], loss: 3.537546, mean_squared_error: 7.075092, mean_q: 29.228432\n",
      " 96031/100000: episode: 545, duration: 1.337s, episode steps: 73, steps per second: 55, episode reward: 82.865, mean reward: 1.135 [-0.683, 99.722], mean action: 0.567 [-1.311, 2.613], mean observation: -0.227 [-0.843, 0.452], loss: 8.534632, mean_squared_error: 17.069263, mean_q: 29.146387\n",
      " 96105/100000: episode: 546, duration: 1.308s, episode steps: 74, steps per second: 57, episode reward: 91.061, mean reward: 1.231 [-0.440, 99.983], mean action: 0.116 [-1.442, 2.097], mean observation: -0.244 [-0.951, 0.479], loss: 4.430317, mean_squared_error: 8.860634, mean_q: 28.558907\n",
      " 96179/100000: episode: 547, duration: 1.371s, episode steps: 74, steps per second: 54, episode reward: 72.852, mean reward: 0.984 [-1.070, 99.116], mean action: 0.927 [-1.078, 3.272], mean observation: -0.224 [-0.850, 0.471], loss: 5.225467, mean_squared_error: 10.450933, mean_q: 28.699829\n",
      " 96266/100000: episode: 548, duration: 1.586s, episode steps: 87, steps per second: 55, episode reward: 95.245, mean reward: 1.095 [-0.204, 99.996], mean action: -0.161 [-1.429, 0.974], mean observation: -0.227 [-1.063, 0.453], loss: 3.950877, mean_squared_error: 7.901754, mean_q: 29.686161\n",
      " 96340/100000: episode: 549, duration: 1.300s, episode steps: 74, steps per second: 57, episode reward: 94.077, mean reward: 1.271 [-0.340, 99.997], mean action: 0.114 [-1.159, 1.844], mean observation: -0.220 [-0.921, 0.471], loss: 2.981706, mean_squared_error: 5.963412, mean_q: 30.887615\n",
      " 96413/100000: episode: 550, duration: 1.304s, episode steps: 73, steps per second: 56, episode reward: 90.008, mean reward: 1.233 [-0.276, 99.977], mean action: -0.093 [-1.662, 1.533], mean observation: -0.284 [-1.067, 0.478], loss: 5.472202, mean_squared_error: 10.944405, mean_q: 30.512123\n",
      " 96494/100000: episode: 551, duration: 1.464s, episode steps: 81, steps per second: 55, episode reward: 92.143, mean reward: 1.138 [-0.333, 99.972], mean action: -0.315 [-1.826, 0.906], mean observation: -0.270 [-1.069, 0.479], loss: 4.717134, mean_squared_error: 9.434267, mean_q: 29.190498\n",
      " 96577/100000: episode: 552, duration: 1.470s, episode steps: 83, steps per second: 56, episode reward: 91.513, mean reward: 1.103 [-0.313, 99.945], mean action: -0.133 [-1.768, 1.266], mean observation: -0.232 [-0.962, 0.474], loss: 5.079138, mean_squared_error: 10.158276, mean_q: 30.119438\n",
      " 96648/100000: episode: 553, duration: 1.288s, episode steps: 71, steps per second: 55, episode reward: 88.894, mean reward: 1.252 [-0.424, 99.980], mean action: 0.332 [-1.184, 2.059], mean observation: -0.263 [-0.999, 0.494], loss: 3.991518, mean_squared_error: 7.983036, mean_q: 29.609959\n",
      " 96720/100000: episode: 554, duration: 1.269s, episode steps: 72, steps per second: 57, episode reward: 87.997, mean reward: 1.222 [-0.618, 99.975], mean action: 0.219 [-1.405, 2.486], mean observation: -0.288 [-1.072, 0.455], loss: 11.880848, mean_squared_error: 23.761696, mean_q: 30.707493\n",
      " 96795/100000: episode: 555, duration: 1.351s, episode steps: 75, steps per second: 56, episode reward: 89.223, mean reward: 1.190 [-0.381, 99.930], mean action: 0.091 [-1.654, 1.951], mean observation: -0.220 [-0.892, 0.465], loss: 8.792121, mean_squared_error: 17.584242, mean_q: 30.912914\n",
      " 96870/100000: episode: 556, duration: 1.355s, episode steps: 75, steps per second: 55, episode reward: 91.114, mean reward: 1.215 [-0.313, 99.990], mean action: 0.082 [-1.346, 1.769], mean observation: -0.250 [-0.928, 0.474], loss: 4.080058, mean_squared_error: 8.160115, mean_q: 31.300974\n",
      " 96942/100000: episode: 557, duration: 1.301s, episode steps: 72, steps per second: 55, episode reward: 74.410, mean reward: 1.033 [-1.160, 99.812], mean action: 0.935 [-1.297, 3.405], mean observation: -0.218 [-0.839, 0.455], loss: 3.097168, mean_squared_error: 6.194335, mean_q: 30.699516\n",
      " 97017/100000: episode: 558, duration: 1.344s, episode steps: 75, steps per second: 56, episode reward: 89.202, mean reward: 1.189 [-0.422, 99.997], mean action: 0.181 [-1.255, 2.053], mean observation: -0.221 [-0.875, 0.466], loss: 3.260966, mean_squared_error: 6.521932, mean_q: 30.414053\n",
      " 97088/100000: episode: 559, duration: 1.246s, episode steps: 71, steps per second: 57, episode reward: 84.725, mean reward: 1.193 [-0.559, 99.997], mean action: 0.138 [-1.818, 2.364], mean observation: -0.257 [-0.964, 0.457], loss: 2.992437, mean_squared_error: 5.984873, mean_q: 31.319485\n",
      " 97162/100000: episode: 560, duration: 1.355s, episode steps: 74, steps per second: 55, episode reward: 92.498, mean reward: 1.250 [-0.266, 99.999], mean action: -0.096 [-1.323, 1.630], mean observation: -0.250 [-0.986, 0.459], loss: 9.021977, mean_squared_error: 18.043955, mean_q: 29.879652\n",
      " 97234/100000: episode: 561, duration: 1.280s, episode steps: 72, steps per second: 56, episode reward: 88.075, mean reward: 1.223 [-0.400, 99.963], mean action: -0.194 [-2.001, 1.511], mean observation: -0.292 [-1.095, 0.466], loss: 3.044486, mean_squared_error: 6.088973, mean_q: 29.973856\n",
      " 97309/100000: episode: 562, duration: 1.313s, episode steps: 75, steps per second: 57, episode reward: 88.435, mean reward: 1.179 [-0.332, 100.000], mean action: -0.296 [-1.817, 1.821], mean observation: -0.262 [-1.017, 0.473], loss: 3.922368, mean_squared_error: 7.844736, mean_q: 30.766914\n",
      " 97389/100000: episode: 563, duration: 1.449s, episode steps: 80, steps per second: 55, episode reward: 89.620, mean reward: 1.120 [-0.437, 99.998], mean action: -0.380 [-2.090, 1.123], mean observation: -0.252 [-0.968, 0.462], loss: 2.316809, mean_squared_error: 4.633618, mean_q: 31.185751\n",
      " 97467/100000: episode: 564, duration: 1.563s, episode steps: 78, steps per second: 50, episode reward: 91.136, mean reward: 1.168 [-0.366, 99.998], mean action: -0.084 [-1.371, 1.914], mean observation: -0.242 [-1.013, 0.467], loss: 5.350747, mean_squared_error: 10.701493, mean_q: 31.379078\n",
      " 97536/100000: episode: 565, duration: 1.336s, episode steps: 69, steps per second: 52, episode reward: 81.323, mean reward: 1.179 [-0.685, 99.994], mean action: 0.481 [-1.491, 2.618], mean observation: -0.273 [-1.043, 0.466], loss: 4.287682, mean_squared_error: 8.575364, mean_q: 30.777054\n",
      " 97621/100000: episode: 566, duration: 1.760s, episode steps: 85, steps per second: 48, episode reward: 91.528, mean reward: 1.077 [-0.348, 99.993], mean action: -0.301 [-1.864, 1.249], mean observation: -0.239 [-1.066, 0.462], loss: 6.740264, mean_squared_error: 13.480528, mean_q: 30.945328\n",
      " 97694/100000: episode: 567, duration: 1.565s, episode steps: 73, steps per second: 47, episode reward: 87.734, mean reward: 1.202 [-0.362, 99.938], mean action: -0.041 [-1.495, 1.902], mean observation: -0.285 [-1.085, 0.454], loss: 1.511895, mean_squared_error: 3.023790, mean_q: 31.453009\n",
      " 97783/100000: episode: 568, duration: 1.711s, episode steps: 89, steps per second: 52, episode reward: 95.547, mean reward: 1.074 [-0.204, 99.997], mean action: -0.101 [-1.266, 1.428], mean observation: -0.213 [-1.070, 0.452], loss: 3.986570, mean_squared_error: 7.973140, mean_q: 31.108541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97872/100000: episode: 569, duration: 1.751s, episode steps: 89, steps per second: 51, episode reward: 91.686, mean reward: 1.030 [-0.302, 99.974], mean action: -0.064 [-1.310, 1.738], mean observation: -0.211 [-0.851, 0.458], loss: 2.650040, mean_squared_error: 5.300081, mean_q: 31.569099\n",
      " 97971/100000: episode: 570, duration: 1.798s, episode steps: 99, steps per second: 55, episode reward: 91.931, mean reward: 0.929 [-0.240, 99.907], mean action: -0.104 [-1.550, 1.492], mean observation: -0.140 [-1.005, 0.450], loss: 7.005425, mean_squared_error: 14.010851, mean_q: 30.755079\n",
      " 98054/100000: episode: 571, duration: 1.466s, episode steps: 83, steps per second: 57, episode reward: 30.024, mean reward: 0.362 [-2.099, 99.372], mean action: 2.089 [-1.059, 4.582], mean observation: -0.127 [-0.701, 0.458], loss: 4.604082, mean_squared_error: 9.208163, mean_q: 31.563639\n",
      " 98133/100000: episode: 572, duration: 1.386s, episode steps: 79, steps per second: 57, episode reward: 92.504, mean reward: 1.171 [-0.296, 100.000], mean action: -0.022 [-1.302, 1.720], mean observation: -0.219 [-0.921, 0.466], loss: 4.350255, mean_squared_error: 8.700509, mean_q: 31.438158\n",
      " 98213/100000: episode: 573, duration: 1.419s, episode steps: 80, steps per second: 56, episode reward: 94.065, mean reward: 1.176 [-0.245, 99.993], mean action: 0.009 [-1.119, 1.565], mean observation: -0.213 [-0.933, 0.465], loss: 6.582171, mean_squared_error: 13.164343, mean_q: 31.498663\n",
      " 98413/100000: episode: 574, duration: 3.657s, episode steps: 200, steps per second: 55, episode reward: -12.768, mean reward: -0.064 [-0.449, -0.000], mean action: -0.222 [-2.119, 1.408], mean observation: -0.137 [-1.200, 0.414], loss: 4.964522, mean_squared_error: 9.929044, mean_q: 31.914400\n",
      " 98585/100000: episode: 575, duration: 3.039s, episode steps: 172, steps per second: 57, episode reward: 75.718, mean reward: 0.440 [-1.328, 98.603], mean action: -0.086 [-3.737, 1.624], mean observation: -0.189 [-1.200, 0.472], loss: 3.161757, mean_squared_error: 6.323514, mean_q: 31.513485\n",
      " 98665/100000: episode: 576, duration: 1.433s, episode steps: 80, steps per second: 56, episode reward: 85.616, mean reward: 1.070 [-0.576, 99.998], mean action: 0.322 [-1.237, 2.399], mean observation: -0.217 [-0.835, 0.473], loss: 3.257704, mean_squared_error: 6.515408, mean_q: 32.770031\n",
      " 98845/100000: episode: 577, duration: 3.201s, episode steps: 180, steps per second: 56, episode reward: 43.885, mean reward: 0.244 [-1.410, 99.996], mean action: 1.239 [-1.079, 3.755], mean observation: -0.161 [-0.849, 0.461], loss: 5.272845, mean_squared_error: 10.545690, mean_q: 31.718363\n",
      " 98915/100000: episode: 578, duration: 1.239s, episode steps: 70, steps per second: 57, episode reward: 85.084, mean reward: 1.215 [-0.595, 99.971], mean action: 0.431 [-1.374, 2.438], mean observation: -0.233 [-0.940, 0.463], loss: 1.483005, mean_squared_error: 2.966009, mean_q: 31.745497\n",
      " 98997/100000: episode: 579, duration: 1.466s, episode steps: 82, steps per second: 56, episode reward: 90.692, mean reward: 1.106 [-0.360, 99.998], mean action: -0.152 [-1.899, 1.602], mean observation: -0.219 [-0.930, 0.453], loss: 5.245753, mean_squared_error: 10.491506, mean_q: 31.178558\n",
      " 99072/100000: episode: 580, duration: 1.335s, episode steps: 75, steps per second: 56, episode reward: 87.544, mean reward: 1.167 [-0.357, 99.998], mean action: -0.230 [-1.888, 1.602], mean observation: -0.238 [-0.960, 0.454], loss: 4.493286, mean_squared_error: 8.986572, mean_q: 32.765137\n",
      " 99146/100000: episode: 581, duration: 1.294s, episode steps: 74, steps per second: 57, episode reward: 82.076, mean reward: 1.109 [-0.835, 100.000], mean action: 0.515 [-1.035, 2.890], mean observation: -0.210 [-0.906, 0.460], loss: 6.592745, mean_squared_error: 13.185490, mean_q: 32.823353\n",
      " 99305/100000: episode: 582, duration: 2.788s, episode steps: 159, steps per second: 57, episode reward: 69.370, mean reward: 0.436 [-1.221, 99.251], mean action: -0.524 [-3.495, 1.379], mean observation: -0.212 [-1.200, 0.465], loss: 4.936469, mean_squared_error: 9.872938, mean_q: 32.454048\n",
      " 99384/100000: episode: 583, duration: 1.371s, episode steps: 79, steps per second: 58, episode reward: 90.302, mean reward: 1.143 [-0.375, 99.999], mean action: 0.123 [-1.277, 1.937], mean observation: -0.221 [-0.900, 0.463], loss: 2.301839, mean_squared_error: 4.603679, mean_q: 32.870823\n",
      " 99459/100000: episode: 584, duration: 1.331s, episode steps: 75, steps per second: 56, episode reward: 93.695, mean reward: 1.249 [-0.270, 99.998], mean action: 0.019 [-1.260, 1.645], mean observation: -0.240 [-1.018, 0.456], loss: 4.352201, mean_squared_error: 8.704402, mean_q: 33.919666\n",
      " 99529/100000: episode: 585, duration: 1.227s, episode steps: 70, steps per second: 57, episode reward: 84.513, mean reward: 1.207 [-0.725, 99.997], mean action: 0.517 [-1.352, 2.693], mean observation: -0.215 [-0.875, 0.479], loss: 7.513877, mean_squared_error: 15.027754, mean_q: 33.401577\n",
      " 99600/100000: episode: 586, duration: 1.268s, episode steps: 71, steps per second: 56, episode reward: 79.907, mean reward: 1.125 [-0.686, 99.993], mean action: 0.333 [-1.812, 2.620], mean observation: -0.252 [-0.996, 0.480], loss: 6.412469, mean_squared_error: 12.824938, mean_q: 32.594967\n",
      " 99675/100000: episode: 587, duration: 1.348s, episode steps: 75, steps per second: 56, episode reward: 87.793, mean reward: 1.171 [-0.349, 99.995], mean action: -0.120 [-1.869, 1.676], mean observation: -0.229 [-0.879, 0.454], loss: 11.560481, mean_squared_error: 23.120962, mean_q: 32.814205\n",
      " 99751/100000: episode: 588, duration: 1.383s, episode steps: 76, steps per second: 55, episode reward: 92.068, mean reward: 1.211 [-0.271, 99.999], mean action: -0.070 [-1.324, 1.645], mean observation: -0.241 [-1.049, 0.469], loss: 2.921328, mean_squared_error: 5.842657, mean_q: 32.429054\n",
      " 99821/100000: episode: 589, duration: 1.290s, episode steps: 70, steps per second: 54, episode reward: 78.021, mean reward: 1.115 [-0.812, 99.968], mean action: 0.475 [-1.503, 2.849], mean observation: -0.274 [-1.064, 0.455], loss: 7.706896, mean_squared_error: 15.413793, mean_q: 33.201817\n",
      " 99893/100000: episode: 590, duration: 1.270s, episode steps: 72, steps per second: 57, episode reward: 86.625, mean reward: 1.203 [-0.489, 100.000], mean action: 0.112 [-1.686, 2.212], mean observation: -0.245 [-0.980, 0.471], loss: 2.874052, mean_squared_error: 5.748104, mean_q: 33.362732\n",
      " 99973/100000: episode: 591, duration: 1.454s, episode steps: 80, steps per second: 55, episode reward: 87.355, mean reward: 1.092 [-0.495, 99.999], mean action: -0.468 [-2.225, 1.058], mean observation: -0.238 [-0.976, 0.454], loss: 4.107504, mean_squared_error: 8.215008, mean_q: 33.178890\n",
      "done, took 1765.206 seconds\n"
     ]
    }
   ],
   "source": [
    "hist = agent.fit(env, nb_steps=100000, visualize=False, verbose=2, nb_max_episode_steps=200)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 86.317, steps: 76\n",
      "Episode 2: reward: 82.216, steps: 74\n",
      "Episode 3: reward: 80.933, steps: 73\n",
      "Episode 4: reward: 81.349, steps: 73\n",
      "Episode 5: reward: 86.221, steps: 76\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15a1a770588>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.test(env, nb_episodes=5, visualize=True, nb_max_episode_steps=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15a2e78dcc0>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC1hmYGFhYWEBywwsLCwsLAD8f15sHywDD89KAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15a2e70d780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plt.subplot(111)\n",
    "ax.set_ylim([-100, 100])\n",
    "ax.plot(hist.history.get('episode_reward'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
