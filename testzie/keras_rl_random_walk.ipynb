{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import itertools\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rl.agents import *\n",
    "from rl.policy import *\n",
    "from rl.memory import *\n",
    "from rl.random import *\n",
    "import gym\n",
    "from gym import Env, Space, spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TestEnv(Env):\n",
    "    metadata = {'render.modes': ['human', 'rgb_array']}\n",
    "\n",
    "    def __init__(self, num, step_size) -> None:\n",
    "        self.num = num\n",
    "        self.step_size = step_size\n",
    "        \n",
    "        self.data = np.cumsum(np.random.choice([-1, 0, 1], self.num))\n",
    "        self.data = (self.data - self.data.min())/(self.data.max() - self.data.min())\n",
    "        \n",
    "        self.step_begin = 0\n",
    "        self.step_end = self.step_begin + self.step_size\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(0, 1, (step_size, ), float)\n",
    "\n",
    "    def step(self, action):\n",
    "        price_del = self.data[self.step_end] - self.data[self.step_end-1]\n",
    "        if action == 1:\n",
    "            reward = price_del\n",
    "        elif action == 0:\n",
    "            reward = -price_del\n",
    "        else:\n",
    "            raise Exception()\n",
    "        \n",
    "        self.step_begin += 1\n",
    "        self.step_end += 1\n",
    "        \n",
    "        state = self.data[self.step_begin:self.step_end]\n",
    "        \n",
    "        if self.step_end < self.data.size-1:\n",
    "            finished = False\n",
    "        else:\n",
    "            finished = True\n",
    "        \n",
    "        return state, reward, finished, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.step_begin = 0\n",
    "        self.step_end = self.step_begin + self.step_size\n",
    "        return self.data[self.step_begin:self.step_end]\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        pass\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        if not close:\n",
    "            plt.plot(self.data)\n",
    "\n",
    "    def close(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = TestEnv(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('./weights/keras_rl_random_walk.npy', env.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.data = np.load('./weights/keras_rl_random_walk.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.71428571, 0.85714286, 1.        , 0.85714286, 0.85714286,\n",
       "       0.85714286, 1.        , 1.        , 1.        , 0.85714286])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqzj5MYi2SNQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c1e839a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_length = 1\n",
    "x = Input((window_length, ) + env.observation_space.shape)\n",
    "y = Flatten()(x)\n",
    "y = Dense(16, activation='elu')(y)\n",
    "y = Dense(16, activation='elu')(y)\n",
    "y = Dense(env.action_space.n)(y)\n",
    "model = Model(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=100000, window_length=window_length)\n",
    "policy = EpsGreedyQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=env.action_space.n, memory=memory, nb_steps_warmup=1000, gamma=.95, batch_size=32,\n",
    "               enable_dueling_network=True, dueling_type='avg', target_model_update=.1, policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.compile(Adam(), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 100000 steps ...\n",
      "    89/100000: episode: 1, duration: 0.348s, episode steps: 89, steps per second: 256, episode reward: -0.143, mean reward: -0.002 [-0.143, 0.143], mean action: 0.888 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   178/100000: episode: 2, duration: 0.121s, episode steps: 89, steps per second: 734, episode reward: 0.143, mean reward: 0.002 [-0.143, 0.143], mean action: 0.843 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   267/100000: episode: 3, duration: 0.126s, episode steps: 89, steps per second: 704, episode reward: 0.429, mean reward: 0.005 [-0.143, 0.143], mean action: 0.798 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   356/100000: episode: 4, duration: 0.111s, episode steps: 89, steps per second: 801, episode reward: -0.429, mean reward: -0.005 [-0.143, 0.143], mean action: 0.831 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   445/100000: episode: 5, duration: 0.125s, episode steps: 89, steps per second: 711, episode reward: 0.143, mean reward: 0.002 [-0.143, 0.143], mean action: 0.854 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   534/100000: episode: 6, duration: 0.119s, episode steps: 89, steps per second: 750, episode reward: -0.143, mean reward: -0.002 [-0.143, 0.143], mean action: 0.854 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   623/100000: episode: 7, duration: 0.120s, episode steps: 89, steps per second: 741, episode reward: 0.143, mean reward: 0.002 [-0.143, 0.143], mean action: 0.865 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   712/100000: episode: 8, duration: 0.114s, episode steps: 89, steps per second: 784, episode reward: 0.429, mean reward: 0.005 [-0.143, 0.143], mean action: 0.843 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   801/100000: episode: 9, duration: 0.120s, episode steps: 89, steps per second: 743, episode reward: 0.429, mean reward: 0.005 [-0.143, 0.143], mean action: 0.831 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   890/100000: episode: 10, duration: 0.120s, episode steps: 89, steps per second: 743, episode reward: -0.429, mean reward: -0.005 [-0.143, 0.143], mean action: 0.764 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "   979/100000: episode: 11, duration: 0.130s, episode steps: 89, steps per second: 687, episode reward: -1.286, mean reward: -0.014 [-0.143, 0.143], mean action: 0.876 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: --, mean_absolute_error: --, mean_q: --\n",
      "  1068/100000: episode: 12, duration: 2.531s, episode steps: 89, steps per second: 35, episode reward: -0.714, mean reward: -0.008 [-0.143, 0.143], mean action: 0.596 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.014370, mean_absolute_error: 0.199050, mean_q: 0.357907\n",
      "  1157/100000: episode: 13, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 0.714, mean reward: 0.008 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.008009, mean_absolute_error: 0.191210, mean_q: 0.349522\n",
      "  1246/100000: episode: 14, duration: 1.125s, episode steps: 89, steps per second: 79, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006443, mean_absolute_error: 0.204815, mean_q: 0.388074\n",
      "  1335/100000: episode: 15, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 2.143, mean reward: 0.024 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006317, mean_absolute_error: 0.233182, mean_q: 0.436798\n",
      "  1424/100000: episode: 16, duration: 0.877s, episode steps: 89, steps per second: 101, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006426, mean_absolute_error: 0.241718, mean_q: 0.462023\n",
      "  1513/100000: episode: 17, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 2.143, mean reward: 0.024 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006919, mean_absolute_error: 0.256138, mean_q: 0.483167\n",
      "  1602/100000: episode: 18, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.596 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.007152, mean_absolute_error: 0.250686, mean_q: 0.469297\n",
      "  1691/100000: episode: 19, duration: 0.893s, episode steps: 89, steps per second: 100, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006918, mean_absolute_error: 0.245283, mean_q: 0.458347\n",
      "  1780/100000: episode: 20, duration: 1.088s, episode steps: 89, steps per second: 82, episode reward: 2.143, mean reward: 0.024 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.007161, mean_absolute_error: 0.225698, mean_q: 0.416047\n",
      "  1869/100000: episode: 21, duration: 1.080s, episode steps: 89, steps per second: 82, episode reward: 1.571, mean reward: 0.018 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006862, mean_absolute_error: 0.220865, mean_q: 0.409795\n",
      "  1958/100000: episode: 22, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006513, mean_absolute_error: 0.232421, mean_q: 0.437648\n",
      "  2047/100000: episode: 23, duration: 0.873s, episode steps: 89, steps per second: 102, episode reward: 1.857, mean reward: 0.021 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006502, mean_absolute_error: 0.247843, mean_q: 0.467944\n",
      "  2136/100000: episode: 24, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 2.143, mean reward: 0.024 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006516, mean_absolute_error: 0.247359, mean_q: 0.467446\n",
      "  2225/100000: episode: 25, duration: 1.120s, episode steps: 89, steps per second: 79, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006726, mean_absolute_error: 0.247319, mean_q: 0.463495\n",
      "  2314/100000: episode: 26, duration: 1.126s, episode steps: 89, steps per second: 79, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006719, mean_absolute_error: 0.243293, mean_q: 0.453203\n",
      "  2403/100000: episode: 27, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006625, mean_absolute_error: 0.242346, mean_q: 0.456401\n",
      "  2492/100000: episode: 28, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006093, mean_absolute_error: 0.245638, mean_q: 0.464619\n",
      "  2581/100000: episode: 29, duration: 1.027s, episode steps: 89, steps per second: 87, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006870, mean_absolute_error: 0.251541, mean_q: 0.470980\n",
      "  2670/100000: episode: 30, duration: 1.037s, episode steps: 89, steps per second: 86, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006572, mean_absolute_error: 0.246121, mean_q: 0.462073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2759/100000: episode: 31, duration: 1.126s, episode steps: 89, steps per second: 79, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006471, mean_absolute_error: 0.254659, mean_q: 0.477101\n",
      "  2848/100000: episode: 32, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.007058, mean_absolute_error: 0.262151, mean_q: 0.490606\n",
      "  2937/100000: episode: 33, duration: 1.135s, episode steps: 89, steps per second: 78, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006836, mean_absolute_error: 0.255574, mean_q: 0.481835\n",
      "  3026/100000: episode: 34, duration: 1.201s, episode steps: 89, steps per second: 74, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006869, mean_absolute_error: 0.254929, mean_q: 0.483068\n",
      "  3115/100000: episode: 35, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006529, mean_absolute_error: 0.264683, mean_q: 0.500715\n",
      "  3204/100000: episode: 36, duration: 0.872s, episode steps: 89, steps per second: 102, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006468, mean_absolute_error: 0.263804, mean_q: 0.502077\n",
      "  3293/100000: episode: 37, duration: 1.270s, episode steps: 89, steps per second: 70, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006298, mean_absolute_error: 0.258566, mean_q: 0.488225\n",
      "  3382/100000: episode: 38, duration: 1.103s, episode steps: 89, steps per second: 81, episode reward: 2.143, mean reward: 0.024 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006773, mean_absolute_error: 0.253344, mean_q: 0.475738\n",
      "  3471/100000: episode: 39, duration: 1.098s, episode steps: 89, steps per second: 81, episode reward: 2.143, mean reward: 0.024 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006690, mean_absolute_error: 0.254400, mean_q: 0.478314\n",
      "  3560/100000: episode: 40, duration: 1.097s, episode steps: 89, steps per second: 81, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006411, mean_absolute_error: 0.257312, mean_q: 0.484651\n",
      "  3649/100000: episode: 41, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006339, mean_absolute_error: 0.249756, mean_q: 0.476913\n",
      "  3738/100000: episode: 42, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006795, mean_absolute_error: 0.263129, mean_q: 0.498644\n",
      "  3827/100000: episode: 43, duration: 0.873s, episode steps: 89, steps per second: 102, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006680, mean_absolute_error: 0.274393, mean_q: 0.518302\n",
      "  3916/100000: episode: 44, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006150, mean_absolute_error: 0.266592, mean_q: 0.506878\n",
      "  4005/100000: episode: 45, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006147, mean_absolute_error: 0.269183, mean_q: 0.517771\n",
      "  4094/100000: episode: 46, duration: 0.889s, episode steps: 89, steps per second: 100, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006300, mean_absolute_error: 0.276772, mean_q: 0.528473\n",
      "  4183/100000: episode: 47, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006292, mean_absolute_error: 0.278281, mean_q: 0.528849\n",
      "  4272/100000: episode: 48, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006680, mean_absolute_error: 0.275936, mean_q: 0.526712\n",
      "  4361/100000: episode: 49, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006327, mean_absolute_error: 0.275149, mean_q: 0.525764\n",
      "  4450/100000: episode: 50, duration: 0.972s, episode steps: 89, steps per second: 92, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006188, mean_absolute_error: 0.270977, mean_q: 0.517547\n",
      "  4539/100000: episode: 51, duration: 0.956s, episode steps: 89, steps per second: 93, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006137, mean_absolute_error: 0.268773, mean_q: 0.511260\n",
      "  4628/100000: episode: 52, duration: 0.964s, episode steps: 89, steps per second: 92, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006296, mean_absolute_error: 0.279520, mean_q: 0.529209\n",
      "  4717/100000: episode: 53, duration: 1.165s, episode steps: 89, steps per second: 76, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006394, mean_absolute_error: 0.273526, mean_q: 0.515793\n",
      "  4806/100000: episode: 54, duration: 1.124s, episode steps: 89, steps per second: 79, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006384, mean_absolute_error: 0.265266, mean_q: 0.502743\n",
      "  4895/100000: episode: 55, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005870, mean_absolute_error: 0.263065, mean_q: 0.502205\n",
      "  4984/100000: episode: 56, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006249, mean_absolute_error: 0.269069, mean_q: 0.514001\n",
      "  5073/100000: episode: 57, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006312, mean_absolute_error: 0.269984, mean_q: 0.515812\n",
      "  5162/100000: episode: 58, duration: 1.104s, episode steps: 89, steps per second: 81, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006260, mean_absolute_error: 0.281941, mean_q: 0.538586\n",
      "  5251/100000: episode: 59, duration: 1.090s, episode steps: 89, steps per second: 82, episode reward: 1.571, mean reward: 0.018 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006444, mean_absolute_error: 0.278990, mean_q: 0.537744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5340/100000: episode: 60, duration: 1.048s, episode steps: 89, steps per second: 85, episode reward: 1.857, mean reward: 0.021 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005945, mean_absolute_error: 0.282080, mean_q: 0.541531\n",
      "  5429/100000: episode: 61, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006209, mean_absolute_error: 0.280792, mean_q: 0.543002\n",
      "  5518/100000: episode: 62, duration: 0.889s, episode steps: 89, steps per second: 100, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006047, mean_absolute_error: 0.282548, mean_q: 0.548681\n",
      "  5607/100000: episode: 63, duration: 0.860s, episode steps: 89, steps per second: 103, episode reward: 1.571, mean reward: 0.018 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005587, mean_absolute_error: 0.288730, mean_q: 0.562654\n",
      "  5696/100000: episode: 64, duration: 0.882s, episode steps: 89, steps per second: 101, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005846, mean_absolute_error: 0.287103, mean_q: 0.557450\n",
      "  5785/100000: episode: 65, duration: 0.882s, episode steps: 89, steps per second: 101, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006136, mean_absolute_error: 0.291955, mean_q: 0.567552\n",
      "  5874/100000: episode: 66, duration: 0.888s, episode steps: 89, steps per second: 100, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.584 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.006110, mean_absolute_error: 0.287211, mean_q: 0.559073\n",
      "  5963/100000: episode: 67, duration: 0.985s, episode steps: 89, steps per second: 90, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005810, mean_absolute_error: 0.294015, mean_q: 0.571491\n",
      "  6052/100000: episode: 68, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005717, mean_absolute_error: 0.296858, mean_q: 0.580777\n",
      "  6141/100000: episode: 69, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005674, mean_absolute_error: 0.296227, mean_q: 0.579341\n",
      "  6230/100000: episode: 70, duration: 0.922s, episode steps: 89, steps per second: 97, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005922, mean_absolute_error: 0.303063, mean_q: 0.598591\n",
      "  6319/100000: episode: 71, duration: 0.922s, episode steps: 89, steps per second: 96, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005518, mean_absolute_error: 0.305399, mean_q: 0.603446\n",
      "  6408/100000: episode: 72, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005569, mean_absolute_error: 0.309488, mean_q: 0.608060\n",
      "  6497/100000: episode: 73, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 2.143, mean reward: 0.024 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005809, mean_absolute_error: 0.307714, mean_q: 0.606590\n",
      "  6586/100000: episode: 74, duration: 0.889s, episode steps: 89, steps per second: 100, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005588, mean_absolute_error: 0.293760, mean_q: 0.576785\n",
      "  6675/100000: episode: 75, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005547, mean_absolute_error: 0.280746, mean_q: 0.554234\n",
      "  6764/100000: episode: 76, duration: 1.171s, episode steps: 89, steps per second: 76, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005907, mean_absolute_error: 0.289969, mean_q: 0.569320\n",
      "  6853/100000: episode: 77, duration: 0.895s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005149, mean_absolute_error: 0.278075, mean_q: 0.553178\n",
      "  6942/100000: episode: 78, duration: 1.302s, episode steps: 89, steps per second: 68, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005363, mean_absolute_error: 0.278112, mean_q: 0.549634\n",
      "  7031/100000: episode: 79, duration: 1.150s, episode steps: 89, steps per second: 77, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005844, mean_absolute_error: 0.286749, mean_q: 0.569587\n",
      "  7120/100000: episode: 80, duration: 1.075s, episode steps: 89, steps per second: 83, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005414, mean_absolute_error: 0.287560, mean_q: 0.569591\n",
      "  7209/100000: episode: 81, duration: 0.914s, episode steps: 89, steps per second: 97, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005585, mean_absolute_error: 0.295841, mean_q: 0.585632\n",
      "  7298/100000: episode: 82, duration: 0.891s, episode steps: 89, steps per second: 100, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005290, mean_absolute_error: 0.297925, mean_q: 0.587505\n",
      "  7387/100000: episode: 83, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005453, mean_absolute_error: 0.293929, mean_q: 0.586119\n",
      "  7476/100000: episode: 84, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005128, mean_absolute_error: 0.286153, mean_q: 0.573069\n",
      "  7565/100000: episode: 85, duration: 0.913s, episode steps: 89, steps per second: 98, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005448, mean_absolute_error: 0.296470, mean_q: 0.588905\n",
      "  7654/100000: episode: 86, duration: 1.028s, episode steps: 89, steps per second: 87, episode reward: 1.857, mean reward: 0.021 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005514, mean_absolute_error: 0.306443, mean_q: 0.610235\n",
      "  7743/100000: episode: 87, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005241, mean_absolute_error: 0.315147, mean_q: 0.629171\n",
      "  7832/100000: episode: 88, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005215, mean_absolute_error: 0.311216, mean_q: 0.622377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  7921/100000: episode: 89, duration: 1.003s, episode steps: 89, steps per second: 89, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004992, mean_absolute_error: 0.301738, mean_q: 0.606918\n",
      "  8010/100000: episode: 90, duration: 0.965s, episode steps: 89, steps per second: 92, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005527, mean_absolute_error: 0.299463, mean_q: 0.601003\n",
      "  8099/100000: episode: 91, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005616, mean_absolute_error: 0.298160, mean_q: 0.592778\n",
      "  8188/100000: episode: 92, duration: 0.884s, episode steps: 89, steps per second: 101, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004736, mean_absolute_error: 0.297676, mean_q: 0.600053\n",
      "  8277/100000: episode: 93, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004695, mean_absolute_error: 0.292297, mean_q: 0.593978\n",
      "  8366/100000: episode: 94, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005201, mean_absolute_error: 0.304560, mean_q: 0.610331\n",
      "  8455/100000: episode: 95, duration: 0.974s, episode steps: 89, steps per second: 91, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004848, mean_absolute_error: 0.307160, mean_q: 0.614793\n",
      "  8544/100000: episode: 96, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005307, mean_absolute_error: 0.308792, mean_q: 0.621993\n",
      "  8633/100000: episode: 97, duration: 1.006s, episode steps: 89, steps per second: 88, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004900, mean_absolute_error: 0.309887, mean_q: 0.624246\n",
      "  8722/100000: episode: 98, duration: 0.891s, episode steps: 89, steps per second: 100, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005136, mean_absolute_error: 0.315050, mean_q: 0.628392\n",
      "  8811/100000: episode: 99, duration: 0.889s, episode steps: 89, steps per second: 100, episode reward: 1.857, mean reward: 0.021 [-0.143, 0.143], mean action: 0.360 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004780, mean_absolute_error: 0.315243, mean_q: 0.638651\n",
      "  8900/100000: episode: 100, duration: 0.957s, episode steps: 89, steps per second: 93, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005413, mean_absolute_error: 0.323821, mean_q: 0.645181\n",
      "  8989/100000: episode: 101, duration: 0.922s, episode steps: 89, steps per second: 97, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005088, mean_absolute_error: 0.311379, mean_q: 0.624489\n",
      "  9078/100000: episode: 102, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004830, mean_absolute_error: 0.310005, mean_q: 0.621188\n",
      "  9167/100000: episode: 103, duration: 1.112s, episode steps: 89, steps per second: 80, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004721, mean_absolute_error: 0.308358, mean_q: 0.624810\n",
      "  9256/100000: episode: 104, duration: 1.024s, episode steps: 89, steps per second: 87, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004838, mean_absolute_error: 0.315090, mean_q: 0.637564\n",
      "  9345/100000: episode: 105, duration: 0.876s, episode steps: 89, steps per second: 102, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005049, mean_absolute_error: 0.327444, mean_q: 0.658902\n",
      "  9434/100000: episode: 106, duration: 0.890s, episode steps: 89, steps per second: 100, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004837, mean_absolute_error: 0.327221, mean_q: 0.662661\n",
      "  9523/100000: episode: 107, duration: 0.907s, episode steps: 89, steps per second: 98, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004736, mean_absolute_error: 0.321414, mean_q: 0.652567\n",
      "  9612/100000: episode: 108, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004760, mean_absolute_error: 0.316046, mean_q: 0.635944\n",
      "  9701/100000: episode: 109, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004828, mean_absolute_error: 0.317556, mean_q: 0.639196\n",
      "  9790/100000: episode: 110, duration: 0.888s, episode steps: 89, steps per second: 100, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005126, mean_absolute_error: 0.322175, mean_q: 0.649240\n",
      "  9879/100000: episode: 111, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004910, mean_absolute_error: 0.323496, mean_q: 0.648590\n",
      "  9968/100000: episode: 112, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004947, mean_absolute_error: 0.320922, mean_q: 0.640508\n",
      " 10057/100000: episode: 113, duration: 0.990s, episode steps: 89, steps per second: 90, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005171, mean_absolute_error: 0.326410, mean_q: 0.653014\n",
      " 10146/100000: episode: 114, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005277, mean_absolute_error: 0.317759, mean_q: 0.632669\n",
      " 10235/100000: episode: 115, duration: 0.964s, episode steps: 89, steps per second: 92, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005200, mean_absolute_error: 0.314000, mean_q: 0.623127\n",
      " 10324/100000: episode: 116, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005314, mean_absolute_error: 0.316840, mean_q: 0.629397\n",
      " 10413/100000: episode: 117, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005612, mean_absolute_error: 0.329240, mean_q: 0.658668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 10502/100000: episode: 118, duration: 0.951s, episode steps: 89, steps per second: 94, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005631, mean_absolute_error: 0.321472, mean_q: 0.636128\n",
      " 10591/100000: episode: 119, duration: 0.952s, episode steps: 89, steps per second: 93, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005496, mean_absolute_error: 0.318627, mean_q: 0.630111\n",
      " 10680/100000: episode: 120, duration: 1.026s, episode steps: 89, steps per second: 87, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005455, mean_absolute_error: 0.317163, mean_q: 0.620612\n",
      " 10769/100000: episode: 121, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005385, mean_absolute_error: 0.309689, mean_q: 0.613017\n",
      " 10858/100000: episode: 122, duration: 1.007s, episode steps: 89, steps per second: 88, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005291, mean_absolute_error: 0.306277, mean_q: 0.612632\n",
      " 10947/100000: episode: 123, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005340, mean_absolute_error: 0.310297, mean_q: 0.616682\n",
      " 11036/100000: episode: 124, duration: 0.985s, episode steps: 89, steps per second: 90, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005622, mean_absolute_error: 0.311391, mean_q: 0.613763\n",
      " 11125/100000: episode: 125, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005581, mean_absolute_error: 0.310506, mean_q: 0.614417\n",
      " 11214/100000: episode: 126, duration: 0.868s, episode steps: 89, steps per second: 103, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005360, mean_absolute_error: 0.301004, mean_q: 0.593719\n",
      " 11303/100000: episode: 127, duration: 0.995s, episode steps: 89, steps per second: 89, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005627, mean_absolute_error: 0.295505, mean_q: 0.576503\n",
      " 11392/100000: episode: 128, duration: 1.040s, episode steps: 89, steps per second: 86, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005406, mean_absolute_error: 0.301400, mean_q: 0.588239\n",
      " 11481/100000: episode: 129, duration: 0.883s, episode steps: 89, steps per second: 101, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005505, mean_absolute_error: 0.292335, mean_q: 0.570482\n",
      " 11570/100000: episode: 130, duration: 0.895s, episode steps: 89, steps per second: 99, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005438, mean_absolute_error: 0.300012, mean_q: 0.594024\n",
      " 11659/100000: episode: 131, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005440, mean_absolute_error: 0.306208, mean_q: 0.600858\n",
      " 11748/100000: episode: 132, duration: 0.893s, episode steps: 89, steps per second: 100, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005221, mean_absolute_error: 0.313100, mean_q: 0.615983\n",
      " 11837/100000: episode: 133, duration: 1.045s, episode steps: 89, steps per second: 85, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005337, mean_absolute_error: 0.311041, mean_q: 0.609724\n",
      " 11926/100000: episode: 134, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005384, mean_absolute_error: 0.319539, mean_q: 0.620873\n",
      " 12015/100000: episode: 135, duration: 0.888s, episode steps: 89, steps per second: 100, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005645, mean_absolute_error: 0.318375, mean_q: 0.627403\n",
      " 12104/100000: episode: 136, duration: 0.895s, episode steps: 89, steps per second: 99, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005289, mean_absolute_error: 0.320427, mean_q: 0.635199\n",
      " 12193/100000: episode: 137, duration: 0.875s, episode steps: 89, steps per second: 102, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005183, mean_absolute_error: 0.317274, mean_q: 0.624525\n",
      " 12282/100000: episode: 138, duration: 1.014s, episode steps: 89, steps per second: 88, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005422, mean_absolute_error: 0.313305, mean_q: 0.616903\n",
      " 12371/100000: episode: 139, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 1.571, mean reward: 0.018 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005191, mean_absolute_error: 0.316149, mean_q: 0.619769\n",
      " 12460/100000: episode: 140, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005029, mean_absolute_error: 0.315747, mean_q: 0.621361\n",
      " 12549/100000: episode: 141, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004910, mean_absolute_error: 0.314429, mean_q: 0.623848\n",
      " 12638/100000: episode: 142, duration: 0.887s, episode steps: 89, steps per second: 100, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005464, mean_absolute_error: 0.313908, mean_q: 0.616391\n",
      " 12727/100000: episode: 143, duration: 0.948s, episode steps: 89, steps per second: 94, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005209, mean_absolute_error: 0.316557, mean_q: 0.619244\n",
      " 12816/100000: episode: 144, duration: 0.988s, episode steps: 89, steps per second: 90, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005241, mean_absolute_error: 0.315122, mean_q: 0.615100\n",
      " 12905/100000: episode: 145, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004961, mean_absolute_error: 0.318518, mean_q: 0.625476\n",
      " 12994/100000: episode: 146, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005168, mean_absolute_error: 0.314459, mean_q: 0.621989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13083/100000: episode: 147, duration: 0.880s, episode steps: 89, steps per second: 101, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005170, mean_absolute_error: 0.306830, mean_q: 0.603770\n",
      " 13172/100000: episode: 148, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004947, mean_absolute_error: 0.315673, mean_q: 0.628729\n",
      " 13261/100000: episode: 149, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005144, mean_absolute_error: 0.307690, mean_q: 0.608086\n",
      " 13350/100000: episode: 150, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005121, mean_absolute_error: 0.301838, mean_q: 0.590598\n",
      " 13439/100000: episode: 151, duration: 0.881s, episode steps: 89, steps per second: 101, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004616, mean_absolute_error: 0.296144, mean_q: 0.591304\n",
      " 13528/100000: episode: 152, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004895, mean_absolute_error: 0.304431, mean_q: 0.605868\n",
      " 13617/100000: episode: 153, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005290, mean_absolute_error: 0.299890, mean_q: 0.596108\n",
      " 13706/100000: episode: 154, duration: 0.893s, episode steps: 89, steps per second: 100, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005373, mean_absolute_error: 0.301720, mean_q: 0.598312\n",
      " 13795/100000: episode: 155, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004986, mean_absolute_error: 0.299466, mean_q: 0.599497\n",
      " 13884/100000: episode: 156, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005082, mean_absolute_error: 0.300111, mean_q: 0.597360\n",
      " 13973/100000: episode: 157, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004932, mean_absolute_error: 0.295777, mean_q: 0.590388\n",
      " 14062/100000: episode: 158, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.005024, mean_absolute_error: 0.298417, mean_q: 0.596054\n",
      " 14151/100000: episode: 159, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004800, mean_absolute_error: 0.313323, mean_q: 0.624625\n",
      " 14240/100000: episode: 160, duration: 1.004s, episode steps: 89, steps per second: 89, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004866, mean_absolute_error: 0.315719, mean_q: 0.630087\n",
      " 14329/100000: episode: 161, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004543, mean_absolute_error: 0.316476, mean_q: 0.636120\n",
      " 14418/100000: episode: 162, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004606, mean_absolute_error: 0.317699, mean_q: 0.643657\n",
      " 14507/100000: episode: 163, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004759, mean_absolute_error: 0.319274, mean_q: 0.641212\n",
      " 14596/100000: episode: 164, duration: 0.998s, episode steps: 89, steps per second: 89, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004700, mean_absolute_error: 0.307764, mean_q: 0.629076\n",
      " 14685/100000: episode: 165, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004790, mean_absolute_error: 0.330326, mean_q: 0.669063\n",
      " 14774/100000: episode: 166, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004946, mean_absolute_error: 0.339618, mean_q: 0.686993\n",
      " 14863/100000: episode: 167, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004750, mean_absolute_error: 0.328714, mean_q: 0.668317\n",
      " 14952/100000: episode: 168, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004271, mean_absolute_error: 0.331739, mean_q: 0.674431\n",
      " 15041/100000: episode: 169, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004589, mean_absolute_error: 0.334806, mean_q: 0.679127\n",
      " 15130/100000: episode: 170, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 1.857, mean reward: 0.021 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004460, mean_absolute_error: 0.331340, mean_q: 0.674266\n",
      " 15219/100000: episode: 171, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004431, mean_absolute_error: 0.326913, mean_q: 0.668447\n",
      " 15308/100000: episode: 172, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004657, mean_absolute_error: 0.320938, mean_q: 0.657463\n",
      " 15397/100000: episode: 173, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004147, mean_absolute_error: 0.315611, mean_q: 0.648246\n",
      " 15486/100000: episode: 174, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004078, mean_absolute_error: 0.319313, mean_q: 0.656447\n",
      " 15575/100000: episode: 175, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004419, mean_absolute_error: 0.330999, mean_q: 0.674432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15664/100000: episode: 176, duration: 0.893s, episode steps: 89, steps per second: 100, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004408, mean_absolute_error: 0.331223, mean_q: 0.681965\n",
      " 15753/100000: episode: 177, duration: 1.012s, episode steps: 89, steps per second: 88, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004280, mean_absolute_error: 0.331962, mean_q: 0.686789\n",
      " 15842/100000: episode: 178, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004234, mean_absolute_error: 0.332238, mean_q: 0.687565\n",
      " 15931/100000: episode: 179, duration: 1.278s, episode steps: 89, steps per second: 70, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004142, mean_absolute_error: 0.339652, mean_q: 0.702818\n",
      " 16020/100000: episode: 180, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004399, mean_absolute_error: 0.336057, mean_q: 0.690134\n",
      " 16109/100000: episode: 181, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 2.714, mean reward: 0.030 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004246, mean_absolute_error: 0.335929, mean_q: 0.692867\n",
      " 16198/100000: episode: 182, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004515, mean_absolute_error: 0.335677, mean_q: 0.690404\n",
      " 16287/100000: episode: 183, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004144, mean_absolute_error: 0.339997, mean_q: 0.706494\n",
      " 16376/100000: episode: 184, duration: 1.077s, episode steps: 89, steps per second: 83, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003915, mean_absolute_error: 0.339980, mean_q: 0.701442\n",
      " 16465/100000: episode: 185, duration: 1.215s, episode steps: 89, steps per second: 73, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003887, mean_absolute_error: 0.337742, mean_q: 0.701304\n",
      " 16554/100000: episode: 186, duration: 1.108s, episode steps: 89, steps per second: 80, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004146, mean_absolute_error: 0.345099, mean_q: 0.709667\n",
      " 16643/100000: episode: 187, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004174, mean_absolute_error: 0.339727, mean_q: 0.705595\n",
      " 16732/100000: episode: 188, duration: 1.074s, episode steps: 89, steps per second: 83, episode reward: 3.000, mean reward: 0.034 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004218, mean_absolute_error: 0.335937, mean_q: 0.698798\n",
      " 16821/100000: episode: 189, duration: 1.005s, episode steps: 89, steps per second: 89, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004294, mean_absolute_error: 0.332737, mean_q: 0.694351\n",
      " 16910/100000: episode: 190, duration: 0.958s, episode steps: 89, steps per second: 93, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003847, mean_absolute_error: 0.329903, mean_q: 0.693100\n",
      " 16999/100000: episode: 191, duration: 0.932s, episode steps: 89, steps per second: 95, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004154, mean_absolute_error: 0.333172, mean_q: 0.692507\n",
      " 17088/100000: episode: 192, duration: 1.030s, episode steps: 89, steps per second: 86, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003861, mean_absolute_error: 0.333142, mean_q: 0.696106\n",
      " 17177/100000: episode: 193, duration: 1.002s, episode steps: 89, steps per second: 89, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003875, mean_absolute_error: 0.332062, mean_q: 0.697437\n",
      " 17266/100000: episode: 194, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003745, mean_absolute_error: 0.325638, mean_q: 0.687871\n",
      " 17355/100000: episode: 195, duration: 1.057s, episode steps: 89, steps per second: 84, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003996, mean_absolute_error: 0.327204, mean_q: 0.685023\n",
      " 17444/100000: episode: 196, duration: 1.131s, episode steps: 89, steps per second: 79, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004116, mean_absolute_error: 0.332409, mean_q: 0.696566\n",
      " 17533/100000: episode: 197, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004081, mean_absolute_error: 0.332867, mean_q: 0.701458\n",
      " 17622/100000: episode: 198, duration: 1.062s, episode steps: 89, steps per second: 84, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003957, mean_absolute_error: 0.329779, mean_q: 0.692728\n",
      " 17711/100000: episode: 199, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004146, mean_absolute_error: 0.339392, mean_q: 0.708278\n",
      " 17800/100000: episode: 200, duration: 0.864s, episode steps: 89, steps per second: 103, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003880, mean_absolute_error: 0.332507, mean_q: 0.696840\n",
      " 17889/100000: episode: 201, duration: 1.034s, episode steps: 89, steps per second: 86, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004368, mean_absolute_error: 0.332125, mean_q: 0.696135\n",
      " 17978/100000: episode: 202, duration: 0.978s, episode steps: 89, steps per second: 91, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003999, mean_absolute_error: 0.337910, mean_q: 0.707867\n",
      " 18067/100000: episode: 203, duration: 0.914s, episode steps: 89, steps per second: 97, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004041, mean_absolute_error: 0.342660, mean_q: 0.718930\n",
      " 18156/100000: episode: 204, duration: 0.885s, episode steps: 89, steps per second: 101, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003468, mean_absolute_error: 0.341484, mean_q: 0.716126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 18245/100000: episode: 205, duration: 0.887s, episode steps: 89, steps per second: 100, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003682, mean_absolute_error: 0.334531, mean_q: 0.712134\n",
      " 18334/100000: episode: 206, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004095, mean_absolute_error: 0.330329, mean_q: 0.702727\n",
      " 18423/100000: episode: 207, duration: 0.992s, episode steps: 89, steps per second: 90, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004128, mean_absolute_error: 0.333262, mean_q: 0.705967\n",
      " 18512/100000: episode: 208, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003969, mean_absolute_error: 0.338563, mean_q: 0.713671\n",
      " 18601/100000: episode: 209, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004177, mean_absolute_error: 0.334628, mean_q: 0.710388\n",
      " 18690/100000: episode: 210, duration: 0.922s, episode steps: 89, steps per second: 97, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003612, mean_absolute_error: 0.337751, mean_q: 0.714791\n",
      " 18779/100000: episode: 211, duration: 0.893s, episode steps: 89, steps per second: 100, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003858, mean_absolute_error: 0.333359, mean_q: 0.716759\n",
      " 18868/100000: episode: 212, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003673, mean_absolute_error: 0.333048, mean_q: 0.717076\n",
      " 18957/100000: episode: 213, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003697, mean_absolute_error: 0.339887, mean_q: 0.727528\n",
      " 19046/100000: episode: 214, duration: 1.087s, episode steps: 89, steps per second: 82, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003595, mean_absolute_error: 0.355462, mean_q: 0.754877\n",
      " 19135/100000: episode: 215, duration: 1.112s, episode steps: 89, steps per second: 80, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003766, mean_absolute_error: 0.340781, mean_q: 0.732727\n",
      " 19224/100000: episode: 216, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003578, mean_absolute_error: 0.338534, mean_q: 0.730104\n",
      " 19313/100000: episode: 217, duration: 1.085s, episode steps: 89, steps per second: 82, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003894, mean_absolute_error: 0.343203, mean_q: 0.736640\n",
      " 19402/100000: episode: 218, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003447, mean_absolute_error: 0.352756, mean_q: 0.751094\n",
      " 19491/100000: episode: 219, duration: 0.883s, episode steps: 89, steps per second: 101, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003870, mean_absolute_error: 0.365592, mean_q: 0.775084\n",
      " 19580/100000: episode: 220, duration: 0.877s, episode steps: 89, steps per second: 102, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003579, mean_absolute_error: 0.374439, mean_q: 0.789198\n",
      " 19669/100000: episode: 221, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003380, mean_absolute_error: 0.373273, mean_q: 0.786529\n",
      " 19758/100000: episode: 222, duration: 1.042s, episode steps: 89, steps per second: 85, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003241, mean_absolute_error: 0.371320, mean_q: 0.788625\n",
      " 19847/100000: episode: 223, duration: 1.021s, episode steps: 89, steps per second: 87, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003366, mean_absolute_error: 0.369196, mean_q: 0.785220\n",
      " 19936/100000: episode: 224, duration: 0.972s, episode steps: 89, steps per second: 92, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003493, mean_absolute_error: 0.371449, mean_q: 0.781328\n",
      " 20025/100000: episode: 225, duration: 0.913s, episode steps: 89, steps per second: 97, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003315, mean_absolute_error: 0.364206, mean_q: 0.779118\n",
      " 20114/100000: episode: 226, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003374, mean_absolute_error: 0.354722, mean_q: 0.755721\n",
      " 20203/100000: episode: 227, duration: 0.964s, episode steps: 89, steps per second: 92, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003739, mean_absolute_error: 0.367024, mean_q: 0.776196\n",
      " 20292/100000: episode: 228, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003400, mean_absolute_error: 0.367566, mean_q: 0.778256\n",
      " 20381/100000: episode: 229, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003269, mean_absolute_error: 0.357076, mean_q: 0.763184\n",
      " 20470/100000: episode: 230, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003207, mean_absolute_error: 0.357434, mean_q: 0.767105\n",
      " 20559/100000: episode: 231, duration: 1.059s, episode steps: 89, steps per second: 84, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003246, mean_absolute_error: 0.359754, mean_q: 0.773189\n",
      " 20648/100000: episode: 232, duration: 1.085s, episode steps: 89, steps per second: 82, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003205, mean_absolute_error: 0.370970, mean_q: 0.789651\n",
      " 20737/100000: episode: 233, duration: 0.960s, episode steps: 89, steps per second: 93, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003151, mean_absolute_error: 0.368540, mean_q: 0.790266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20826/100000: episode: 234, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003170, mean_absolute_error: 0.374202, mean_q: 0.792217\n",
      " 20915/100000: episode: 235, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003131, mean_absolute_error: 0.376922, mean_q: 0.800271\n",
      " 21004/100000: episode: 236, duration: 0.952s, episode steps: 89, steps per second: 93, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003299, mean_absolute_error: 0.379295, mean_q: 0.804663\n",
      " 21093/100000: episode: 237, duration: 0.983s, episode steps: 89, steps per second: 91, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002931, mean_absolute_error: 0.378415, mean_q: 0.807335\n",
      " 21182/100000: episode: 238, duration: 1.041s, episode steps: 89, steps per second: 85, episode reward: 2.429, mean reward: 0.027 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003356, mean_absolute_error: 0.385108, mean_q: 0.807194\n",
      " 21271/100000: episode: 239, duration: 0.946s, episode steps: 89, steps per second: 94, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002989, mean_absolute_error: 0.378843, mean_q: 0.797412\n",
      " 21360/100000: episode: 240, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003159, mean_absolute_error: 0.369929, mean_q: 0.784023\n",
      " 21449/100000: episode: 241, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002974, mean_absolute_error: 0.367357, mean_q: 0.785953\n",
      " 21538/100000: episode: 242, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.573 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003058, mean_absolute_error: 0.368099, mean_q: 0.786900\n",
      " 21627/100000: episode: 243, duration: 1.011s, episode steps: 89, steps per second: 88, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003422, mean_absolute_error: 0.377901, mean_q: 0.797280\n",
      " 21716/100000: episode: 244, duration: 1.112s, episode steps: 89, steps per second: 80, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003018, mean_absolute_error: 0.376904, mean_q: 0.801745\n",
      " 21805/100000: episode: 245, duration: 0.982s, episode steps: 89, steps per second: 91, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002804, mean_absolute_error: 0.374311, mean_q: 0.797218\n",
      " 21894/100000: episode: 246, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003087, mean_absolute_error: 0.375510, mean_q: 0.801110\n",
      " 21983/100000: episode: 247, duration: 1.013s, episode steps: 89, steps per second: 88, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003235, mean_absolute_error: 0.382071, mean_q: 0.808689\n",
      " 22072/100000: episode: 248, duration: 1.351s, episode steps: 89, steps per second: 66, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002857, mean_absolute_error: 0.379527, mean_q: 0.811438\n",
      " 22161/100000: episode: 249, duration: 0.907s, episode steps: 89, steps per second: 98, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003225, mean_absolute_error: 0.391497, mean_q: 0.823287\n",
      " 22250/100000: episode: 250, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003013, mean_absolute_error: 0.399944, mean_q: 0.846123\n",
      " 22339/100000: episode: 251, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003231, mean_absolute_error: 0.407327, mean_q: 0.858118\n",
      " 22428/100000: episode: 252, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003498, mean_absolute_error: 0.403074, mean_q: 0.852766\n",
      " 22517/100000: episode: 253, duration: 1.007s, episode steps: 89, steps per second: 88, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003484, mean_absolute_error: 0.417191, mean_q: 0.879296\n",
      " 22606/100000: episode: 254, duration: 0.992s, episode steps: 89, steps per second: 90, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003092, mean_absolute_error: 0.400087, mean_q: 0.856429\n",
      " 22695/100000: episode: 255, duration: 1.030s, episode steps: 89, steps per second: 86, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003351, mean_absolute_error: 0.401002, mean_q: 0.848226\n",
      " 22784/100000: episode: 256, duration: 0.962s, episode steps: 89, steps per second: 93, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003460, mean_absolute_error: 0.398763, mean_q: 0.851887\n",
      " 22873/100000: episode: 257, duration: 1.078s, episode steps: 89, steps per second: 83, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003457, mean_absolute_error: 0.411561, mean_q: 0.866221\n",
      " 22962/100000: episode: 258, duration: 0.948s, episode steps: 89, steps per second: 94, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003490, mean_absolute_error: 0.408220, mean_q: 0.864718\n",
      " 23051/100000: episode: 259, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003881, mean_absolute_error: 0.422253, mean_q: 0.898290\n",
      " 23140/100000: episode: 260, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003568, mean_absolute_error: 0.414538, mean_q: 0.884509\n",
      " 23229/100000: episode: 261, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003650, mean_absolute_error: 0.411433, mean_q: 0.873917\n",
      " 23318/100000: episode: 262, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003278, mean_absolute_error: 0.413619, mean_q: 0.875617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23407/100000: episode: 263, duration: 0.956s, episode steps: 89, steps per second: 93, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003535, mean_absolute_error: 0.421986, mean_q: 0.892962\n",
      " 23496/100000: episode: 264, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003324, mean_absolute_error: 0.412040, mean_q: 0.874422\n",
      " 23585/100000: episode: 265, duration: 0.917s, episode steps: 89, steps per second: 97, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003226, mean_absolute_error: 0.426273, mean_q: 0.903727\n",
      " 23674/100000: episode: 266, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003854, mean_absolute_error: 0.426123, mean_q: 0.894298\n",
      " 23763/100000: episode: 267, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003477, mean_absolute_error: 0.423510, mean_q: 0.893990\n",
      " 23852/100000: episode: 268, duration: 0.947s, episode steps: 89, steps per second: 94, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003403, mean_absolute_error: 0.424752, mean_q: 0.899779\n",
      " 23941/100000: episode: 269, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003804, mean_absolute_error: 0.425129, mean_q: 0.898141\n",
      " 24030/100000: episode: 270, duration: 0.913s, episode steps: 89, steps per second: 97, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003475, mean_absolute_error: 0.419466, mean_q: 0.889166\n",
      " 24119/100000: episode: 271, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003769, mean_absolute_error: 0.425842, mean_q: 0.901776\n",
      " 24208/100000: episode: 272, duration: 0.969s, episode steps: 89, steps per second: 92, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003502, mean_absolute_error: 0.422449, mean_q: 0.897566\n",
      " 24297/100000: episode: 273, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003383, mean_absolute_error: 0.431024, mean_q: 0.912047\n",
      " 24386/100000: episode: 274, duration: 0.976s, episode steps: 89, steps per second: 91, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.584 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003636, mean_absolute_error: 0.430397, mean_q: 0.911647\n",
      " 24475/100000: episode: 275, duration: 1.085s, episode steps: 89, steps per second: 82, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003356, mean_absolute_error: 0.428444, mean_q: 0.911038\n",
      " 24564/100000: episode: 276, duration: 1.122s, episode steps: 89, steps per second: 79, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003333, mean_absolute_error: 0.434225, mean_q: 0.917775\n",
      " 24653/100000: episode: 277, duration: 1.110s, episode steps: 89, steps per second: 80, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003359, mean_absolute_error: 0.434510, mean_q: 0.918800\n",
      " 24742/100000: episode: 278, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003452, mean_absolute_error: 0.451848, mean_q: 0.951490\n",
      " 24831/100000: episode: 279, duration: 0.987s, episode steps: 89, steps per second: 90, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.607 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003517, mean_absolute_error: 0.456717, mean_q: 0.957304\n",
      " 24920/100000: episode: 280, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003700, mean_absolute_error: 0.455121, mean_q: 0.959647\n",
      " 25009/100000: episode: 281, duration: 0.922s, episode steps: 89, steps per second: 97, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.573 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003433, mean_absolute_error: 0.454264, mean_q: 0.960277\n",
      " 25098/100000: episode: 282, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003380, mean_absolute_error: 0.445731, mean_q: 0.945884\n",
      " 25187/100000: episode: 283, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003336, mean_absolute_error: 0.442530, mean_q: 0.934614\n",
      " 25276/100000: episode: 284, duration: 0.891s, episode steps: 89, steps per second: 100, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003834, mean_absolute_error: 0.451001, mean_q: 0.942504\n",
      " 25365/100000: episode: 285, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003361, mean_absolute_error: 0.442972, mean_q: 0.928142\n",
      " 25454/100000: episode: 286, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003667, mean_absolute_error: 0.445366, mean_q: 0.936365\n",
      " 25543/100000: episode: 287, duration: 0.914s, episode steps: 89, steps per second: 97, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.573 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003636, mean_absolute_error: 0.461673, mean_q: 0.965982\n",
      " 25632/100000: episode: 288, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003798, mean_absolute_error: 0.470760, mean_q: 0.984145\n",
      " 25721/100000: episode: 289, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003507, mean_absolute_error: 0.467163, mean_q: 0.976962\n",
      " 25810/100000: episode: 290, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003363, mean_absolute_error: 0.441536, mean_q: 0.933325\n",
      " 25899/100000: episode: 291, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003646, mean_absolute_error: 0.439183, mean_q: 0.937922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 25988/100000: episode: 292, duration: 0.988s, episode steps: 89, steps per second: 90, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003697, mean_absolute_error: 0.447759, mean_q: 0.943541\n",
      " 26077/100000: episode: 293, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003565, mean_absolute_error: 0.448440, mean_q: 0.941873\n",
      " 26166/100000: episode: 294, duration: 1.146s, episode steps: 89, steps per second: 78, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003749, mean_absolute_error: 0.444459, mean_q: 0.942829\n",
      " 26255/100000: episode: 295, duration: 1.067s, episode steps: 89, steps per second: 83, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003429, mean_absolute_error: 0.455435, mean_q: 0.960972\n",
      " 26344/100000: episode: 296, duration: 1.089s, episode steps: 89, steps per second: 82, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003430, mean_absolute_error: 0.456208, mean_q: 0.967840\n",
      " 26433/100000: episode: 297, duration: 1.021s, episode steps: 89, steps per second: 87, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003660, mean_absolute_error: 0.461946, mean_q: 0.975069\n",
      " 26522/100000: episode: 298, duration: 0.991s, episode steps: 89, steps per second: 90, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.607 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003306, mean_absolute_error: 0.464603, mean_q: 0.986437\n",
      " 26611/100000: episode: 299, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003427, mean_absolute_error: 0.463825, mean_q: 0.985733\n",
      " 26700/100000: episode: 300, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003876, mean_absolute_error: 0.464396, mean_q: 0.982109\n",
      " 26789/100000: episode: 301, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003806, mean_absolute_error: 0.472311, mean_q: 0.997346\n",
      " 26878/100000: episode: 302, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003996, mean_absolute_error: 0.465371, mean_q: 0.979541\n",
      " 26967/100000: episode: 303, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003681, mean_absolute_error: 0.460067, mean_q: 0.974298\n",
      " 27056/100000: episode: 304, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003998, mean_absolute_error: 0.463109, mean_q: 0.973933\n",
      " 27145/100000: episode: 305, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003700, mean_absolute_error: 0.459684, mean_q: 0.972157\n",
      " 27234/100000: episode: 306, duration: 0.913s, episode steps: 89, steps per second: 97, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003575, mean_absolute_error: 0.461527, mean_q: 0.979931\n",
      " 27323/100000: episode: 307, duration: 0.979s, episode steps: 89, steps per second: 91, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003687, mean_absolute_error: 0.462320, mean_q: 0.981642\n",
      " 27412/100000: episode: 308, duration: 0.955s, episode steps: 89, steps per second: 93, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003562, mean_absolute_error: 0.470432, mean_q: 0.987515\n",
      " 27501/100000: episode: 309, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003681, mean_absolute_error: 0.479447, mean_q: 1.005413\n",
      " 27590/100000: episode: 310, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003646, mean_absolute_error: 0.483731, mean_q: 1.013595\n",
      " 27679/100000: episode: 311, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 3.286, mean reward: 0.037 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003577, mean_absolute_error: 0.482489, mean_q: 1.019214\n",
      " 27768/100000: episode: 312, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003561, mean_absolute_error: 0.478639, mean_q: 1.012748\n",
      " 27857/100000: episode: 313, duration: 0.982s, episode steps: 89, steps per second: 91, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003709, mean_absolute_error: 0.472034, mean_q: 1.005141\n",
      " 27946/100000: episode: 314, duration: 0.952s, episode steps: 89, steps per second: 94, episode reward: 3.571, mean reward: 0.040 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004189, mean_absolute_error: 0.468732, mean_q: 0.993435\n",
      " 28035/100000: episode: 315, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003625, mean_absolute_error: 0.478767, mean_q: 1.009393\n",
      " 28124/100000: episode: 316, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.584 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003839, mean_absolute_error: 0.491673, mean_q: 1.037324\n",
      " 28213/100000: episode: 317, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003967, mean_absolute_error: 0.480088, mean_q: 1.013104\n",
      " 28302/100000: episode: 318, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003507, mean_absolute_error: 0.470833, mean_q: 0.997030\n",
      " 28391/100000: episode: 319, duration: 0.954s, episode steps: 89, steps per second: 93, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003665, mean_absolute_error: 0.464627, mean_q: 0.984748\n",
      " 28480/100000: episode: 320, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003937, mean_absolute_error: 0.467345, mean_q: 0.992348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28569/100000: episode: 321, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003889, mean_absolute_error: 0.466196, mean_q: 0.990159\n",
      " 28658/100000: episode: 322, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003739, mean_absolute_error: 0.478791, mean_q: 1.012237\n",
      " 28747/100000: episode: 323, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003736, mean_absolute_error: 0.481915, mean_q: 1.022945\n",
      " 28836/100000: episode: 324, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003703, mean_absolute_error: 0.485385, mean_q: 1.023497\n",
      " 28925/100000: episode: 325, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003449, mean_absolute_error: 0.487971, mean_q: 1.042499\n",
      " 29014/100000: episode: 326, duration: 0.977s, episode steps: 89, steps per second: 91, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003456, mean_absolute_error: 0.497348, mean_q: 1.055405\n",
      " 29103/100000: episode: 327, duration: 0.952s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003932, mean_absolute_error: 0.495540, mean_q: 1.055726\n",
      " 29192/100000: episode: 328, duration: 0.907s, episode steps: 89, steps per second: 98, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003876, mean_absolute_error: 0.487380, mean_q: 1.041350\n",
      " 29281/100000: episode: 329, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003952, mean_absolute_error: 0.491038, mean_q: 1.038196\n",
      " 29370/100000: episode: 330, duration: 0.998s, episode steps: 89, steps per second: 89, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003958, mean_absolute_error: 0.495392, mean_q: 1.047651\n",
      " 29459/100000: episode: 331, duration: 1.154s, episode steps: 89, steps per second: 77, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004200, mean_absolute_error: 0.503329, mean_q: 1.067203\n",
      " 29548/100000: episode: 332, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003801, mean_absolute_error: 0.508203, mean_q: 1.074454\n",
      " 29637/100000: episode: 333, duration: 1.037s, episode steps: 89, steps per second: 86, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003879, mean_absolute_error: 0.508255, mean_q: 1.072560\n",
      " 29726/100000: episode: 334, duration: 1.015s, episode steps: 89, steps per second: 88, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.573 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004293, mean_absolute_error: 0.508774, mean_q: 1.081109\n",
      " 29815/100000: episode: 335, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003783, mean_absolute_error: 0.495586, mean_q: 1.052333\n",
      " 29904/100000: episode: 336, duration: 1.109s, episode steps: 89, steps per second: 80, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003703, mean_absolute_error: 0.503315, mean_q: 1.069972\n",
      " 29993/100000: episode: 337, duration: 0.913s, episode steps: 89, steps per second: 98, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.573 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004011, mean_absolute_error: 0.520864, mean_q: 1.098163\n",
      " 30082/100000: episode: 338, duration: 1.079s, episode steps: 89, steps per second: 82, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003960, mean_absolute_error: 0.516166, mean_q: 1.092667\n",
      " 30171/100000: episode: 339, duration: 1.078s, episode steps: 89, steps per second: 83, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003462, mean_absolute_error: 0.520175, mean_q: 1.098501\n",
      " 30260/100000: episode: 340, duration: 0.990s, episode steps: 89, steps per second: 90, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003880, mean_absolute_error: 0.522072, mean_q: 1.107457\n",
      " 30349/100000: episode: 341, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004325, mean_absolute_error: 0.522436, mean_q: 1.106633\n",
      " 30438/100000: episode: 342, duration: 1.111s, episode steps: 89, steps per second: 80, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003682, mean_absolute_error: 0.517427, mean_q: 1.092124\n",
      " 30527/100000: episode: 343, duration: 1.051s, episode steps: 89, steps per second: 85, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003809, mean_absolute_error: 0.512396, mean_q: 1.086682\n",
      " 30616/100000: episode: 344, duration: 0.907s, episode steps: 89, steps per second: 98, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004070, mean_absolute_error: 0.529566, mean_q: 1.114247\n",
      " 30705/100000: episode: 345, duration: 1.088s, episode steps: 89, steps per second: 82, episode reward: 3.857, mean reward: 0.043 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004092, mean_absolute_error: 0.524640, mean_q: 1.098610\n",
      " 30794/100000: episode: 346, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004297, mean_absolute_error: 0.530299, mean_q: 1.118153\n",
      " 30883/100000: episode: 347, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003763, mean_absolute_error: 0.518656, mean_q: 1.096216\n",
      " 30972/100000: episode: 348, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003849, mean_absolute_error: 0.544006, mean_q: 1.137948\n",
      " 31061/100000: episode: 349, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004312, mean_absolute_error: 0.545192, mean_q: 1.143800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 31150/100000: episode: 350, duration: 0.999s, episode steps: 89, steps per second: 89, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003844, mean_absolute_error: 0.526295, mean_q: 1.107486\n",
      " 31239/100000: episode: 351, duration: 0.985s, episode steps: 89, steps per second: 90, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004269, mean_absolute_error: 0.522560, mean_q: 1.101689\n",
      " 31328/100000: episode: 352, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003980, mean_absolute_error: 0.530114, mean_q: 1.118204\n",
      " 31417/100000: episode: 353, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004128, mean_absolute_error: 0.539593, mean_q: 1.133603\n",
      " 31506/100000: episode: 354, duration: 1.041s, episode steps: 89, steps per second: 85, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003771, mean_absolute_error: 0.549072, mean_q: 1.154688\n",
      " 31595/100000: episode: 355, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003471, mean_absolute_error: 0.534729, mean_q: 1.132756\n",
      " 31684/100000: episode: 356, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004559, mean_absolute_error: 0.522028, mean_q: 1.100739\n",
      " 31773/100000: episode: 357, duration: 1.082s, episode steps: 89, steps per second: 82, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004068, mean_absolute_error: 0.530306, mean_q: 1.121475\n",
      " 31862/100000: episode: 358, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003684, mean_absolute_error: 0.532833, mean_q: 1.130106\n",
      " 31951/100000: episode: 359, duration: 0.911s, episode steps: 89, steps per second: 98, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004033, mean_absolute_error: 0.543443, mean_q: 1.136467\n",
      " 32040/100000: episode: 360, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004072, mean_absolute_error: 0.554484, mean_q: 1.171646\n",
      " 32129/100000: episode: 361, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004089, mean_absolute_error: 0.547932, mean_q: 1.149126\n",
      " 32218/100000: episode: 362, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004067, mean_absolute_error: 0.550901, mean_q: 1.159554\n",
      " 32307/100000: episode: 363, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004322, mean_absolute_error: 0.545557, mean_q: 1.150158\n",
      " 32396/100000: episode: 364, duration: 0.896s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004217, mean_absolute_error: 0.541878, mean_q: 1.136274\n",
      " 32485/100000: episode: 365, duration: 0.895s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004106, mean_absolute_error: 0.537855, mean_q: 1.127565\n",
      " 32574/100000: episode: 366, duration: 1.141s, episode steps: 89, steps per second: 78, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004212, mean_absolute_error: 0.531152, mean_q: 1.121827\n",
      " 32663/100000: episode: 367, duration: 1.124s, episode steps: 89, steps per second: 79, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004039, mean_absolute_error: 0.542843, mean_q: 1.146903\n",
      " 32752/100000: episode: 368, duration: 1.113s, episode steps: 89, steps per second: 80, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004465, mean_absolute_error: 0.549785, mean_q: 1.157808\n",
      " 32841/100000: episode: 369, duration: 0.896s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004021, mean_absolute_error: 0.560768, mean_q: 1.170592\n",
      " 32930/100000: episode: 370, duration: 0.887s, episode steps: 89, steps per second: 100, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004572, mean_absolute_error: 0.544931, mean_q: 1.144521\n",
      " 33019/100000: episode: 371, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004193, mean_absolute_error: 0.550316, mean_q: 1.157200\n",
      " 33108/100000: episode: 372, duration: 0.884s, episode steps: 89, steps per second: 101, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004258, mean_absolute_error: 0.556848, mean_q: 1.164359\n",
      " 33197/100000: episode: 373, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004172, mean_absolute_error: 0.554291, mean_q: 1.169278\n",
      " 33286/100000: episode: 374, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004361, mean_absolute_error: 0.549471, mean_q: 1.160568\n",
      " 33375/100000: episode: 375, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004138, mean_absolute_error: 0.564677, mean_q: 1.182443\n",
      " 33464/100000: episode: 376, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004236, mean_absolute_error: 0.560149, mean_q: 1.185037\n",
      " 33553/100000: episode: 377, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004590, mean_absolute_error: 0.579029, mean_q: 1.210149\n",
      " 33642/100000: episode: 378, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004835, mean_absolute_error: 0.584220, mean_q: 1.216264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 33731/100000: episode: 379, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004663, mean_absolute_error: 0.586463, mean_q: 1.223447\n",
      " 33820/100000: episode: 380, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004846, mean_absolute_error: 0.586542, mean_q: 1.228329\n",
      " 33909/100000: episode: 381, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004635, mean_absolute_error: 0.571799, mean_q: 1.192355\n",
      " 33998/100000: episode: 382, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004397, mean_absolute_error: 0.581523, mean_q: 1.216098\n",
      " 34087/100000: episode: 383, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004271, mean_absolute_error: 0.576075, mean_q: 1.210205\n",
      " 34176/100000: episode: 384, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004488, mean_absolute_error: 0.560423, mean_q: 1.179598\n",
      " 34265/100000: episode: 385, duration: 0.885s, episode steps: 89, steps per second: 101, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004422, mean_absolute_error: 0.559633, mean_q: 1.168693\n",
      " 34354/100000: episode: 386, duration: 1.077s, episode steps: 89, steps per second: 83, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004369, mean_absolute_error: 0.550250, mean_q: 1.148814\n",
      " 34443/100000: episode: 387, duration: 0.944s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004587, mean_absolute_error: 0.566720, mean_q: 1.181729\n",
      " 34532/100000: episode: 388, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004345, mean_absolute_error: 0.561501, mean_q: 1.177286\n",
      " 34621/100000: episode: 389, duration: 0.886s, episode steps: 89, steps per second: 100, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004443, mean_absolute_error: 0.557998, mean_q: 1.175515\n",
      " 34710/100000: episode: 390, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004945, mean_absolute_error: 0.571334, mean_q: 1.187024\n",
      " 34799/100000: episode: 391, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004564, mean_absolute_error: 0.583146, mean_q: 1.219844\n",
      " 34888/100000: episode: 392, duration: 0.877s, episode steps: 89, steps per second: 101, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004534, mean_absolute_error: 0.581333, mean_q: 1.218670\n",
      " 34977/100000: episode: 393, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004641, mean_absolute_error: 0.580657, mean_q: 1.213187\n",
      " 35066/100000: episode: 394, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004563, mean_absolute_error: 0.591593, mean_q: 1.240871\n",
      " 35155/100000: episode: 395, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004276, mean_absolute_error: 0.588294, mean_q: 1.231309\n",
      " 35244/100000: episode: 396, duration: 0.890s, episode steps: 89, steps per second: 100, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004369, mean_absolute_error: 0.584580, mean_q: 1.223950\n",
      " 35333/100000: episode: 397, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004550, mean_absolute_error: 0.576344, mean_q: 1.208153\n",
      " 35422/100000: episode: 398, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004702, mean_absolute_error: 0.575376, mean_q: 1.206710\n",
      " 35511/100000: episode: 399, duration: 0.884s, episode steps: 89, steps per second: 101, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004552, mean_absolute_error: 0.582045, mean_q: 1.220229\n",
      " 35600/100000: episode: 400, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004304, mean_absolute_error: 0.591099, mean_q: 1.235091\n",
      " 35689/100000: episode: 401, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004372, mean_absolute_error: 0.583564, mean_q: 1.226796\n",
      " 35778/100000: episode: 402, duration: 0.882s, episode steps: 89, steps per second: 101, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004389, mean_absolute_error: 0.594172, mean_q: 1.237538\n",
      " 35867/100000: episode: 403, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004241, mean_absolute_error: 0.593528, mean_q: 1.245132\n",
      " 35956/100000: episode: 404, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 4.429, mean reward: 0.050 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004489, mean_absolute_error: 0.601156, mean_q: 1.256546\n",
      " 36045/100000: episode: 405, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004874, mean_absolute_error: 0.595248, mean_q: 1.236892\n",
      " 36134/100000: episode: 406, duration: 0.983s, episode steps: 89, steps per second: 91, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004366, mean_absolute_error: 0.604825, mean_q: 1.265900\n",
      " 36223/100000: episode: 407, duration: 0.888s, episode steps: 89, steps per second: 100, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004484, mean_absolute_error: 0.608062, mean_q: 1.272870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36312/100000: episode: 408, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004311, mean_absolute_error: 0.612655, mean_q: 1.280443\n",
      " 36401/100000: episode: 409, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004059, mean_absolute_error: 0.616657, mean_q: 1.294347\n",
      " 36490/100000: episode: 410, duration: 0.982s, episode steps: 89, steps per second: 91, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004611, mean_absolute_error: 0.609280, mean_q: 1.270066\n",
      " 36579/100000: episode: 411, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004621, mean_absolute_error: 0.594201, mean_q: 1.243133\n",
      " 36668/100000: episode: 412, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.551 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004178, mean_absolute_error: 0.594981, mean_q: 1.246558\n",
      " 36757/100000: episode: 413, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004380, mean_absolute_error: 0.592261, mean_q: 1.236121\n",
      " 36846/100000: episode: 414, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003705, mean_absolute_error: 0.580847, mean_q: 1.222538\n",
      " 36935/100000: episode: 415, duration: 0.973s, episode steps: 89, steps per second: 91, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004175, mean_absolute_error: 0.581362, mean_q: 1.225431\n",
      " 37024/100000: episode: 416, duration: 0.974s, episode steps: 89, steps per second: 91, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004462, mean_absolute_error: 0.591704, mean_q: 1.234848\n",
      " 37113/100000: episode: 417, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004369, mean_absolute_error: 0.590303, mean_q: 1.233645\n",
      " 37202/100000: episode: 418, duration: 1.005s, episode steps: 89, steps per second: 89, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004033, mean_absolute_error: 0.584200, mean_q: 1.220782\n",
      " 37291/100000: episode: 419, duration: 1.089s, episode steps: 89, steps per second: 82, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004698, mean_absolute_error: 0.603303, mean_q: 1.260828\n",
      " 37380/100000: episode: 420, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004267, mean_absolute_error: 0.604141, mean_q: 1.263692\n",
      " 37469/100000: episode: 421, duration: 0.890s, episode steps: 89, steps per second: 100, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004227, mean_absolute_error: 0.610068, mean_q: 1.268579\n",
      " 37558/100000: episode: 422, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004162, mean_absolute_error: 0.596355, mean_q: 1.255682\n",
      " 37647/100000: episode: 423, duration: 0.893s, episode steps: 89, steps per second: 100, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004368, mean_absolute_error: 0.601654, mean_q: 1.256917\n",
      " 37736/100000: episode: 424, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004232, mean_absolute_error: 0.602765, mean_q: 1.259827\n",
      " 37825/100000: episode: 425, duration: 0.911s, episode steps: 89, steps per second: 98, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004249, mean_absolute_error: 0.604621, mean_q: 1.269707\n",
      " 37914/100000: episode: 426, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004460, mean_absolute_error: 0.607664, mean_q: 1.268981\n",
      " 38003/100000: episode: 427, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003913, mean_absolute_error: 0.597246, mean_q: 1.256700\n",
      " 38092/100000: episode: 428, duration: 0.888s, episode steps: 89, steps per second: 100, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004096, mean_absolute_error: 0.594474, mean_q: 1.251495\n",
      " 38181/100000: episode: 429, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004078, mean_absolute_error: 0.590658, mean_q: 1.235695\n",
      " 38270/100000: episode: 430, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004373, mean_absolute_error: 0.597333, mean_q: 1.246630\n",
      " 38359/100000: episode: 431, duration: 0.914s, episode steps: 89, steps per second: 97, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004238, mean_absolute_error: 0.605588, mean_q: 1.264959\n",
      " 38448/100000: episode: 432, duration: 0.895s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004285, mean_absolute_error: 0.598577, mean_q: 1.248869\n",
      " 38537/100000: episode: 433, duration: 0.913s, episode steps: 89, steps per second: 98, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004169, mean_absolute_error: 0.596968, mean_q: 1.252177\n",
      " 38626/100000: episode: 434, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004534, mean_absolute_error: 0.590128, mean_q: 1.234600\n",
      " 38715/100000: episode: 435, duration: 0.889s, episode steps: 89, steps per second: 100, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003944, mean_absolute_error: 0.573471, mean_q: 1.203093\n",
      " 38804/100000: episode: 436, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003981, mean_absolute_error: 0.584778, mean_q: 1.227582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 38893/100000: episode: 437, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003937, mean_absolute_error: 0.574711, mean_q: 1.216318\n",
      " 38982/100000: episode: 438, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004109, mean_absolute_error: 0.581737, mean_q: 1.217376\n",
      " 39071/100000: episode: 439, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004291, mean_absolute_error: 0.585649, mean_q: 1.222193\n",
      " 39160/100000: episode: 440, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004188, mean_absolute_error: 0.578024, mean_q: 1.210977\n",
      " 39249/100000: episode: 441, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004068, mean_absolute_error: 0.578540, mean_q: 1.212352\n",
      " 39338/100000: episode: 442, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003633, mean_absolute_error: 0.584839, mean_q: 1.228896\n",
      " 39427/100000: episode: 443, duration: 0.922s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004027, mean_absolute_error: 0.576629, mean_q: 1.213605\n",
      " 39516/100000: episode: 444, duration: 1.024s, episode steps: 89, steps per second: 87, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004259, mean_absolute_error: 0.580113, mean_q: 1.221472\n",
      " 39605/100000: episode: 445, duration: 0.947s, episode steps: 89, steps per second: 94, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003786, mean_absolute_error: 0.573216, mean_q: 1.211152\n",
      " 39694/100000: episode: 446, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003915, mean_absolute_error: 0.571035, mean_q: 1.204216\n",
      " 39783/100000: episode: 447, duration: 0.983s, episode steps: 89, steps per second: 91, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004046, mean_absolute_error: 0.575576, mean_q: 1.213651\n",
      " 39872/100000: episode: 448, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004163, mean_absolute_error: 0.576532, mean_q: 1.211112\n",
      " 39961/100000: episode: 449, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004239, mean_absolute_error: 0.572941, mean_q: 1.199644\n",
      " 40050/100000: episode: 450, duration: 1.128s, episode steps: 89, steps per second: 79, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003887, mean_absolute_error: 0.566906, mean_q: 1.194529\n",
      " 40139/100000: episode: 451, duration: 1.235s, episode steps: 89, steps per second: 72, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003914, mean_absolute_error: 0.557660, mean_q: 1.179412\n",
      " 40228/100000: episode: 452, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004159, mean_absolute_error: 0.566459, mean_q: 1.188986\n",
      " 40317/100000: episode: 453, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003974, mean_absolute_error: 0.572679, mean_q: 1.208850\n",
      " 40406/100000: episode: 454, duration: 1.006s, episode steps: 89, steps per second: 88, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003979, mean_absolute_error: 0.564939, mean_q: 1.188272\n",
      " 40495/100000: episode: 455, duration: 0.968s, episode steps: 89, steps per second: 92, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004028, mean_absolute_error: 0.562044, mean_q: 1.178806\n",
      " 40584/100000: episode: 456, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003828, mean_absolute_error: 0.544804, mean_q: 1.152720\n",
      " 40673/100000: episode: 457, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003964, mean_absolute_error: 0.557249, mean_q: 1.172835\n",
      " 40762/100000: episode: 458, duration: 0.956s, episode steps: 89, steps per second: 93, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003951, mean_absolute_error: 0.560738, mean_q: 1.183533\n",
      " 40851/100000: episode: 459, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003686, mean_absolute_error: 0.559178, mean_q: 1.177730\n",
      " 40940/100000: episode: 460, duration: 1.018s, episode steps: 89, steps per second: 87, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003658, mean_absolute_error: 0.557871, mean_q: 1.174864\n",
      " 41029/100000: episode: 461, duration: 0.948s, episode steps: 89, steps per second: 94, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003826, mean_absolute_error: 0.565082, mean_q: 1.192079\n",
      " 41118/100000: episode: 462, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003720, mean_absolute_error: 0.566972, mean_q: 1.193757\n",
      " 41207/100000: episode: 463, duration: 0.946s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003807, mean_absolute_error: 0.556344, mean_q: 1.172247\n",
      " 41296/100000: episode: 464, duration: 0.944s, episode steps: 89, steps per second: 94, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003737, mean_absolute_error: 0.556627, mean_q: 1.172391\n",
      " 41385/100000: episode: 465, duration: 1.045s, episode steps: 89, steps per second: 85, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003797, mean_absolute_error: 0.546756, mean_q: 1.150665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 41474/100000: episode: 466, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003493, mean_absolute_error: 0.540219, mean_q: 1.145874\n",
      " 41563/100000: episode: 467, duration: 1.002s, episode steps: 89, steps per second: 89, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003784, mean_absolute_error: 0.553612, mean_q: 1.167924\n",
      " 41652/100000: episode: 468, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003634, mean_absolute_error: 0.555405, mean_q: 1.169742\n",
      " 41741/100000: episode: 469, duration: 0.890s, episode steps: 89, steps per second: 100, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003541, mean_absolute_error: 0.539374, mean_q: 1.137681\n",
      " 41830/100000: episode: 470, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003685, mean_absolute_error: 0.550653, mean_q: 1.167522\n",
      " 41919/100000: episode: 471, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003785, mean_absolute_error: 0.554141, mean_q: 1.173100\n",
      " 42008/100000: episode: 472, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003631, mean_absolute_error: 0.551099, mean_q: 1.163462\n",
      " 42097/100000: episode: 473, duration: 0.895s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003439, mean_absolute_error: 0.544378, mean_q: 1.153994\n",
      " 42186/100000: episode: 474, duration: 0.896s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003837, mean_absolute_error: 0.551053, mean_q: 1.159153\n",
      " 42275/100000: episode: 475, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003301, mean_absolute_error: 0.554175, mean_q: 1.171982\n",
      " 42364/100000: episode: 476, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003638, mean_absolute_error: 0.561121, mean_q: 1.185925\n",
      " 42453/100000: episode: 477, duration: 1.059s, episode steps: 89, steps per second: 84, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003644, mean_absolute_error: 0.556288, mean_q: 1.171596\n",
      " 42542/100000: episode: 478, duration: 1.111s, episode steps: 89, steps per second: 80, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003701, mean_absolute_error: 0.534307, mean_q: 1.130177\n",
      " 42631/100000: episode: 479, duration: 0.896s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003749, mean_absolute_error: 0.543727, mean_q: 1.140799\n",
      " 42720/100000: episode: 480, duration: 0.895s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.004008, mean_absolute_error: 0.546731, mean_q: 1.144448\n",
      " 42809/100000: episode: 481, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003642, mean_absolute_error: 0.539129, mean_q: 1.136598\n",
      " 42898/100000: episode: 482, duration: 1.019s, episode steps: 89, steps per second: 87, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003647, mean_absolute_error: 0.550306, mean_q: 1.163486\n",
      " 42987/100000: episode: 483, duration: 0.972s, episode steps: 89, steps per second: 92, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003646, mean_absolute_error: 0.561027, mean_q: 1.182348\n",
      " 43076/100000: episode: 484, duration: 1.139s, episode steps: 89, steps per second: 78, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003508, mean_absolute_error: 0.560090, mean_q: 1.188465\n",
      " 43165/100000: episode: 485, duration: 0.961s, episode steps: 89, steps per second: 93, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003628, mean_absolute_error: 0.560429, mean_q: 1.183281\n",
      " 43254/100000: episode: 486, duration: 1.075s, episode steps: 89, steps per second: 83, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003606, mean_absolute_error: 0.546985, mean_q: 1.154363\n",
      " 43343/100000: episode: 487, duration: 1.027s, episode steps: 89, steps per second: 87, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003625, mean_absolute_error: 0.539634, mean_q: 1.144997\n",
      " 43432/100000: episode: 488, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003864, mean_absolute_error: 0.534734, mean_q: 1.128887\n",
      " 43521/100000: episode: 489, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003867, mean_absolute_error: 0.535519, mean_q: 1.134194\n",
      " 43610/100000: episode: 490, duration: 1.014s, episode steps: 89, steps per second: 88, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003737, mean_absolute_error: 0.535992, mean_q: 1.133479\n",
      " 43699/100000: episode: 491, duration: 1.046s, episode steps: 89, steps per second: 85, episode reward: 4.143, mean reward: 0.047 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003691, mean_absolute_error: 0.539227, mean_q: 1.138823\n",
      " 43788/100000: episode: 492, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003436, mean_absolute_error: 0.548945, mean_q: 1.158546\n",
      " 43877/100000: episode: 493, duration: 1.015s, episode steps: 89, steps per second: 88, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003415, mean_absolute_error: 0.545236, mean_q: 1.153402\n",
      " 43966/100000: episode: 494, duration: 0.886s, episode steps: 89, steps per second: 100, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003656, mean_absolute_error: 0.542672, mean_q: 1.140929\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44055/100000: episode: 495, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003534, mean_absolute_error: 0.539679, mean_q: 1.140831\n",
      " 44144/100000: episode: 496, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003548, mean_absolute_error: 0.551953, mean_q: 1.158001\n",
      " 44233/100000: episode: 497, duration: 0.954s, episode steps: 89, steps per second: 93, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003367, mean_absolute_error: 0.532531, mean_q: 1.126585\n",
      " 44322/100000: episode: 498, duration: 0.956s, episode steps: 89, steps per second: 93, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003949, mean_absolute_error: 0.526877, mean_q: 1.107211\n",
      " 44411/100000: episode: 499, duration: 0.989s, episode steps: 89, steps per second: 90, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003368, mean_absolute_error: 0.512379, mean_q: 1.086711\n",
      " 44500/100000: episode: 500, duration: 1.020s, episode steps: 89, steps per second: 87, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003613, mean_absolute_error: 0.526373, mean_q: 1.105851\n",
      " 44589/100000: episode: 501, duration: 1.025s, episode steps: 89, steps per second: 87, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003374, mean_absolute_error: 0.524949, mean_q: 1.110989\n",
      " 44678/100000: episode: 502, duration: 1.031s, episode steps: 89, steps per second: 86, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003395, mean_absolute_error: 0.521407, mean_q: 1.100903\n",
      " 44767/100000: episode: 503, duration: 1.059s, episode steps: 89, steps per second: 84, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003415, mean_absolute_error: 0.532703, mean_q: 1.128005\n",
      " 44856/100000: episode: 504, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003717, mean_absolute_error: 0.549100, mean_q: 1.157338\n",
      " 44945/100000: episode: 505, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003219, mean_absolute_error: 0.537306, mean_q: 1.141683\n",
      " 45034/100000: episode: 506, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003472, mean_absolute_error: 0.534670, mean_q: 1.129218\n",
      " 45123/100000: episode: 507, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003671, mean_absolute_error: 0.526325, mean_q: 1.113047\n",
      " 45212/100000: episode: 508, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003427, mean_absolute_error: 0.512652, mean_q: 1.090904\n",
      " 45301/100000: episode: 509, duration: 1.014s, episode steps: 89, steps per second: 88, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003412, mean_absolute_error: 0.528407, mean_q: 1.114417\n",
      " 45390/100000: episode: 510, duration: 0.982s, episode steps: 89, steps per second: 91, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003501, mean_absolute_error: 0.524919, mean_q: 1.109636\n",
      " 45479/100000: episode: 511, duration: 1.069s, episode steps: 89, steps per second: 83, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003483, mean_absolute_error: 0.521074, mean_q: 1.101631\n",
      " 45568/100000: episode: 512, duration: 0.983s, episode steps: 89, steps per second: 91, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003707, mean_absolute_error: 0.540469, mean_q: 1.142654\n",
      " 45657/100000: episode: 513, duration: 0.982s, episode steps: 89, steps per second: 91, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003235, mean_absolute_error: 0.539676, mean_q: 1.143698\n",
      " 45746/100000: episode: 514, duration: 1.049s, episode steps: 89, steps per second: 85, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003307, mean_absolute_error: 0.538479, mean_q: 1.136375\n",
      " 45835/100000: episode: 515, duration: 1.167s, episode steps: 89, steps per second: 76, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003537, mean_absolute_error: 0.528041, mean_q: 1.117696\n",
      " 45924/100000: episode: 516, duration: 1.296s, episode steps: 89, steps per second: 69, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003519, mean_absolute_error: 0.522188, mean_q: 1.103303\n",
      " 46013/100000: episode: 517, duration: 0.976s, episode steps: 89, steps per second: 91, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003620, mean_absolute_error: 0.526273, mean_q: 1.112692\n",
      " 46102/100000: episode: 518, duration: 0.893s, episode steps: 89, steps per second: 100, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003237, mean_absolute_error: 0.515613, mean_q: 1.092707\n",
      " 46191/100000: episode: 519, duration: 0.991s, episode steps: 89, steps per second: 90, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003394, mean_absolute_error: 0.500999, mean_q: 1.061064\n",
      " 46280/100000: episode: 520, duration: 0.997s, episode steps: 89, steps per second: 89, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003316, mean_absolute_error: 0.508594, mean_q: 1.076149\n",
      " 46369/100000: episode: 521, duration: 0.887s, episode steps: 89, steps per second: 100, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003260, mean_absolute_error: 0.508222, mean_q: 1.080221\n",
      " 46458/100000: episode: 522, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003235, mean_absolute_error: 0.516175, mean_q: 1.093458\n",
      " 46547/100000: episode: 523, duration: 0.896s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002992, mean_absolute_error: 0.509756, mean_q: 1.086013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 46636/100000: episode: 524, duration: 0.977s, episode steps: 89, steps per second: 91, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003199, mean_absolute_error: 0.508932, mean_q: 1.083469\n",
      " 46725/100000: episode: 525, duration: 0.914s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002984, mean_absolute_error: 0.497269, mean_q: 1.062555\n",
      " 46814/100000: episode: 526, duration: 0.961s, episode steps: 89, steps per second: 93, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003306, mean_absolute_error: 0.506527, mean_q: 1.073090\n",
      " 46903/100000: episode: 527, duration: 0.946s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003569, mean_absolute_error: 0.504614, mean_q: 1.063563\n",
      " 46992/100000: episode: 528, duration: 0.963s, episode steps: 89, steps per second: 92, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003212, mean_absolute_error: 0.494261, mean_q: 1.048443\n",
      " 47081/100000: episode: 529, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003180, mean_absolute_error: 0.504426, mean_q: 1.068116\n",
      " 47170/100000: episode: 530, duration: 0.967s, episode steps: 89, steps per second: 92, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003090, mean_absolute_error: 0.515681, mean_q: 1.094717\n",
      " 47259/100000: episode: 531, duration: 1.015s, episode steps: 89, steps per second: 88, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003093, mean_absolute_error: 0.508798, mean_q: 1.081802\n",
      " 47348/100000: episode: 532, duration: 0.964s, episode steps: 89, steps per second: 92, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003261, mean_absolute_error: 0.499710, mean_q: 1.062821\n",
      " 47437/100000: episode: 533, duration: 0.979s, episode steps: 89, steps per second: 91, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003459, mean_absolute_error: 0.505455, mean_q: 1.075579\n",
      " 47526/100000: episode: 534, duration: 1.014s, episode steps: 89, steps per second: 88, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003341, mean_absolute_error: 0.498066, mean_q: 1.063936\n",
      " 47615/100000: episode: 535, duration: 0.907s, episode steps: 89, steps per second: 98, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002836, mean_absolute_error: 0.484241, mean_q: 1.037228\n",
      " 47704/100000: episode: 536, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003388, mean_absolute_error: 0.499672, mean_q: 1.062429\n",
      " 47793/100000: episode: 537, duration: 0.888s, episode steps: 89, steps per second: 100, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003300, mean_absolute_error: 0.496555, mean_q: 1.060325\n",
      " 47882/100000: episode: 538, duration: 0.946s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003240, mean_absolute_error: 0.500855, mean_q: 1.064145\n",
      " 47971/100000: episode: 539, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002956, mean_absolute_error: 0.504103, mean_q: 1.079528\n",
      " 48060/100000: episode: 540, duration: 0.944s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003054, mean_absolute_error: 0.520554, mean_q: 1.105334\n",
      " 48149/100000: episode: 541, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003000, mean_absolute_error: 0.499754, mean_q: 1.062870\n",
      " 48238/100000: episode: 542, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003129, mean_absolute_error: 0.490302, mean_q: 1.046799\n",
      " 48327/100000: episode: 543, duration: 1.088s, episode steps: 89, steps per second: 82, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003299, mean_absolute_error: 0.485228, mean_q: 1.037293\n",
      " 48416/100000: episode: 544, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003087, mean_absolute_error: 0.486920, mean_q: 1.042096\n",
      " 48505/100000: episode: 545, duration: 0.958s, episode steps: 89, steps per second: 93, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003446, mean_absolute_error: 0.489151, mean_q: 1.040582\n",
      " 48594/100000: episode: 546, duration: 1.068s, episode steps: 89, steps per second: 83, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003135, mean_absolute_error: 0.478702, mean_q: 1.028725\n",
      " 48683/100000: episode: 547, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002907, mean_absolute_error: 0.482822, mean_q: 1.035794\n",
      " 48772/100000: episode: 548, duration: 0.997s, episode steps: 89, steps per second: 89, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003154, mean_absolute_error: 0.486648, mean_q: 1.039780\n",
      " 48861/100000: episode: 549, duration: 0.947s, episode steps: 89, steps per second: 94, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003136, mean_absolute_error: 0.489441, mean_q: 1.042935\n",
      " 48950/100000: episode: 550, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003276, mean_absolute_error: 0.478768, mean_q: 1.020800\n",
      " 49039/100000: episode: 551, duration: 0.968s, episode steps: 89, steps per second: 92, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003209, mean_absolute_error: 0.481566, mean_q: 1.027408\n",
      " 49128/100000: episode: 552, duration: 1.181s, episode steps: 89, steps per second: 75, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003033, mean_absolute_error: 0.481494, mean_q: 1.034265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 49217/100000: episode: 553, duration: 0.979s, episode steps: 89, steps per second: 91, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002732, mean_absolute_error: 0.493159, mean_q: 1.056701\n",
      " 49306/100000: episode: 554, duration: 1.067s, episode steps: 89, steps per second: 83, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003070, mean_absolute_error: 0.496072, mean_q: 1.063661\n",
      " 49395/100000: episode: 555, duration: 0.969s, episode steps: 89, steps per second: 92, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002994, mean_absolute_error: 0.492721, mean_q: 1.050545\n",
      " 49484/100000: episode: 556, duration: 1.085s, episode steps: 89, steps per second: 82, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002730, mean_absolute_error: 0.494995, mean_q: 1.060891\n",
      " 49573/100000: episode: 557, duration: 1.019s, episode steps: 89, steps per second: 87, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002869, mean_absolute_error: 0.494477, mean_q: 1.059754\n",
      " 49662/100000: episode: 558, duration: 1.079s, episode steps: 89, steps per second: 82, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003157, mean_absolute_error: 0.494696, mean_q: 1.053472\n",
      " 49751/100000: episode: 559, duration: 0.966s, episode steps: 89, steps per second: 92, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003341, mean_absolute_error: 0.485273, mean_q: 1.040421\n",
      " 49840/100000: episode: 560, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002932, mean_absolute_error: 0.488580, mean_q: 1.043447\n",
      " 49929/100000: episode: 561, duration: 0.935s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002984, mean_absolute_error: 0.491753, mean_q: 1.049248\n",
      " 50018/100000: episode: 562, duration: 0.999s, episode steps: 89, steps per second: 89, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002888, mean_absolute_error: 0.493685, mean_q: 1.055035\n",
      " 50107/100000: episode: 563, duration: 0.986s, episode steps: 89, steps per second: 90, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002829, mean_absolute_error: 0.507870, mean_q: 1.084738\n",
      " 50196/100000: episode: 564, duration: 1.001s, episode steps: 89, steps per second: 89, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003032, mean_absolute_error: 0.514243, mean_q: 1.095882\n",
      " 50285/100000: episode: 565, duration: 1.100s, episode steps: 89, steps per second: 81, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002766, mean_absolute_error: 0.509940, mean_q: 1.093748\n",
      " 50374/100000: episode: 566, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003379, mean_absolute_error: 0.515143, mean_q: 1.094990\n",
      " 50463/100000: episode: 567, duration: 1.062s, episode steps: 89, steps per second: 84, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002890, mean_absolute_error: 0.504378, mean_q: 1.079760\n",
      " 50552/100000: episode: 568, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002779, mean_absolute_error: 0.501061, mean_q: 1.076244\n",
      " 50641/100000: episode: 569, duration: 1.066s, episode steps: 89, steps per second: 83, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002824, mean_absolute_error: 0.498214, mean_q: 1.065194\n",
      " 50730/100000: episode: 570, duration: 0.913s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002815, mean_absolute_error: 0.483117, mean_q: 1.042215\n",
      " 50819/100000: episode: 571, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003008, mean_absolute_error: 0.493188, mean_q: 1.054074\n",
      " 50908/100000: episode: 572, duration: 0.907s, episode steps: 89, steps per second: 98, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002959, mean_absolute_error: 0.486414, mean_q: 1.042179\n",
      " 50997/100000: episode: 573, duration: 0.881s, episode steps: 89, steps per second: 101, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002765, mean_absolute_error: 0.502600, mean_q: 1.078198\n",
      " 51086/100000: episode: 574, duration: 0.893s, episode steps: 89, steps per second: 100, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002827, mean_absolute_error: 0.490565, mean_q: 1.054910\n",
      " 51175/100000: episode: 575, duration: 0.985s, episode steps: 89, steps per second: 90, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002614, mean_absolute_error: 0.490789, mean_q: 1.056902\n",
      " 51264/100000: episode: 576, duration: 0.977s, episode steps: 89, steps per second: 91, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002630, mean_absolute_error: 0.488969, mean_q: 1.050272\n",
      " 51353/100000: episode: 577, duration: 1.003s, episode steps: 89, steps per second: 89, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002796, mean_absolute_error: 0.502118, mean_q: 1.071063\n",
      " 51442/100000: episode: 578, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002896, mean_absolute_error: 0.493599, mean_q: 1.059241\n",
      " 51531/100000: episode: 579, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002601, mean_absolute_error: 0.483603, mean_q: 1.049809\n",
      " 51620/100000: episode: 580, duration: 0.913s, episode steps: 89, steps per second: 98, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002957, mean_absolute_error: 0.486542, mean_q: 1.045392\n",
      " 51709/100000: episode: 581, duration: 0.960s, episode steps: 89, steps per second: 93, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002911, mean_absolute_error: 0.475392, mean_q: 1.023408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51798/100000: episode: 582, duration: 1.023s, episode steps: 89, steps per second: 87, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002860, mean_absolute_error: 0.481553, mean_q: 1.041060\n",
      " 51887/100000: episode: 583, duration: 1.193s, episode steps: 89, steps per second: 75, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002737, mean_absolute_error: 0.501917, mean_q: 1.082572\n",
      " 51976/100000: episode: 584, duration: 1.065s, episode steps: 89, steps per second: 84, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002980, mean_absolute_error: 0.499095, mean_q: 1.065280\n",
      " 52065/100000: episode: 585, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002790, mean_absolute_error: 0.486336, mean_q: 1.043177\n",
      " 52154/100000: episode: 586, duration: 0.992s, episode steps: 89, steps per second: 90, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002571, mean_absolute_error: 0.493196, mean_q: 1.060878\n",
      " 52243/100000: episode: 587, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.003198, mean_absolute_error: 0.481542, mean_q: 1.035041\n",
      " 52332/100000: episode: 588, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002673, mean_absolute_error: 0.493448, mean_q: 1.060466\n",
      " 52421/100000: episode: 589, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002836, mean_absolute_error: 0.487896, mean_q: 1.049669\n",
      " 52510/100000: episode: 590, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002716, mean_absolute_error: 0.495576, mean_q: 1.060126\n",
      " 52599/100000: episode: 591, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002972, mean_absolute_error: 0.511745, mean_q: 1.092074\n",
      " 52688/100000: episode: 592, duration: 0.998s, episode steps: 89, steps per second: 89, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.539 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002925, mean_absolute_error: 0.518195, mean_q: 1.103203\n",
      " 52777/100000: episode: 593, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002654, mean_absolute_error: 0.502655, mean_q: 1.078776\n",
      " 52866/100000: episode: 594, duration: 0.972s, episode steps: 89, steps per second: 92, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002641, mean_absolute_error: 0.490018, mean_q: 1.059930\n",
      " 52955/100000: episode: 595, duration: 0.979s, episode steps: 89, steps per second: 91, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002844, mean_absolute_error: 0.494198, mean_q: 1.057584\n",
      " 53044/100000: episode: 596, duration: 1.064s, episode steps: 89, steps per second: 84, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002447, mean_absolute_error: 0.488640, mean_q: 1.050390\n",
      " 53133/100000: episode: 597, duration: 1.166s, episode steps: 89, steps per second: 76, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002556, mean_absolute_error: 0.484879, mean_q: 1.042911\n",
      " 53222/100000: episode: 598, duration: 1.073s, episode steps: 89, steps per second: 83, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002780, mean_absolute_error: 0.500985, mean_q: 1.074389\n",
      " 53311/100000: episode: 599, duration: 0.932s, episode steps: 89, steps per second: 95, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002444, mean_absolute_error: 0.503036, mean_q: 1.081402\n",
      " 53400/100000: episode: 600, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002702, mean_absolute_error: 0.496931, mean_q: 1.070330\n",
      " 53489/100000: episode: 601, duration: 1.013s, episode steps: 89, steps per second: 88, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002462, mean_absolute_error: 0.499307, mean_q: 1.073614\n",
      " 53578/100000: episode: 602, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002635, mean_absolute_error: 0.499495, mean_q: 1.069582\n",
      " 53667/100000: episode: 603, duration: 1.007s, episode steps: 89, steps per second: 88, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002419, mean_absolute_error: 0.499545, mean_q: 1.077547\n",
      " 53756/100000: episode: 604, duration: 1.028s, episode steps: 89, steps per second: 87, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002327, mean_absolute_error: 0.496285, mean_q: 1.070980\n",
      " 53845/100000: episode: 605, duration: 0.922s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002357, mean_absolute_error: 0.515389, mean_q: 1.106655\n",
      " 53934/100000: episode: 606, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 4.714, mean reward: 0.053 [-0.143, 0.143], mean action: 0.562 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002486, mean_absolute_error: 0.527193, mean_q: 1.127221\n",
      " 54023/100000: episode: 607, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002604, mean_absolute_error: 0.515369, mean_q: 1.099179\n",
      " 54112/100000: episode: 608, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002564, mean_absolute_error: 0.505394, mean_q: 1.086168\n",
      " 54201/100000: episode: 609, duration: 0.917s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002355, mean_absolute_error: 0.490943, mean_q: 1.066248\n",
      " 54290/100000: episode: 610, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002543, mean_absolute_error: 0.480830, mean_q: 1.043923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 54379/100000: episode: 611, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002480, mean_absolute_error: 0.486337, mean_q: 1.052848\n",
      " 54468/100000: episode: 612, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002233, mean_absolute_error: 0.495515, mean_q: 1.069929\n",
      " 54557/100000: episode: 613, duration: 1.047s, episode steps: 89, steps per second: 85, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002388, mean_absolute_error: 0.480164, mean_q: 1.038104\n",
      " 54646/100000: episode: 614, duration: 0.917s, episode steps: 89, steps per second: 97, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.326 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002005, mean_absolute_error: 0.489124, mean_q: 1.060418\n",
      " 54735/100000: episode: 615, duration: 1.013s, episode steps: 89, steps per second: 88, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002327, mean_absolute_error: 0.500494, mean_q: 1.077428\n",
      " 54824/100000: episode: 616, duration: 0.957s, episode steps: 89, steps per second: 93, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002658, mean_absolute_error: 0.513136, mean_q: 1.099310\n",
      " 54913/100000: episode: 617, duration: 0.949s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002444, mean_absolute_error: 0.504827, mean_q: 1.086208\n",
      " 55002/100000: episode: 618, duration: 0.960s, episode steps: 89, steps per second: 93, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002439, mean_absolute_error: 0.496651, mean_q: 1.063725\n",
      " 55091/100000: episode: 619, duration: 0.917s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002488, mean_absolute_error: 0.492941, mean_q: 1.060336\n",
      " 55180/100000: episode: 620, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002439, mean_absolute_error: 0.488410, mean_q: 1.052157\n",
      " 55269/100000: episode: 621, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002466, mean_absolute_error: 0.499979, mean_q: 1.076784\n",
      " 55358/100000: episode: 622, duration: 0.984s, episode steps: 89, steps per second: 90, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002290, mean_absolute_error: 0.501528, mean_q: 1.080666\n",
      " 55447/100000: episode: 623, duration: 1.011s, episode steps: 89, steps per second: 88, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002283, mean_absolute_error: 0.499758, mean_q: 1.079267\n",
      " 55536/100000: episode: 624, duration: 0.881s, episode steps: 89, steps per second: 101, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002133, mean_absolute_error: 0.501025, mean_q: 1.083450\n",
      " 55625/100000: episode: 625, duration: 0.888s, episode steps: 89, steps per second: 100, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002305, mean_absolute_error: 0.494513, mean_q: 1.068736\n",
      " 55714/100000: episode: 626, duration: 0.891s, episode steps: 89, steps per second: 100, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002174, mean_absolute_error: 0.484882, mean_q: 1.050463\n",
      " 55803/100000: episode: 627, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.360 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002454, mean_absolute_error: 0.481307, mean_q: 1.038963\n",
      " 55892/100000: episode: 628, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002450, mean_absolute_error: 0.479299, mean_q: 1.038247\n",
      " 55981/100000: episode: 629, duration: 0.889s, episode steps: 89, steps per second: 100, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002377, mean_absolute_error: 0.485921, mean_q: 1.047100\n",
      " 56070/100000: episode: 630, duration: 0.889s, episode steps: 89, steps per second: 100, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002220, mean_absolute_error: 0.496555, mean_q: 1.071859\n",
      " 56159/100000: episode: 631, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002308, mean_absolute_error: 0.504241, mean_q: 1.088645\n",
      " 56248/100000: episode: 632, duration: 0.897s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002359, mean_absolute_error: 0.501662, mean_q: 1.081033\n",
      " 56337/100000: episode: 633, duration: 0.951s, episode steps: 89, steps per second: 94, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002250, mean_absolute_error: 0.497458, mean_q: 1.069148\n",
      " 56426/100000: episode: 634, duration: 1.029s, episode steps: 89, steps per second: 86, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002286, mean_absolute_error: 0.496524, mean_q: 1.068091\n",
      " 56515/100000: episode: 635, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002131, mean_absolute_error: 0.491816, mean_q: 1.057211\n",
      " 56604/100000: episode: 636, duration: 1.001s, episode steps: 89, steps per second: 89, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002389, mean_absolute_error: 0.491451, mean_q: 1.058100\n",
      " 56693/100000: episode: 637, duration: 1.203s, episode steps: 89, steps per second: 74, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002115, mean_absolute_error: 0.489951, mean_q: 1.058350\n",
      " 56782/100000: episode: 638, duration: 1.126s, episode steps: 89, steps per second: 79, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002221, mean_absolute_error: 0.484556, mean_q: 1.049229\n",
      " 56871/100000: episode: 639, duration: 1.234s, episode steps: 89, steps per second: 72, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002222, mean_absolute_error: 0.484089, mean_q: 1.050686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 56960/100000: episode: 640, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002196, mean_absolute_error: 0.479891, mean_q: 1.033914\n",
      " 57049/100000: episode: 641, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001923, mean_absolute_error: 0.476285, mean_q: 1.036726\n",
      " 57138/100000: episode: 642, duration: 1.004s, episode steps: 89, steps per second: 89, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002286, mean_absolute_error: 0.472859, mean_q: 1.024369\n",
      " 57227/100000: episode: 643, duration: 0.976s, episode steps: 89, steps per second: 91, episode reward: 5.000, mean reward: 0.056 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002059, mean_absolute_error: 0.474830, mean_q: 1.026343\n",
      " 57316/100000: episode: 644, duration: 0.983s, episode steps: 89, steps per second: 91, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002124, mean_absolute_error: 0.467750, mean_q: 1.013390\n",
      " 57405/100000: episode: 645, duration: 1.062s, episode steps: 89, steps per second: 84, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002212, mean_absolute_error: 0.467443, mean_q: 1.012384\n",
      " 57494/100000: episode: 646, duration: 1.022s, episode steps: 89, steps per second: 87, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002292, mean_absolute_error: 0.459396, mean_q: 0.992869\n",
      " 57583/100000: episode: 647, duration: 0.964s, episode steps: 89, steps per second: 92, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002054, mean_absolute_error: 0.475508, mean_q: 1.028483\n",
      " 57672/100000: episode: 648, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002060, mean_absolute_error: 0.485060, mean_q: 1.047329\n",
      " 57761/100000: episode: 649, duration: 0.973s, episode steps: 89, steps per second: 91, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002082, mean_absolute_error: 0.481504, mean_q: 1.035725\n",
      " 57850/100000: episode: 650, duration: 1.003s, episode steps: 89, steps per second: 89, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002301, mean_absolute_error: 0.474348, mean_q: 1.031094\n",
      " 57939/100000: episode: 651, duration: 1.027s, episode steps: 89, steps per second: 87, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002143, mean_absolute_error: 0.475103, mean_q: 1.026900\n",
      " 58028/100000: episode: 652, duration: 0.965s, episode steps: 89, steps per second: 92, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001981, mean_absolute_error: 0.474725, mean_q: 1.032606\n",
      " 58117/100000: episode: 653, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002321, mean_absolute_error: 0.473918, mean_q: 1.023864\n",
      " 58206/100000: episode: 654, duration: 1.058s, episode steps: 89, steps per second: 84, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002019, mean_absolute_error: 0.475979, mean_q: 1.033380\n",
      " 58295/100000: episode: 655, duration: 0.972s, episode steps: 89, steps per second: 92, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002159, mean_absolute_error: 0.471016, mean_q: 1.026432\n",
      " 58384/100000: episode: 656, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002058, mean_absolute_error: 0.460183, mean_q: 1.000011\n",
      " 58473/100000: episode: 657, duration: 0.975s, episode steps: 89, steps per second: 91, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001917, mean_absolute_error: 0.470942, mean_q: 1.025547\n",
      " 58562/100000: episode: 658, duration: 1.003s, episode steps: 89, steps per second: 89, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002009, mean_absolute_error: 0.468430, mean_q: 1.021807\n",
      " 58651/100000: episode: 659, duration: 0.965s, episode steps: 89, steps per second: 92, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002030, mean_absolute_error: 0.463356, mean_q: 1.006270\n",
      " 58740/100000: episode: 660, duration: 1.053s, episode steps: 89, steps per second: 85, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002074, mean_absolute_error: 0.473437, mean_q: 1.028421\n",
      " 58829/100000: episode: 661, duration: 1.010s, episode steps: 89, steps per second: 88, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002398, mean_absolute_error: 0.484239, mean_q: 1.047231\n",
      " 58918/100000: episode: 662, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002097, mean_absolute_error: 0.472216, mean_q: 1.025794\n",
      " 59007/100000: episode: 663, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002062, mean_absolute_error: 0.486791, mean_q: 1.061791\n",
      " 59096/100000: episode: 664, duration: 1.020s, episode steps: 89, steps per second: 87, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001859, mean_absolute_error: 0.483916, mean_q: 1.052526\n",
      " 59185/100000: episode: 665, duration: 0.883s, episode steps: 89, steps per second: 101, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001903, mean_absolute_error: 0.481067, mean_q: 1.044198\n",
      " 59274/100000: episode: 666, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002096, mean_absolute_error: 0.472432, mean_q: 1.026509\n",
      " 59363/100000: episode: 667, duration: 1.077s, episode steps: 89, steps per second: 83, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001956, mean_absolute_error: 0.473274, mean_q: 1.024759\n",
      " 59452/100000: episode: 668, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001945, mean_absolute_error: 0.476981, mean_q: 1.032896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 59541/100000: episode: 669, duration: 1.021s, episode steps: 89, steps per second: 87, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002017, mean_absolute_error: 0.474390, mean_q: 1.024616\n",
      " 59630/100000: episode: 670, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002087, mean_absolute_error: 0.465162, mean_q: 1.012110\n",
      " 59719/100000: episode: 671, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002047, mean_absolute_error: 0.465749, mean_q: 1.009596\n",
      " 59808/100000: episode: 672, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001898, mean_absolute_error: 0.470604, mean_q: 1.028594\n",
      " 59897/100000: episode: 673, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002008, mean_absolute_error: 0.473123, mean_q: 1.030875\n",
      " 59986/100000: episode: 674, duration: 0.935s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001874, mean_absolute_error: 0.476368, mean_q: 1.035044\n",
      " 60075/100000: episode: 675, duration: 0.957s, episode steps: 89, steps per second: 93, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002091, mean_absolute_error: 0.464973, mean_q: 1.008771\n",
      " 60164/100000: episode: 676, duration: 0.962s, episode steps: 89, steps per second: 92, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001999, mean_absolute_error: 0.462345, mean_q: 1.006504\n",
      " 60253/100000: episode: 677, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001945, mean_absolute_error: 0.456643, mean_q: 0.998636\n",
      " 60342/100000: episode: 678, duration: 0.949s, episode steps: 89, steps per second: 94, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001934, mean_absolute_error: 0.467943, mean_q: 1.015785\n",
      " 60431/100000: episode: 679, duration: 1.024s, episode steps: 89, steps per second: 87, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002132, mean_absolute_error: 0.478195, mean_q: 1.032541\n",
      " 60520/100000: episode: 680, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001851, mean_absolute_error: 0.470057, mean_q: 1.024421\n",
      " 60609/100000: episode: 681, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001973, mean_absolute_error: 0.468189, mean_q: 1.018251\n",
      " 60698/100000: episode: 682, duration: 0.898s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002162, mean_absolute_error: 0.474608, mean_q: 1.030494\n",
      " 60787/100000: episode: 683, duration: 0.914s, episode steps: 89, steps per second: 97, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001712, mean_absolute_error: 0.460415, mean_q: 1.007389\n",
      " 60876/100000: episode: 684, duration: 1.194s, episode steps: 89, steps per second: 75, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001963, mean_absolute_error: 0.478570, mean_q: 1.038179\n",
      " 60965/100000: episode: 685, duration: 0.968s, episode steps: 89, steps per second: 92, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001822, mean_absolute_error: 0.463954, mean_q: 1.012273\n",
      " 61054/100000: episode: 686, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.360 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001969, mean_absolute_error: 0.472845, mean_q: 1.024827\n",
      " 61143/100000: episode: 687, duration: 1.035s, episode steps: 89, steps per second: 86, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001956, mean_absolute_error: 0.469522, mean_q: 1.020198\n",
      " 61232/100000: episode: 688, duration: 0.974s, episode steps: 89, steps per second: 91, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001918, mean_absolute_error: 0.476960, mean_q: 1.033992\n",
      " 61321/100000: episode: 689, duration: 1.147s, episode steps: 89, steps per second: 78, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001877, mean_absolute_error: 0.471446, mean_q: 1.020714\n",
      " 61410/100000: episode: 690, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001926, mean_absolute_error: 0.468754, mean_q: 1.016364\n",
      " 61499/100000: episode: 691, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001873, mean_absolute_error: 0.464833, mean_q: 1.013597\n",
      " 61588/100000: episode: 692, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001944, mean_absolute_error: 0.455725, mean_q: 0.992679\n",
      " 61677/100000: episode: 693, duration: 0.955s, episode steps: 89, steps per second: 93, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001776, mean_absolute_error: 0.472799, mean_q: 1.033367\n",
      " 61766/100000: episode: 694, duration: 0.979s, episode steps: 89, steps per second: 91, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001938, mean_absolute_error: 0.489570, mean_q: 1.067260\n",
      " 61855/100000: episode: 695, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001670, mean_absolute_error: 0.475719, mean_q: 1.035300\n",
      " 61944/100000: episode: 696, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001720, mean_absolute_error: 0.486189, mean_q: 1.058074\n",
      " 62033/100000: episode: 697, duration: 1.029s, episode steps: 89, steps per second: 86, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001927, mean_absolute_error: 0.486477, mean_q: 1.057495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62122/100000: episode: 698, duration: 0.922s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002069, mean_absolute_error: 0.481101, mean_q: 1.043935\n",
      " 62211/100000: episode: 699, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001823, mean_absolute_error: 0.472337, mean_q: 1.030709\n",
      " 62300/100000: episode: 700, duration: 0.900s, episode steps: 89, steps per second: 99, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001843, mean_absolute_error: 0.472326, mean_q: 1.029798\n",
      " 62389/100000: episode: 701, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001835, mean_absolute_error: 0.479255, mean_q: 1.042204\n",
      " 62478/100000: episode: 702, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002018, mean_absolute_error: 0.478524, mean_q: 1.039340\n",
      " 62567/100000: episode: 703, duration: 0.913s, episode steps: 89, steps per second: 98, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001930, mean_absolute_error: 0.479873, mean_q: 1.040561\n",
      " 62656/100000: episode: 704, duration: 0.907s, episode steps: 89, steps per second: 98, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001765, mean_absolute_error: 0.463140, mean_q: 1.015559\n",
      " 62745/100000: episode: 705, duration: 0.887s, episode steps: 89, steps per second: 100, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001921, mean_absolute_error: 0.468252, mean_q: 1.018236\n",
      " 62834/100000: episode: 706, duration: 0.889s, episode steps: 89, steps per second: 100, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001990, mean_absolute_error: 0.464280, mean_q: 1.010885\n",
      " 62923/100000: episode: 707, duration: 0.922s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.002005, mean_absolute_error: 0.480242, mean_q: 1.037945\n",
      " 63012/100000: episode: 708, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001633, mean_absolute_error: 0.473217, mean_q: 1.029190\n",
      " 63101/100000: episode: 709, duration: 0.903s, episode steps: 89, steps per second: 99, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001858, mean_absolute_error: 0.482352, mean_q: 1.045852\n",
      " 63190/100000: episode: 710, duration: 0.880s, episode steps: 89, steps per second: 101, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001593, mean_absolute_error: 0.463652, mean_q: 1.012700\n",
      " 63279/100000: episode: 711, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001814, mean_absolute_error: 0.477100, mean_q: 1.036246\n",
      " 63368/100000: episode: 712, duration: 0.896s, episode steps: 89, steps per second: 99, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001823, mean_absolute_error: 0.484300, mean_q: 1.050264\n",
      " 63457/100000: episode: 713, duration: 0.894s, episode steps: 89, steps per second: 100, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001865, mean_absolute_error: 0.477191, mean_q: 1.037955\n",
      " 63546/100000: episode: 714, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001807, mean_absolute_error: 0.487972, mean_q: 1.059684\n",
      " 63635/100000: episode: 715, duration: 0.913s, episode steps: 89, steps per second: 97, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001908, mean_absolute_error: 0.487473, mean_q: 1.055384\n",
      " 63724/100000: episode: 716, duration: 0.899s, episode steps: 89, steps per second: 99, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001829, mean_absolute_error: 0.482803, mean_q: 1.046206\n",
      " 63813/100000: episode: 717, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001835, mean_absolute_error: 0.481488, mean_q: 1.049103\n",
      " 63902/100000: episode: 718, duration: 0.917s, episode steps: 89, steps per second: 97, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001679, mean_absolute_error: 0.481237, mean_q: 1.049179\n",
      " 63991/100000: episode: 719, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001616, mean_absolute_error: 0.489846, mean_q: 1.069317\n",
      " 64080/100000: episode: 720, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001711, mean_absolute_error: 0.487336, mean_q: 1.063743\n",
      " 64169/100000: episode: 721, duration: 1.031s, episode steps: 89, steps per second: 86, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001600, mean_absolute_error: 0.488789, mean_q: 1.066273\n",
      " 64258/100000: episode: 722, duration: 0.907s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001739, mean_absolute_error: 0.492605, mean_q: 1.067646\n",
      " 64347/100000: episode: 723, duration: 0.892s, episode steps: 89, steps per second: 100, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001934, mean_absolute_error: 0.490716, mean_q: 1.066309\n",
      " 64436/100000: episode: 724, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001730, mean_absolute_error: 0.469763, mean_q: 1.025553\n",
      " 64525/100000: episode: 725, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001788, mean_absolute_error: 0.492905, mean_q: 1.066906\n",
      " 64614/100000: episode: 726, duration: 0.984s, episode steps: 89, steps per second: 90, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001633, mean_absolute_error: 0.477334, mean_q: 1.042638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 64703/100000: episode: 727, duration: 1.033s, episode steps: 89, steps per second: 86, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001779, mean_absolute_error: 0.481900, mean_q: 1.050180\n",
      " 64792/100000: episode: 728, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001748, mean_absolute_error: 0.485107, mean_q: 1.054462\n",
      " 64881/100000: episode: 729, duration: 0.922s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001748, mean_absolute_error: 0.482190, mean_q: 1.054706\n",
      " 64970/100000: episode: 730, duration: 0.986s, episode steps: 89, steps per second: 90, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001647, mean_absolute_error: 0.483024, mean_q: 1.054227\n",
      " 65059/100000: episode: 731, duration: 0.953s, episode steps: 89, steps per second: 93, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001645, mean_absolute_error: 0.491292, mean_q: 1.071626\n",
      " 65148/100000: episode: 732, duration: 0.947s, episode steps: 89, steps per second: 94, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001621, mean_absolute_error: 0.494597, mean_q: 1.073005\n",
      " 65237/100000: episode: 733, duration: 1.009s, episode steps: 89, steps per second: 88, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001858, mean_absolute_error: 0.490830, mean_q: 1.063585\n",
      " 65326/100000: episode: 734, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001706, mean_absolute_error: 0.483075, mean_q: 1.053575\n",
      " 65415/100000: episode: 735, duration: 0.901s, episode steps: 89, steps per second: 99, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001716, mean_absolute_error: 0.477595, mean_q: 1.040826\n",
      " 65504/100000: episode: 736, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001719, mean_absolute_error: 0.471074, mean_q: 1.023568\n",
      " 65593/100000: episode: 737, duration: 1.021s, episode steps: 89, steps per second: 87, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001736, mean_absolute_error: 0.476688, mean_q: 1.038964\n",
      " 65682/100000: episode: 738, duration: 0.952s, episode steps: 89, steps per second: 93, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001821, mean_absolute_error: 0.477778, mean_q: 1.043266\n",
      " 65771/100000: episode: 739, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001662, mean_absolute_error: 0.471061, mean_q: 1.032533\n",
      " 65860/100000: episode: 740, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001535, mean_absolute_error: 0.472395, mean_q: 1.033630\n",
      " 65949/100000: episode: 741, duration: 0.956s, episode steps: 89, steps per second: 93, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001579, mean_absolute_error: 0.477224, mean_q: 1.045509\n",
      " 66038/100000: episode: 742, duration: 0.960s, episode steps: 89, steps per second: 93, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001680, mean_absolute_error: 0.488364, mean_q: 1.061253\n",
      " 66127/100000: episode: 743, duration: 0.985s, episode steps: 89, steps per second: 90, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001617, mean_absolute_error: 0.488521, mean_q: 1.061152\n",
      " 66216/100000: episode: 744, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001680, mean_absolute_error: 0.496350, mean_q: 1.080627\n",
      " 66305/100000: episode: 745, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001654, mean_absolute_error: 0.493918, mean_q: 1.074897\n",
      " 66394/100000: episode: 746, duration: 1.034s, episode steps: 89, steps per second: 86, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001743, mean_absolute_error: 0.494392, mean_q: 1.075062\n",
      " 66483/100000: episode: 747, duration: 1.008s, episode steps: 89, steps per second: 88, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001789, mean_absolute_error: 0.486360, mean_q: 1.060099\n",
      " 66572/100000: episode: 748, duration: 0.932s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001493, mean_absolute_error: 0.483195, mean_q: 1.058301\n",
      " 66661/100000: episode: 749, duration: 1.189s, episode steps: 89, steps per second: 75, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001678, mean_absolute_error: 0.463932, mean_q: 1.017829\n",
      " 66750/100000: episode: 750, duration: 1.033s, episode steps: 89, steps per second: 86, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001735, mean_absolute_error: 0.478314, mean_q: 1.044210\n",
      " 66839/100000: episode: 751, duration: 0.984s, episode steps: 89, steps per second: 90, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001632, mean_absolute_error: 0.483585, mean_q: 1.058571\n",
      " 66928/100000: episode: 752, duration: 1.083s, episode steps: 89, steps per second: 82, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001621, mean_absolute_error: 0.481252, mean_q: 1.049811\n",
      " 67017/100000: episode: 753, duration: 1.117s, episode steps: 89, steps per second: 80, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001629, mean_absolute_error: 0.483609, mean_q: 1.057071\n",
      " 67106/100000: episode: 754, duration: 1.090s, episode steps: 89, steps per second: 82, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001673, mean_absolute_error: 0.490757, mean_q: 1.069766\n",
      " 67195/100000: episode: 755, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001473, mean_absolute_error: 0.489744, mean_q: 1.070482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 67284/100000: episode: 756, duration: 1.071s, episode steps: 89, steps per second: 83, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001550, mean_absolute_error: 0.485531, mean_q: 1.058882\n",
      " 67373/100000: episode: 757, duration: 1.080s, episode steps: 89, steps per second: 82, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001643, mean_absolute_error: 0.480692, mean_q: 1.050754\n",
      " 67462/100000: episode: 758, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001654, mean_absolute_error: 0.485521, mean_q: 1.060747\n",
      " 67551/100000: episode: 759, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001565, mean_absolute_error: 0.485496, mean_q: 1.065832\n",
      " 67640/100000: episode: 760, duration: 0.965s, episode steps: 89, steps per second: 92, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001582, mean_absolute_error: 0.478778, mean_q: 1.046826\n",
      " 67729/100000: episode: 761, duration: 0.915s, episode steps: 89, steps per second: 97, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001626, mean_absolute_error: 0.485001, mean_q: 1.059435\n",
      " 67818/100000: episode: 762, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001700, mean_absolute_error: 0.490844, mean_q: 1.067580\n",
      " 67907/100000: episode: 763, duration: 1.035s, episode steps: 89, steps per second: 86, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001569, mean_absolute_error: 0.481290, mean_q: 1.052995\n",
      " 67996/100000: episode: 764, duration: 1.078s, episode steps: 89, steps per second: 83, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001621, mean_absolute_error: 0.496046, mean_q: 1.079871\n",
      " 68085/100000: episode: 765, duration: 0.968s, episode steps: 89, steps per second: 92, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001942, mean_absolute_error: 0.511593, mean_q: 1.107842\n",
      " 68174/100000: episode: 766, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001599, mean_absolute_error: 0.498434, mean_q: 1.090296\n",
      " 68263/100000: episode: 767, duration: 0.935s, episode steps: 89, steps per second: 95, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001541, mean_absolute_error: 0.494048, mean_q: 1.080700\n",
      " 68352/100000: episode: 768, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001605, mean_absolute_error: 0.508509, mean_q: 1.105710\n",
      " 68441/100000: episode: 769, duration: 0.999s, episode steps: 89, steps per second: 89, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001608, mean_absolute_error: 0.498804, mean_q: 1.084531\n",
      " 68530/100000: episode: 770, duration: 0.963s, episode steps: 89, steps per second: 92, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001493, mean_absolute_error: 0.504711, mean_q: 1.099916\n",
      " 68619/100000: episode: 771, duration: 0.972s, episode steps: 89, steps per second: 92, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001518, mean_absolute_error: 0.497657, mean_q: 1.088344\n",
      " 68708/100000: episode: 772, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001601, mean_absolute_error: 0.485101, mean_q: 1.054186\n",
      " 68797/100000: episode: 773, duration: 1.018s, episode steps: 89, steps per second: 87, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001514, mean_absolute_error: 0.486462, mean_q: 1.059364\n",
      " 68886/100000: episode: 774, duration: 1.104s, episode steps: 89, steps per second: 81, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001554, mean_absolute_error: 0.487613, mean_q: 1.068133\n",
      " 68975/100000: episode: 775, duration: 1.073s, episode steps: 89, steps per second: 83, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001578, mean_absolute_error: 0.488260, mean_q: 1.069847\n",
      " 69064/100000: episode: 776, duration: 1.140s, episode steps: 89, steps per second: 78, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001681, mean_absolute_error: 0.486934, mean_q: 1.061019\n",
      " 69153/100000: episode: 777, duration: 1.138s, episode steps: 89, steps per second: 78, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001580, mean_absolute_error: 0.481230, mean_q: 1.056252\n",
      " 69242/100000: episode: 778, duration: 1.174s, episode steps: 89, steps per second: 76, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001596, mean_absolute_error: 0.498395, mean_q: 1.091134\n",
      " 69331/100000: episode: 779, duration: 1.196s, episode steps: 89, steps per second: 74, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001572, mean_absolute_error: 0.500052, mean_q: 1.092939\n",
      " 69420/100000: episode: 780, duration: 1.127s, episode steps: 89, steps per second: 79, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001493, mean_absolute_error: 0.490627, mean_q: 1.071654\n",
      " 69509/100000: episode: 781, duration: 1.055s, episode steps: 89, steps per second: 84, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001535, mean_absolute_error: 0.493042, mean_q: 1.082282\n",
      " 69598/100000: episode: 782, duration: 1.002s, episode steps: 89, steps per second: 89, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001529, mean_absolute_error: 0.506034, mean_q: 1.098642\n",
      " 69687/100000: episode: 783, duration: 0.991s, episode steps: 89, steps per second: 90, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001522, mean_absolute_error: 0.492869, mean_q: 1.080560\n",
      " 69776/100000: episode: 784, duration: 0.987s, episode steps: 89, steps per second: 90, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001662, mean_absolute_error: 0.493636, mean_q: 1.075398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 69865/100000: episode: 785, duration: 0.984s, episode steps: 89, steps per second: 90, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001613, mean_absolute_error: 0.501988, mean_q: 1.097051\n",
      " 69954/100000: episode: 786, duration: 1.027s, episode steps: 89, steps per second: 87, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001414, mean_absolute_error: 0.488021, mean_q: 1.069600\n",
      " 70043/100000: episode: 787, duration: 1.019s, episode steps: 89, steps per second: 87, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001395, mean_absolute_error: 0.485175, mean_q: 1.065081\n",
      " 70132/100000: episode: 788, duration: 1.013s, episode steps: 89, steps per second: 88, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001429, mean_absolute_error: 0.491041, mean_q: 1.076416\n",
      " 70221/100000: episode: 789, duration: 1.063s, episode steps: 89, steps per second: 84, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001403, mean_absolute_error: 0.499932, mean_q: 1.090211\n",
      " 70310/100000: episode: 790, duration: 1.043s, episode steps: 89, steps per second: 85, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001534, mean_absolute_error: 0.513225, mean_q: 1.118703\n",
      " 70399/100000: episode: 791, duration: 1.037s, episode steps: 89, steps per second: 86, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001490, mean_absolute_error: 0.504970, mean_q: 1.096553\n",
      " 70488/100000: episode: 792, duration: 1.030s, episode steps: 89, steps per second: 86, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001470, mean_absolute_error: 0.500953, mean_q: 1.094258\n",
      " 70577/100000: episode: 793, duration: 0.976s, episode steps: 89, steps per second: 91, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001355, mean_absolute_error: 0.501044, mean_q: 1.095050\n",
      " 70666/100000: episode: 794, duration: 0.922s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001579, mean_absolute_error: 0.515099, mean_q: 1.117615\n",
      " 70755/100000: episode: 795, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001639, mean_absolute_error: 0.519987, mean_q: 1.126687\n",
      " 70844/100000: episode: 796, duration: 1.008s, episode steps: 89, steps per second: 88, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001551, mean_absolute_error: 0.525613, mean_q: 1.141577\n",
      " 70933/100000: episode: 797, duration: 1.143s, episode steps: 89, steps per second: 78, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001558, mean_absolute_error: 0.524183, mean_q: 1.136890\n",
      " 71022/100000: episode: 798, duration: 1.136s, episode steps: 89, steps per second: 78, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001500, mean_absolute_error: 0.518957, mean_q: 1.131891\n",
      " 71111/100000: episode: 799, duration: 1.001s, episode steps: 89, steps per second: 89, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001410, mean_absolute_error: 0.520120, mean_q: 1.132636\n",
      " 71200/100000: episode: 800, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001530, mean_absolute_error: 0.507434, mean_q: 1.103151\n",
      " 71289/100000: episode: 801, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001396, mean_absolute_error: 0.513629, mean_q: 1.125291\n",
      " 71378/100000: episode: 802, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001486, mean_absolute_error: 0.516177, mean_q: 1.123323\n",
      " 71467/100000: episode: 803, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001450, mean_absolute_error: 0.517559, mean_q: 1.130644\n",
      " 71556/100000: episode: 804, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001269, mean_absolute_error: 0.522531, mean_q: 1.141359\n",
      " 71645/100000: episode: 805, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001371, mean_absolute_error: 0.514985, mean_q: 1.127442\n",
      " 71734/100000: episode: 806, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001465, mean_absolute_error: 0.518345, mean_q: 1.130392\n",
      " 71823/100000: episode: 807, duration: 0.947s, episode steps: 89, steps per second: 94, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001502, mean_absolute_error: 0.511026, mean_q: 1.114079\n",
      " 71912/100000: episode: 808, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001541, mean_absolute_error: 0.511613, mean_q: 1.112528\n",
      " 72001/100000: episode: 809, duration: 0.999s, episode steps: 89, steps per second: 89, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001578, mean_absolute_error: 0.523054, mean_q: 1.135619\n",
      " 72090/100000: episode: 810, duration: 1.024s, episode steps: 89, steps per second: 87, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001482, mean_absolute_error: 0.520908, mean_q: 1.136025\n",
      " 72179/100000: episode: 811, duration: 0.981s, episode steps: 89, steps per second: 91, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001427, mean_absolute_error: 0.525061, mean_q: 1.142753\n",
      " 72268/100000: episode: 812, duration: 1.003s, episode steps: 89, steps per second: 89, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001545, mean_absolute_error: 0.536469, mean_q: 1.159595\n",
      " 72357/100000: episode: 813, duration: 0.975s, episode steps: 89, steps per second: 91, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001481, mean_absolute_error: 0.521345, mean_q: 1.133679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 72446/100000: episode: 814, duration: 0.944s, episode steps: 89, steps per second: 94, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001345, mean_absolute_error: 0.520527, mean_q: 1.137459\n",
      " 72535/100000: episode: 815, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001331, mean_absolute_error: 0.527465, mean_q: 1.152374\n",
      " 72624/100000: episode: 816, duration: 0.961s, episode steps: 89, steps per second: 93, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001426, mean_absolute_error: 0.526349, mean_q: 1.149611\n",
      " 72713/100000: episode: 817, duration: 0.953s, episode steps: 89, steps per second: 93, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001294, mean_absolute_error: 0.516859, mean_q: 1.130806\n",
      " 72802/100000: episode: 818, duration: 0.997s, episode steps: 89, steps per second: 89, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001385, mean_absolute_error: 0.511531, mean_q: 1.114172\n",
      " 72891/100000: episode: 819, duration: 1.000s, episode steps: 89, steps per second: 89, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001372, mean_absolute_error: 0.508282, mean_q: 1.111191\n",
      " 72980/100000: episode: 820, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001467, mean_absolute_error: 0.504445, mean_q: 1.104686\n",
      " 73069/100000: episode: 821, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001480, mean_absolute_error: 0.508316, mean_q: 1.105720\n",
      " 73158/100000: episode: 822, duration: 0.964s, episode steps: 89, steps per second: 92, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001672, mean_absolute_error: 0.501469, mean_q: 1.093238\n",
      " 73247/100000: episode: 823, duration: 0.971s, episode steps: 89, steps per second: 92, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001298, mean_absolute_error: 0.508296, mean_q: 1.114134\n",
      " 73336/100000: episode: 824, duration: 1.142s, episode steps: 89, steps per second: 78, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001405, mean_absolute_error: 0.496346, mean_q: 1.086671\n",
      " 73425/100000: episode: 825, duration: 0.985s, episode steps: 89, steps per second: 90, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001337, mean_absolute_error: 0.515059, mean_q: 1.126229\n",
      " 73514/100000: episode: 826, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001507, mean_absolute_error: 0.514835, mean_q: 1.121413\n",
      " 73603/100000: episode: 827, duration: 1.053s, episode steps: 89, steps per second: 85, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001322, mean_absolute_error: 0.497279, mean_q: 1.096926\n",
      " 73692/100000: episode: 828, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001511, mean_absolute_error: 0.492927, mean_q: 1.086219\n",
      " 73781/100000: episode: 829, duration: 0.970s, episode steps: 89, steps per second: 92, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001401, mean_absolute_error: 0.503797, mean_q: 1.107732\n",
      " 73870/100000: episode: 830, duration: 1.100s, episode steps: 89, steps per second: 81, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001388, mean_absolute_error: 0.522295, mean_q: 1.143568\n",
      " 73959/100000: episode: 831, duration: 1.211s, episode steps: 89, steps per second: 74, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001320, mean_absolute_error: 0.531331, mean_q: 1.158460\n",
      " 74048/100000: episode: 832, duration: 1.011s, episode steps: 89, steps per second: 88, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.360 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001679, mean_absolute_error: 0.519885, mean_q: 1.133301\n",
      " 74137/100000: episode: 833, duration: 0.999s, episode steps: 89, steps per second: 89, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001488, mean_absolute_error: 0.509676, mean_q: 1.112496\n",
      " 74226/100000: episode: 834, duration: 0.952s, episode steps: 89, steps per second: 94, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001352, mean_absolute_error: 0.506309, mean_q: 1.109448\n",
      " 74315/100000: episode: 835, duration: 1.105s, episode steps: 89, steps per second: 81, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001255, mean_absolute_error: 0.514645, mean_q: 1.128517\n",
      " 74404/100000: episode: 836, duration: 0.946s, episode steps: 89, steps per second: 94, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001441, mean_absolute_error: 0.515727, mean_q: 1.127061\n",
      " 74493/100000: episode: 837, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001501, mean_absolute_error: 0.522951, mean_q: 1.136655\n",
      " 74582/100000: episode: 838, duration: 1.083s, episode steps: 89, steps per second: 82, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001385, mean_absolute_error: 0.516901, mean_q: 1.132346\n",
      " 74671/100000: episode: 839, duration: 1.159s, episode steps: 89, steps per second: 77, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001392, mean_absolute_error: 0.521366, mean_q: 1.140305\n",
      " 74760/100000: episode: 840, duration: 0.954s, episode steps: 89, steps per second: 93, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001453, mean_absolute_error: 0.523244, mean_q: 1.139359\n",
      " 74849/100000: episode: 841, duration: 1.118s, episode steps: 89, steps per second: 80, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001493, mean_absolute_error: 0.505454, mean_q: 1.106482\n",
      " 74938/100000: episode: 842, duration: 0.955s, episode steps: 89, steps per second: 93, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001196, mean_absolute_error: 0.513769, mean_q: 1.128654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 75027/100000: episode: 843, duration: 1.152s, episode steps: 89, steps per second: 77, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001510, mean_absolute_error: 0.521081, mean_q: 1.139827\n",
      " 75116/100000: episode: 844, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001411, mean_absolute_error: 0.509419, mean_q: 1.112168\n",
      " 75205/100000: episode: 845, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001311, mean_absolute_error: 0.505535, mean_q: 1.112146\n",
      " 75294/100000: episode: 846, duration: 1.041s, episode steps: 89, steps per second: 85, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001451, mean_absolute_error: 0.510678, mean_q: 1.115705\n",
      " 75383/100000: episode: 847, duration: 0.980s, episode steps: 89, steps per second: 91, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001329, mean_absolute_error: 0.506105, mean_q: 1.106188\n",
      " 75472/100000: episode: 848, duration: 1.027s, episode steps: 89, steps per second: 87, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001399, mean_absolute_error: 0.518800, mean_q: 1.128149\n",
      " 75561/100000: episode: 849, duration: 1.074s, episode steps: 89, steps per second: 83, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001330, mean_absolute_error: 0.517952, mean_q: 1.130460\n",
      " 75650/100000: episode: 850, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001194, mean_absolute_error: 0.511674, mean_q: 1.126424\n",
      " 75739/100000: episode: 851, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001458, mean_absolute_error: 0.518467, mean_q: 1.131397\n",
      " 75828/100000: episode: 852, duration: 1.008s, episode steps: 89, steps per second: 88, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001297, mean_absolute_error: 0.524714, mean_q: 1.144524\n",
      " 75917/100000: episode: 853, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001416, mean_absolute_error: 0.528028, mean_q: 1.153833\n",
      " 76006/100000: episode: 854, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001106, mean_absolute_error: 0.531540, mean_q: 1.165162\n",
      " 76095/100000: episode: 855, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001372, mean_absolute_error: 0.529786, mean_q: 1.154637\n",
      " 76184/100000: episode: 856, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001374, mean_absolute_error: 0.527218, mean_q: 1.150845\n",
      " 76273/100000: episode: 857, duration: 0.935s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001406, mean_absolute_error: 0.521053, mean_q: 1.137450\n",
      " 76362/100000: episode: 858, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001422, mean_absolute_error: 0.519610, mean_q: 1.129650\n",
      " 76451/100000: episode: 859, duration: 0.972s, episode steps: 89, steps per second: 92, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001393, mean_absolute_error: 0.515732, mean_q: 1.121090\n",
      " 76540/100000: episode: 860, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001321, mean_absolute_error: 0.519528, mean_q: 1.131731\n",
      " 76629/100000: episode: 861, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001344, mean_absolute_error: 0.523352, mean_q: 1.141053\n",
      " 76718/100000: episode: 862, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001383, mean_absolute_error: 0.520567, mean_q: 1.134510\n",
      " 76807/100000: episode: 863, duration: 1.128s, episode steps: 89, steps per second: 79, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001356, mean_absolute_error: 0.520416, mean_q: 1.138566\n",
      " 76896/100000: episode: 864, duration: 0.961s, episode steps: 89, steps per second: 93, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001296, mean_absolute_error: 0.527089, mean_q: 1.149705\n",
      " 76985/100000: episode: 865, duration: 0.947s, episode steps: 89, steps per second: 94, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001334, mean_absolute_error: 0.522745, mean_q: 1.145318\n",
      " 77074/100000: episode: 866, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001206, mean_absolute_error: 0.513771, mean_q: 1.126261\n",
      " 77163/100000: episode: 867, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001313, mean_absolute_error: 0.504738, mean_q: 1.106999\n",
      " 77252/100000: episode: 868, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001226, mean_absolute_error: 0.503148, mean_q: 1.108956\n",
      " 77341/100000: episode: 869, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001360, mean_absolute_error: 0.514410, mean_q: 1.127362\n",
      " 77430/100000: episode: 870, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001413, mean_absolute_error: 0.512298, mean_q: 1.120299\n",
      " 77519/100000: episode: 871, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001381, mean_absolute_error: 0.502710, mean_q: 1.102938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 77608/100000: episode: 872, duration: 0.963s, episode steps: 89, steps per second: 92, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001273, mean_absolute_error: 0.501922, mean_q: 1.103748\n",
      " 77697/100000: episode: 873, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001248, mean_absolute_error: 0.495848, mean_q: 1.088795\n",
      " 77786/100000: episode: 874, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001442, mean_absolute_error: 0.501960, mean_q: 1.099968\n",
      " 77875/100000: episode: 875, duration: 0.966s, episode steps: 89, steps per second: 92, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001199, mean_absolute_error: 0.494577, mean_q: 1.088093\n",
      " 77964/100000: episode: 876, duration: 0.965s, episode steps: 89, steps per second: 92, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001355, mean_absolute_error: 0.505345, mean_q: 1.102910\n",
      " 78053/100000: episode: 877, duration: 0.932s, episode steps: 89, steps per second: 95, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001455, mean_absolute_error: 0.511231, mean_q: 1.117826\n",
      " 78142/100000: episode: 878, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001394, mean_absolute_error: 0.514589, mean_q: 1.128339\n",
      " 78231/100000: episode: 879, duration: 1.016s, episode steps: 89, steps per second: 88, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001183, mean_absolute_error: 0.516227, mean_q: 1.138444\n",
      " 78320/100000: episode: 880, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001098, mean_absolute_error: 0.527785, mean_q: 1.158789\n",
      " 78409/100000: episode: 881, duration: 0.948s, episode steps: 89, steps per second: 94, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001290, mean_absolute_error: 0.533376, mean_q: 1.169143\n",
      " 78498/100000: episode: 882, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001353, mean_absolute_error: 0.518712, mean_q: 1.137792\n",
      " 78587/100000: episode: 883, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001347, mean_absolute_error: 0.515135, mean_q: 1.126389\n",
      " 78676/100000: episode: 884, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001211, mean_absolute_error: 0.517872, mean_q: 1.144446\n",
      " 78765/100000: episode: 885, duration: 0.917s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001324, mean_absolute_error: 0.535043, mean_q: 1.169096\n",
      " 78854/100000: episode: 886, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001294, mean_absolute_error: 0.528366, mean_q: 1.155465\n",
      " 78943/100000: episode: 887, duration: 0.908s, episode steps: 89, steps per second: 98, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001156, mean_absolute_error: 0.524486, mean_q: 1.154045\n",
      " 79032/100000: episode: 888, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001192, mean_absolute_error: 0.517662, mean_q: 1.137771\n",
      " 79121/100000: episode: 889, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001390, mean_absolute_error: 0.513797, mean_q: 1.125044\n",
      " 79210/100000: episode: 890, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001266, mean_absolute_error: 0.519192, mean_q: 1.140402\n",
      " 79299/100000: episode: 891, duration: 0.904s, episode steps: 89, steps per second: 98, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001303, mean_absolute_error: 0.523517, mean_q: 1.145914\n",
      " 79388/100000: episode: 892, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001178, mean_absolute_error: 0.516672, mean_q: 1.133397\n",
      " 79477/100000: episode: 893, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001441, mean_absolute_error: 0.510538, mean_q: 1.122258\n",
      " 79566/100000: episode: 894, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001302, mean_absolute_error: 0.518043, mean_q: 1.134158\n",
      " 79655/100000: episode: 895, duration: 0.957s, episode steps: 89, steps per second: 93, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001183, mean_absolute_error: 0.521127, mean_q: 1.142661\n",
      " 79744/100000: episode: 896, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001352, mean_absolute_error: 0.520970, mean_q: 1.142126\n",
      " 79833/100000: episode: 897, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001193, mean_absolute_error: 0.516993, mean_q: 1.133524\n",
      " 79922/100000: episode: 898, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001006, mean_absolute_error: 0.504577, mean_q: 1.117906\n",
      " 80011/100000: episode: 899, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001314, mean_absolute_error: 0.510834, mean_q: 1.125222\n",
      " 80100/100000: episode: 900, duration: 1.002s, episode steps: 89, steps per second: 89, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001361, mean_absolute_error: 0.514518, mean_q: 1.129336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80189/100000: episode: 901, duration: 1.136s, episode steps: 89, steps per second: 78, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001327, mean_absolute_error: 0.518751, mean_q: 1.140704\n",
      " 80278/100000: episode: 902, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001190, mean_absolute_error: 0.528002, mean_q: 1.157325\n",
      " 80367/100000: episode: 903, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001237, mean_absolute_error: 0.515065, mean_q: 1.130991\n",
      " 80456/100000: episode: 904, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001056, mean_absolute_error: 0.512222, mean_q: 1.125885\n",
      " 80545/100000: episode: 905, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001154, mean_absolute_error: 0.516896, mean_q: 1.137820\n",
      " 80634/100000: episode: 906, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001191, mean_absolute_error: 0.522697, mean_q: 1.145123\n",
      " 80723/100000: episode: 907, duration: 1.127s, episode steps: 89, steps per second: 79, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001199, mean_absolute_error: 0.513015, mean_q: 1.125542\n",
      " 80812/100000: episode: 908, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001347, mean_absolute_error: 0.524595, mean_q: 1.146150\n",
      " 80901/100000: episode: 909, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001085, mean_absolute_error: 0.515790, mean_q: 1.137385\n",
      " 80990/100000: episode: 910, duration: 1.327s, episode steps: 89, steps per second: 67, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001050, mean_absolute_error: 0.524871, mean_q: 1.154468\n",
      " 81079/100000: episode: 911, duration: 0.935s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001231, mean_absolute_error: 0.515909, mean_q: 1.136558\n",
      " 81168/100000: episode: 912, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001235, mean_absolute_error: 0.527958, mean_q: 1.156747\n",
      " 81257/100000: episode: 913, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001223, mean_absolute_error: 0.529521, mean_q: 1.156648\n",
      " 81346/100000: episode: 914, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001271, mean_absolute_error: 0.540395, mean_q: 1.181200\n",
      " 81435/100000: episode: 915, duration: 0.922s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001363, mean_absolute_error: 0.544129, mean_q: 1.186904\n",
      " 81524/100000: episode: 916, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001253, mean_absolute_error: 0.535359, mean_q: 1.171942\n",
      " 81613/100000: episode: 917, duration: 1.102s, episode steps: 89, steps per second: 81, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001256, mean_absolute_error: 0.537312, mean_q: 1.175185\n",
      " 81702/100000: episode: 918, duration: 1.148s, episode steps: 89, steps per second: 77, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001084, mean_absolute_error: 0.525281, mean_q: 1.157452\n",
      " 81791/100000: episode: 919, duration: 1.038s, episode steps: 89, steps per second: 86, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001380, mean_absolute_error: 0.530687, mean_q: 1.156078\n",
      " 81880/100000: episode: 920, duration: 1.036s, episode steps: 89, steps per second: 86, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001255, mean_absolute_error: 0.536348, mean_q: 1.171499\n",
      " 81969/100000: episode: 921, duration: 1.114s, episode steps: 89, steps per second: 80, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001079, mean_absolute_error: 0.534224, mean_q: 1.177819\n",
      " 82058/100000: episode: 922, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001206, mean_absolute_error: 0.534323, mean_q: 1.170045\n",
      " 82147/100000: episode: 923, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001016, mean_absolute_error: 0.531107, mean_q: 1.168306\n",
      " 82236/100000: episode: 924, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001047, mean_absolute_error: 0.532491, mean_q: 1.171420\n",
      " 82325/100000: episode: 925, duration: 0.961s, episode steps: 89, steps per second: 93, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001172, mean_absolute_error: 0.532291, mean_q: 1.168637\n",
      " 82414/100000: episode: 926, duration: 0.914s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001120, mean_absolute_error: 0.528302, mean_q: 1.161268\n",
      " 82503/100000: episode: 927, duration: 1.116s, episode steps: 89, steps per second: 80, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001142, mean_absolute_error: 0.536087, mean_q: 1.174493\n",
      " 82592/100000: episode: 928, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001111, mean_absolute_error: 0.521902, mean_q: 1.147492\n",
      " 82681/100000: episode: 929, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001125, mean_absolute_error: 0.519370, mean_q: 1.143109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82770/100000: episode: 930, duration: 1.110s, episode steps: 89, steps per second: 80, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001107, mean_absolute_error: 0.518195, mean_q: 1.141136\n",
      " 82859/100000: episode: 931, duration: 0.952s, episode steps: 89, steps per second: 93, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001180, mean_absolute_error: 0.513194, mean_q: 1.132109\n",
      " 82948/100000: episode: 932, duration: 0.947s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001130, mean_absolute_error: 0.512250, mean_q: 1.130357\n",
      " 83037/100000: episode: 933, duration: 0.971s, episode steps: 89, steps per second: 92, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001061, mean_absolute_error: 0.516612, mean_q: 1.143548\n",
      " 83126/100000: episode: 934, duration: 1.106s, episode steps: 89, steps per second: 80, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001018, mean_absolute_error: 0.523998, mean_q: 1.159168\n",
      " 83215/100000: episode: 935, duration: 0.995s, episode steps: 89, steps per second: 89, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001212, mean_absolute_error: 0.532603, mean_q: 1.170020\n",
      " 83304/100000: episode: 936, duration: 1.123s, episode steps: 89, steps per second: 79, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001258, mean_absolute_error: 0.538315, mean_q: 1.183845\n",
      " 83393/100000: episode: 937, duration: 1.103s, episode steps: 89, steps per second: 81, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001094, mean_absolute_error: 0.520680, mean_q: 1.148031\n",
      " 83482/100000: episode: 938, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001225, mean_absolute_error: 0.529649, mean_q: 1.163147\n",
      " 83571/100000: episode: 939, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 5.571, mean reward: 0.063 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001246, mean_absolute_error: 0.524722, mean_q: 1.153024\n",
      " 83660/100000: episode: 940, duration: 0.946s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001290, mean_absolute_error: 0.527068, mean_q: 1.159429\n",
      " 83749/100000: episode: 941, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001166, mean_absolute_error: 0.527384, mean_q: 1.154385\n",
      " 83838/100000: episode: 942, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001097, mean_absolute_error: 0.527641, mean_q: 1.157925\n",
      " 83927/100000: episode: 943, duration: 1.091s, episode steps: 89, steps per second: 82, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001249, mean_absolute_error: 0.523331, mean_q: 1.150918\n",
      " 84016/100000: episode: 944, duration: 1.106s, episode steps: 89, steps per second: 80, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001026, mean_absolute_error: 0.520539, mean_q: 1.145184\n",
      " 84105/100000: episode: 945, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001227, mean_absolute_error: 0.512408, mean_q: 1.126987\n",
      " 84194/100000: episode: 946, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001106, mean_absolute_error: 0.515458, mean_q: 1.137590\n",
      " 84283/100000: episode: 947, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001053, mean_absolute_error: 0.528499, mean_q: 1.164788\n",
      " 84372/100000: episode: 948, duration: 1.107s, episode steps: 89, steps per second: 80, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001114, mean_absolute_error: 0.531360, mean_q: 1.166422\n",
      " 84461/100000: episode: 949, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001123, mean_absolute_error: 0.549834, mean_q: 1.204834\n",
      " 84550/100000: episode: 950, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001233, mean_absolute_error: 0.539252, mean_q: 1.183241\n",
      " 84639/100000: episode: 951, duration: 0.939s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001222, mean_absolute_error: 0.525055, mean_q: 1.155136\n",
      " 84728/100000: episode: 952, duration: 1.088s, episode steps: 89, steps per second: 82, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001257, mean_absolute_error: 0.533934, mean_q: 1.169296\n",
      " 84817/100000: episode: 953, duration: 0.956s, episode steps: 89, steps per second: 93, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001156, mean_absolute_error: 0.534771, mean_q: 1.175605\n",
      " 84906/100000: episode: 954, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001197, mean_absolute_error: 0.524966, mean_q: 1.148899\n",
      " 84995/100000: episode: 955, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001164, mean_absolute_error: 0.538976, mean_q: 1.184297\n",
      " 85084/100000: episode: 956, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001056, mean_absolute_error: 0.543970, mean_q: 1.194278\n",
      " 85173/100000: episode: 957, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000962, mean_absolute_error: 0.549666, mean_q: 1.201998\n",
      " 85262/100000: episode: 958, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001355, mean_absolute_error: 0.543450, mean_q: 1.191538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 85351/100000: episode: 959, duration: 1.114s, episode steps: 89, steps per second: 80, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001133, mean_absolute_error: 0.540811, mean_q: 1.187284\n",
      " 85440/100000: episode: 960, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001197, mean_absolute_error: 0.532331, mean_q: 1.171783\n",
      " 85529/100000: episode: 961, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001108, mean_absolute_error: 0.534411, mean_q: 1.176927\n",
      " 85618/100000: episode: 962, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001104, mean_absolute_error: 0.541613, mean_q: 1.194446\n",
      " 85707/100000: episode: 963, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001219, mean_absolute_error: 0.520409, mean_q: 1.145395\n",
      " 85796/100000: episode: 964, duration: 1.317s, episode steps: 89, steps per second: 68, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001152, mean_absolute_error: 0.523235, mean_q: 1.149225\n",
      " 85885/100000: episode: 965, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001063, mean_absolute_error: 0.529432, mean_q: 1.164285\n",
      " 85974/100000: episode: 966, duration: 1.140s, episode steps: 89, steps per second: 78, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001195, mean_absolute_error: 0.535286, mean_q: 1.179499\n",
      " 86063/100000: episode: 967, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001068, mean_absolute_error: 0.527187, mean_q: 1.161236\n",
      " 86152/100000: episode: 968, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001176, mean_absolute_error: 0.524772, mean_q: 1.153862\n",
      " 86241/100000: episode: 969, duration: 1.137s, episode steps: 89, steps per second: 78, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001146, mean_absolute_error: 0.529121, mean_q: 1.160654\n",
      " 86330/100000: episode: 970, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000977, mean_absolute_error: 0.522699, mean_q: 1.154962\n",
      " 86419/100000: episode: 971, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001288, mean_absolute_error: 0.520245, mean_q: 1.141638\n",
      " 86508/100000: episode: 972, duration: 0.919s, episode steps: 89, steps per second: 97, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001209, mean_absolute_error: 0.520381, mean_q: 1.143700\n",
      " 86597/100000: episode: 973, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001095, mean_absolute_error: 0.515898, mean_q: 1.136915\n",
      " 86686/100000: episode: 974, duration: 1.001s, episode steps: 89, steps per second: 89, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000982, mean_absolute_error: 0.517773, mean_q: 1.144109\n",
      " 86775/100000: episode: 975, duration: 1.050s, episode steps: 89, steps per second: 85, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000982, mean_absolute_error: 0.519025, mean_q: 1.150023\n",
      " 86864/100000: episode: 976, duration: 1.130s, episode steps: 89, steps per second: 79, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001030, mean_absolute_error: 0.526669, mean_q: 1.155241\n",
      " 86953/100000: episode: 977, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001140, mean_absolute_error: 0.531733, mean_q: 1.167127\n",
      " 87042/100000: episode: 978, duration: 1.298s, episode steps: 89, steps per second: 69, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001043, mean_absolute_error: 0.527156, mean_q: 1.159884\n",
      " 87131/100000: episode: 979, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001031, mean_absolute_error: 0.532288, mean_q: 1.170882\n",
      " 87220/100000: episode: 980, duration: 0.949s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001307, mean_absolute_error: 0.531341, mean_q: 1.166806\n",
      " 87309/100000: episode: 981, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001073, mean_absolute_error: 0.533820, mean_q: 1.175837\n",
      " 87398/100000: episode: 982, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001093, mean_absolute_error: 0.529453, mean_q: 1.168885\n",
      " 87487/100000: episode: 983, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001126, mean_absolute_error: 0.533688, mean_q: 1.172162\n",
      " 87576/100000: episode: 984, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001204, mean_absolute_error: 0.524295, mean_q: 1.153734\n",
      " 87665/100000: episode: 985, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001401, mean_absolute_error: 0.513843, mean_q: 1.135431\n",
      " 87754/100000: episode: 986, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001140, mean_absolute_error: 0.518205, mean_q: 1.143832\n",
      " 87843/100000: episode: 987, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001131, mean_absolute_error: 0.525628, mean_q: 1.159950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 87932/100000: episode: 988, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001118, mean_absolute_error: 0.534107, mean_q: 1.173891\n",
      " 88021/100000: episode: 989, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001021, mean_absolute_error: 0.526553, mean_q: 1.160521\n",
      " 88110/100000: episode: 990, duration: 0.931s, episode steps: 89, steps per second: 96, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001175, mean_absolute_error: 0.524980, mean_q: 1.149394\n",
      " 88199/100000: episode: 991, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001114, mean_absolute_error: 0.516196, mean_q: 1.139391\n",
      " 88288/100000: episode: 992, duration: 0.944s, episode steps: 89, steps per second: 94, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001002, mean_absolute_error: 0.524110, mean_q: 1.159342\n",
      " 88377/100000: episode: 993, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001020, mean_absolute_error: 0.512273, mean_q: 1.137911\n",
      " 88466/100000: episode: 994, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001053, mean_absolute_error: 0.521595, mean_q: 1.156862\n",
      " 88555/100000: episode: 995, duration: 0.942s, episode steps: 89, steps per second: 94, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001203, mean_absolute_error: 0.536357, mean_q: 1.173941\n",
      " 88644/100000: episode: 996, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000989, mean_absolute_error: 0.526205, mean_q: 1.162632\n",
      " 88733/100000: episode: 997, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.528 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001089, mean_absolute_error: 0.533507, mean_q: 1.174428\n",
      " 88822/100000: episode: 998, duration: 0.926s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000974, mean_absolute_error: 0.521547, mean_q: 1.148105\n",
      " 88911/100000: episode: 999, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001131, mean_absolute_error: 0.530139, mean_q: 1.170082\n",
      " 89000/100000: episode: 1000, duration: 0.941s, episode steps: 89, steps per second: 95, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000947, mean_absolute_error: 0.530720, mean_q: 1.171414\n",
      " 89089/100000: episode: 1001, duration: 0.902s, episode steps: 89, steps per second: 99, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001087, mean_absolute_error: 0.533013, mean_q: 1.170428\n",
      " 89178/100000: episode: 1002, duration: 0.914s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000938, mean_absolute_error: 0.530490, mean_q: 1.175357\n",
      " 89267/100000: episode: 1003, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001052, mean_absolute_error: 0.536704, mean_q: 1.179861\n",
      " 89356/100000: episode: 1004, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001062, mean_absolute_error: 0.536231, mean_q: 1.179604\n",
      " 89445/100000: episode: 1005, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001037, mean_absolute_error: 0.524542, mean_q: 1.156229\n",
      " 89534/100000: episode: 1006, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000975, mean_absolute_error: 0.518179, mean_q: 1.145503\n",
      " 89623/100000: episode: 1007, duration: 0.935s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001118, mean_absolute_error: 0.521136, mean_q: 1.142056\n",
      " 89712/100000: episode: 1008, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001156, mean_absolute_error: 0.522481, mean_q: 1.150422\n",
      " 89801/100000: episode: 1009, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001172, mean_absolute_error: 0.511312, mean_q: 1.123191\n",
      " 89890/100000: episode: 1010, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000957, mean_absolute_error: 0.517475, mean_q: 1.149731\n",
      " 89979/100000: episode: 1011, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001085, mean_absolute_error: 0.512774, mean_q: 1.133757\n",
      " 90068/100000: episode: 1012, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001135, mean_absolute_error: 0.529660, mean_q: 1.166207\n",
      " 90157/100000: episode: 1013, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001111, mean_absolute_error: 0.520266, mean_q: 1.146989\n",
      " 90246/100000: episode: 1014, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001173, mean_absolute_error: 0.528409, mean_q: 1.166938\n",
      " 90335/100000: episode: 1015, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001016, mean_absolute_error: 0.522690, mean_q: 1.160110\n",
      " 90424/100000: episode: 1016, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001019, mean_absolute_error: 0.533419, mean_q: 1.179877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 90513/100000: episode: 1017, duration: 0.934s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000990, mean_absolute_error: 0.535131, mean_q: 1.181973\n",
      " 90602/100000: episode: 1018, duration: 0.935s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001109, mean_absolute_error: 0.518514, mean_q: 1.145136\n",
      " 90691/100000: episode: 1019, duration: 0.920s, episode steps: 89, steps per second: 97, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001007, mean_absolute_error: 0.524659, mean_q: 1.161209\n",
      " 90780/100000: episode: 1020, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001099, mean_absolute_error: 0.517366, mean_q: 1.138408\n",
      " 90869/100000: episode: 1021, duration: 0.912s, episode steps: 89, steps per second: 98, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001075, mean_absolute_error: 0.521666, mean_q: 1.150913\n",
      " 90958/100000: episode: 1022, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001038, mean_absolute_error: 0.521886, mean_q: 1.152106\n",
      " 91047/100000: episode: 1023, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001101, mean_absolute_error: 0.514123, mean_q: 1.136976\n",
      " 91136/100000: episode: 1024, duration: 0.932s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001112, mean_absolute_error: 0.498659, mean_q: 1.104070\n",
      " 91225/100000: episode: 1025, duration: 0.949s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001169, mean_absolute_error: 0.509450, mean_q: 1.124552\n",
      " 91314/100000: episode: 1026, duration: 0.921s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001053, mean_absolute_error: 0.504425, mean_q: 1.117949\n",
      " 91403/100000: episode: 1027, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001087, mean_absolute_error: 0.517962, mean_q: 1.144274\n",
      " 91492/100000: episode: 1028, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001005, mean_absolute_error: 0.521112, mean_q: 1.152376\n",
      " 91581/100000: episode: 1029, duration: 0.911s, episode steps: 89, steps per second: 98, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000953, mean_absolute_error: 0.508586, mean_q: 1.126501\n",
      " 91670/100000: episode: 1030, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001023, mean_absolute_error: 0.521976, mean_q: 1.156326\n",
      " 91759/100000: episode: 1031, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001080, mean_absolute_error: 0.537842, mean_q: 1.184164\n",
      " 91848/100000: episode: 1032, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 5.286, mean reward: 0.059 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000990, mean_absolute_error: 0.521248, mean_q: 1.155847\n",
      " 91937/100000: episode: 1033, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001028, mean_absolute_error: 0.516459, mean_q: 1.145356\n",
      " 92026/100000: episode: 1034, duration: 0.924s, episode steps: 89, steps per second: 96, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000842, mean_absolute_error: 0.519662, mean_q: 1.152949\n",
      " 92115/100000: episode: 1035, duration: 0.913s, episode steps: 89, steps per second: 98, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001045, mean_absolute_error: 0.537248, mean_q: 1.181987\n",
      " 92204/100000: episode: 1036, duration: 0.938s, episode steps: 89, steps per second: 95, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000993, mean_absolute_error: 0.528503, mean_q: 1.172360\n",
      " 92293/100000: episode: 1037, duration: 0.948s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001099, mean_absolute_error: 0.533450, mean_q: 1.175968\n",
      " 92382/100000: episode: 1038, duration: 0.905s, episode steps: 89, steps per second: 98, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001018, mean_absolute_error: 0.525499, mean_q: 1.161754\n",
      " 92471/100000: episode: 1039, duration: 1.032s, episode steps: 89, steps per second: 86, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001082, mean_absolute_error: 0.522394, mean_q: 1.158523\n",
      " 92560/100000: episode: 1040, duration: 1.007s, episode steps: 89, steps per second: 88, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001005, mean_absolute_error: 0.525795, mean_q: 1.162961\n",
      " 92649/100000: episode: 1041, duration: 0.958s, episode steps: 89, steps per second: 93, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001035, mean_absolute_error: 0.525987, mean_q: 1.161666\n",
      " 92738/100000: episode: 1042, duration: 0.982s, episode steps: 89, steps per second: 91, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000923, mean_absolute_error: 0.535942, mean_q: 1.182659\n",
      " 92827/100000: episode: 1043, duration: 0.984s, episode steps: 89, steps per second: 90, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000980, mean_absolute_error: 0.538310, mean_q: 1.185160\n",
      " 92916/100000: episode: 1044, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001068, mean_absolute_error: 0.542601, mean_q: 1.189925\n",
      " 93005/100000: episode: 1045, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001061, mean_absolute_error: 0.544581, mean_q: 1.198303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 93094/100000: episode: 1046, duration: 0.925s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001029, mean_absolute_error: 0.542684, mean_q: 1.189095\n",
      " 93183/100000: episode: 1047, duration: 0.936s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000939, mean_absolute_error: 0.532589, mean_q: 1.172278\n",
      " 93272/100000: episode: 1048, duration: 0.937s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001003, mean_absolute_error: 0.522503, mean_q: 1.155704\n",
      " 93361/100000: episode: 1049, duration: 0.909s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000871, mean_absolute_error: 0.522964, mean_q: 1.156365\n",
      " 93450/100000: episode: 1050, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000990, mean_absolute_error: 0.519245, mean_q: 1.143583\n",
      " 93539/100000: episode: 1051, duration: 0.955s, episode steps: 89, steps per second: 93, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000993, mean_absolute_error: 0.526120, mean_q: 1.163444\n",
      " 93628/100000: episode: 1052, duration: 1.029s, episode steps: 89, steps per second: 86, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000859, mean_absolute_error: 0.521272, mean_q: 1.155318\n",
      " 93717/100000: episode: 1053, duration: 1.324s, episode steps: 89, steps per second: 67, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000979, mean_absolute_error: 0.524158, mean_q: 1.158808\n",
      " 93806/100000: episode: 1054, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000967, mean_absolute_error: 0.528180, mean_q: 1.168393\n",
      " 93895/100000: episode: 1055, duration: 0.928s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001133, mean_absolute_error: 0.528028, mean_q: 1.161842\n",
      " 93984/100000: episode: 1056, duration: 1.138s, episode steps: 89, steps per second: 78, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001075, mean_absolute_error: 0.526580, mean_q: 1.161909\n",
      " 94073/100000: episode: 1057, duration: 1.159s, episode steps: 89, steps per second: 77, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001198, mean_absolute_error: 0.531992, mean_q: 1.171370\n",
      " 94162/100000: episode: 1058, duration: 1.224s, episode steps: 89, steps per second: 73, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000924, mean_absolute_error: 0.529544, mean_q: 1.171194\n",
      " 94251/100000: episode: 1059, duration: 0.999s, episode steps: 89, steps per second: 89, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000960, mean_absolute_error: 0.529611, mean_q: 1.171148\n",
      " 94340/100000: episode: 1060, duration: 0.944s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000919, mean_absolute_error: 0.530545, mean_q: 1.179868\n",
      " 94429/100000: episode: 1061, duration: 0.932s, episode steps: 89, steps per second: 95, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000947, mean_absolute_error: 0.530497, mean_q: 1.173533\n",
      " 94518/100000: episode: 1062, duration: 0.906s, episode steps: 89, steps per second: 98, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001002, mean_absolute_error: 0.532310, mean_q: 1.172986\n",
      " 94607/100000: episode: 1063, duration: 0.913s, episode steps: 89, steps per second: 97, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000946, mean_absolute_error: 0.535294, mean_q: 1.182779\n",
      " 94696/100000: episode: 1064, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000854, mean_absolute_error: 0.534244, mean_q: 1.185609\n",
      " 94785/100000: episode: 1065, duration: 0.929s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000949, mean_absolute_error: 0.541780, mean_q: 1.196754\n",
      " 94874/100000: episode: 1066, duration: 0.940s, episode steps: 89, steps per second: 95, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000974, mean_absolute_error: 0.540813, mean_q: 1.189651\n",
      " 94963/100000: episode: 1067, duration: 0.917s, episode steps: 89, steps per second: 97, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000872, mean_absolute_error: 0.529359, mean_q: 1.170125\n",
      " 95052/100000: episode: 1068, duration: 0.933s, episode steps: 89, steps per second: 95, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000935, mean_absolute_error: 0.518418, mean_q: 1.151801\n",
      " 95141/100000: episode: 1069, duration: 0.910s, episode steps: 89, steps per second: 98, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001145, mean_absolute_error: 0.512915, mean_q: 1.136006\n",
      " 95230/100000: episode: 1070, duration: 0.932s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000989, mean_absolute_error: 0.511493, mean_q: 1.138952\n",
      " 95319/100000: episode: 1071, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001137, mean_absolute_error: 0.510535, mean_q: 1.125743\n",
      " 95408/100000: episode: 1072, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.360 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000973, mean_absolute_error: 0.510047, mean_q: 1.131899\n",
      " 95497/100000: episode: 1073, duration: 0.927s, episode steps: 89, steps per second: 96, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000960, mean_absolute_error: 0.508853, mean_q: 1.124174\n",
      " 95586/100000: episode: 1074, duration: 0.922s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001097, mean_absolute_error: 0.510241, mean_q: 1.129585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 95675/100000: episode: 1075, duration: 0.942s, episode steps: 89, steps per second: 95, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000960, mean_absolute_error: 0.510884, mean_q: 1.131688\n",
      " 95764/100000: episode: 1076, duration: 0.916s, episode steps: 89, steps per second: 97, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.360 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001032, mean_absolute_error: 0.518513, mean_q: 1.148485\n",
      " 95853/100000: episode: 1077, duration: 0.930s, episode steps: 89, steps per second: 96, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001060, mean_absolute_error: 0.531772, mean_q: 1.172041\n",
      " 95942/100000: episode: 1078, duration: 0.945s, episode steps: 89, steps per second: 94, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000968, mean_absolute_error: 0.515295, mean_q: 1.139471\n",
      " 96031/100000: episode: 1079, duration: 0.943s, episode steps: 89, steps per second: 94, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000952, mean_absolute_error: 0.525073, mean_q: 1.167933\n",
      " 96120/100000: episode: 1080, duration: 0.918s, episode steps: 89, steps per second: 97, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000991, mean_absolute_error: 0.526857, mean_q: 1.165685\n",
      " 96209/100000: episode: 1081, duration: 0.923s, episode steps: 89, steps per second: 96, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000988, mean_absolute_error: 0.515992, mean_q: 1.143838\n",
      " 96298/100000: episode: 1082, duration: 0.959s, episode steps: 89, steps per second: 93, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001047, mean_absolute_error: 0.508086, mean_q: 1.126897\n",
      " 96387/100000: episode: 1083, duration: 1.085s, episode steps: 89, steps per second: 82, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001138, mean_absolute_error: 0.504591, mean_q: 1.120243\n",
      " 96476/100000: episode: 1084, duration: 1.041s, episode steps: 89, steps per second: 85, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000970, mean_absolute_error: 0.520216, mean_q: 1.150928\n",
      " 96565/100000: episode: 1085, duration: 1.245s, episode steps: 89, steps per second: 71, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001173, mean_absolute_error: 0.511222, mean_q: 1.132897\n",
      " 96654/100000: episode: 1086, duration: 1.228s, episode steps: 89, steps per second: 72, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001021, mean_absolute_error: 0.503997, mean_q: 1.119081\n",
      " 96743/100000: episode: 1087, duration: 1.194s, episode steps: 89, steps per second: 75, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000928, mean_absolute_error: 0.520282, mean_q: 1.156019\n",
      " 96832/100000: episode: 1088, duration: 1.125s, episode steps: 89, steps per second: 79, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000956, mean_absolute_error: 0.507121, mean_q: 1.124540\n",
      " 96921/100000: episode: 1089, duration: 1.075s, episode steps: 89, steps per second: 83, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000876, mean_absolute_error: 0.516270, mean_q: 1.146128\n",
      " 97010/100000: episode: 1090, duration: 1.115s, episode steps: 89, steps per second: 80, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000967, mean_absolute_error: 0.509228, mean_q: 1.126729\n",
      " 97099/100000: episode: 1091, duration: 1.093s, episode steps: 89, steps per second: 81, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000921, mean_absolute_error: 0.505368, mean_q: 1.123376\n",
      " 97188/100000: episode: 1092, duration: 1.120s, episode steps: 89, steps per second: 79, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000849, mean_absolute_error: 0.510275, mean_q: 1.130098\n",
      " 97277/100000: episode: 1093, duration: 1.085s, episode steps: 89, steps per second: 82, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001013, mean_absolute_error: 0.519732, mean_q: 1.151603\n",
      " 97366/100000: episode: 1094, duration: 1.111s, episode steps: 89, steps per second: 80, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.472 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000975, mean_absolute_error: 0.512842, mean_q: 1.135223\n",
      " 97455/100000: episode: 1095, duration: 1.106s, episode steps: 89, steps per second: 80, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000770, mean_absolute_error: 0.519564, mean_q: 1.156039\n",
      " 97544/100000: episode: 1096, duration: 1.111s, episode steps: 89, steps per second: 80, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001090, mean_absolute_error: 0.531078, mean_q: 1.166399\n",
      " 97633/100000: episode: 1097, duration: 1.128s, episode steps: 89, steps per second: 79, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.371 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001035, mean_absolute_error: 0.520416, mean_q: 1.151272\n",
      " 97722/100000: episode: 1098, duration: 1.093s, episode steps: 89, steps per second: 81, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000945, mean_absolute_error: 0.517763, mean_q: 1.147045\n",
      " 97811/100000: episode: 1099, duration: 1.145s, episode steps: 89, steps per second: 78, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001009, mean_absolute_error: 0.507284, mean_q: 1.122524\n",
      " 97900/100000: episode: 1100, duration: 1.104s, episode steps: 89, steps per second: 81, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001100, mean_absolute_error: 0.492090, mean_q: 1.094296\n",
      " 97989/100000: episode: 1101, duration: 1.108s, episode steps: 89, steps per second: 80, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000996, mean_absolute_error: 0.504964, mean_q: 1.121354\n",
      " 98078/100000: episode: 1102, duration: 1.087s, episode steps: 89, steps per second: 82, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000947, mean_absolute_error: 0.516465, mean_q: 1.146930\n",
      " 98167/100000: episode: 1103, duration: 1.105s, episode steps: 89, steps per second: 81, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.483 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000966, mean_absolute_error: 0.506473, mean_q: 1.124985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98256/100000: episode: 1104, duration: 1.134s, episode steps: 89, steps per second: 78, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000944, mean_absolute_error: 0.515493, mean_q: 1.146190\n",
      " 98345/100000: episode: 1105, duration: 1.128s, episode steps: 89, steps per second: 79, episode reward: 7.571, mean reward: 0.085 [0.000, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000957, mean_absolute_error: 0.520864, mean_q: 1.154970\n",
      " 98434/100000: episode: 1106, duration: 1.121s, episode steps: 89, steps per second: 79, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.517 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001069, mean_absolute_error: 0.511832, mean_q: 1.131700\n",
      " 98523/100000: episode: 1107, duration: 1.142s, episode steps: 89, steps per second: 78, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001027, mean_absolute_error: 0.505911, mean_q: 1.121472\n",
      " 98612/100000: episode: 1108, duration: 1.107s, episode steps: 89, steps per second: 80, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.404 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000989, mean_absolute_error: 0.525711, mean_q: 1.161168\n",
      " 98701/100000: episode: 1109, duration: 1.107s, episode steps: 89, steps per second: 80, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.494 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000912, mean_absolute_error: 0.510961, mean_q: 1.130719\n",
      " 98790/100000: episode: 1110, duration: 1.082s, episode steps: 89, steps per second: 82, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.393 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001061, mean_absolute_error: 0.526049, mean_q: 1.162218\n",
      " 98879/100000: episode: 1111, duration: 1.075s, episode steps: 89, steps per second: 83, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.506 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001085, mean_absolute_error: 0.524868, mean_q: 1.158668\n",
      " 98968/100000: episode: 1112, duration: 1.075s, episode steps: 89, steps per second: 83, episode reward: 6.429, mean reward: 0.072 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001061, mean_absolute_error: 0.514165, mean_q: 1.143129\n",
      " 99057/100000: episode: 1113, duration: 1.069s, episode steps: 89, steps per second: 83, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001096, mean_absolute_error: 0.517101, mean_q: 1.143289\n",
      " 99146/100000: episode: 1114, duration: 1.099s, episode steps: 89, steps per second: 81, episode reward: 5.857, mean reward: 0.066 [-0.143, 0.143], mean action: 0.427 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000936, mean_absolute_error: 0.508115, mean_q: 1.124675\n",
      " 99235/100000: episode: 1115, duration: 1.091s, episode steps: 89, steps per second: 82, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000979, mean_absolute_error: 0.517473, mean_q: 1.150635\n",
      " 99324/100000: episode: 1116, duration: 1.126s, episode steps: 89, steps per second: 79, episode reward: 6.714, mean reward: 0.075 [-0.143, 0.143], mean action: 0.461 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001017, mean_absolute_error: 0.533107, mean_q: 1.179704\n",
      " 99413/100000: episode: 1117, duration: 1.085s, episode steps: 89, steps per second: 82, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000824, mean_absolute_error: 0.525475, mean_q: 1.160509\n",
      " 99502/100000: episode: 1118, duration: 1.075s, episode steps: 89, steps per second: 83, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000847, mean_absolute_error: 0.527434, mean_q: 1.175683\n",
      " 99591/100000: episode: 1119, duration: 1.092s, episode steps: 89, steps per second: 81, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.416 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000985, mean_absolute_error: 0.522296, mean_q: 1.156132\n",
      " 99680/100000: episode: 1120, duration: 1.104s, episode steps: 89, steps per second: 81, episode reward: 7.286, mean reward: 0.082 [-0.143, 0.143], mean action: 0.438 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001006, mean_absolute_error: 0.510593, mean_q: 1.129516\n",
      " 99769/100000: episode: 1121, duration: 1.063s, episode steps: 89, steps per second: 84, episode reward: 6.143, mean reward: 0.069 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000915, mean_absolute_error: 0.512893, mean_q: 1.137317\n",
      " 99858/100000: episode: 1122, duration: 1.082s, episode steps: 89, steps per second: 82, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.382 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.001049, mean_absolute_error: 0.511652, mean_q: 1.133196\n",
      " 99947/100000: episode: 1123, duration: 1.017s, episode steps: 89, steps per second: 88, episode reward: 7.000, mean reward: 0.079 [-0.143, 0.143], mean action: 0.449 [0.000, 1.000], mean observation: 0.439 [0.000, 1.000], loss: 0.000801, mean_absolute_error: 0.506066, mean_q: 1.127776\n",
      "done, took 1080.901 seconds\n"
     ]
    }
   ],
   "source": [
    "hist = dqn.fit(env, nb_steps=100000, visualize=False, verbose=2, callbacks=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('./weights/keras_rl_random_walk.hdf5', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.load_weights('./weights/keras_rl_random_walk.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22c18d99b70>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c1b8c0518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(hist.history.get('episode_reward'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 1 episodes ...\n",
      "Episode 1: reward: 0.444, steps: 89\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22c1e8826d8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztvXu0HNV55v3srktfqrr7oKPDTQJEQDYIsBFWsGeSNeNJ\nxuHiJOD58BdM4gTHlrjYg8YGoSNpJiaTkXSEwLZYMReJeIgTY+IlYxk7wgxfMk5mfYkTyxGxAYEl\ng0HIgKQjTndX36t6zx9Vu++X6u7qy6l+f2tp6XR1dddb5/JW9d7Pfh7GOQdBEAThLwKjLoAgCILw\nHmruBEEQPoSaO0EQhA+h5k4QBOFDqLkTBEH4EGruBEEQPoSaO0EQhA+h5k4QBOFDqLkTBEH4EHlU\nB166dClfsWLFqA5PEASxKPnRj350knM+02m/kTX3FStW4MCBA6M6PEEQxKKEMfaam/1oWIYgCMKH\nUHMnCILwIdTcCYIgfAg1d4IgCB9CzZ0gCMKHdGzujLGvMMaOM8aeb/E8Y4w9wBg7whj7MWPsCu/L\nJAiCILrBzZ37YwCubvP8NQBWOv/WAXio/7IIgiCIfujY3Dnnfw/gVJtdrgPwVW7zAwBTjLGzvCqQ\nIIjFx599/hb8+Z+sbdj+5S3r8LW5T46gosnDizH3ZQCOVj1+w9nWAGNsHWPsAGPswIkTJzw4NEEQ\n48jSS1/CzBWNixTPuvBtnPHLf4edW9aPoKrJYqgTqpzz3ZzzNZzzNTMzHVfPEgSxSFEiCaihFO6/\n566a7ZKWAGMcUTk3osomBy+a+zEA51Q9Xu5sIwhiAtm5ZT3UkAHGODQrVfOcHEkAAPSQOYrSJgov\nmvtTAH7fUc18AECCc/6mB+9LEMQiJCblwBgHUNvE77/nLqghu9kHNbpzHzQdjcMYY18H8EEASxlj\nbwD4PAAFADjnDwPYD+BaAEcAZAB8YlDFEgQx/mjhSkMPRitNPGIZCATspi9FjaHXNWl0bO6c8491\neJ4D+LRnFREEsahRnbtyzhkkvdLE9VCxsl1LjKS2SWJklr8EQfgTWTdgmTIKuRgkLVneHtTtpp9N\nnFEeeycGB9kPEAThKZKWRCEbh5meqmnicjQNy5JRmD8TasjAA9s2j7BK/0PNnSAIT5EjCZiZOKx0\nFGo4hT27tgIQTT8GK6UhEChBSs+PuFJ/Q82dIAjP2LNrK9RwCiUjCtOwm3jhxOsAKk0/mwkCAGIa\nySEHCTV3giA8wzxxFIFACUVDR95wmnjYcpp+EpYRQyrLAABBLT/KUn0PNXeCIDxDd2SQ+bQKIysB\nAFQtj8Lx1xAIlGAZGpasXA3LkiDr6VGW6nuouRME4RmqczeezAUwtfJylEoByLqBWLgEAMgbQdx0\n820oZGMI6Ml2b0X0CTV3giA8Q9YNWJaEJReuLjdxSU9B1Z2mn7VbjpWJQw6THHKQUHMnCMIzZD2J\nYi6Km26+DQBgZuKQIwnIehqlUgDq6ecBAKx0DGo4iccfo/iHQUHNnSAIz5AiCRQzU+XHdhNPQNKT\nKGSjWLt+CwDATGmQJAunjhwcVam+h5o7QRCe8PhjD0ENJ1EyYuVtZkqHJFkITb0Js6rp59MhAEAs\nVBp6nZMCNXeCIDzh1OGDkCQLpqGVt+XTthxSUXOw0pWmb2Rt55OgVhhukRMENXeCIDwhGrYdH0VD\nBwAjU7GvMlOVpi/PnINSKQBFJ3fIQUHNnSAITwhFHEVMutLQi5FplEr2oqW8ESpvX7t+CwrZKAJ6\nbZgH4R3U3AmC8AQ5mkapxGBp0+Vtd2zehkIuCgBI52pNaM1snNwhBwhZ/hKET3lgyy04YzqL3/nc\nV13t/+zefTj1xjfxxlth3Dm3u+vjSVoShVwUd2zeVrPdzMTBwymkpWjNdsuIIXzaMXzz0Y8AAEr5\nEKbO+B186Ibry/vcf89dOGfJm3jtWBwbdjzYdU2DZufG23HesgSOnjoLd95z36jLqYHu3AnCp5y9\nLIGll///eOi/3upq/5//ZD+WvucfcO7ZvdkCyJptDFZP4a1lMI6f39D8ssenwXkA0fNeRGzF81hy\n8T/j1R9/r2afM5R3MH3pD3D2kvGM5Vt2Wg7Tl/4AZyjvjLqUBujOnSB8iuSMZ+thd+6LkYjpvK77\nSc5n9+6DEkuicOqshuc+esfXmr7md2e/Uv76f/73dTj3V/8GeqRYs4/qxPSperbrmoaBErXrqo4T\nHBfozp0gfIqIsnMbRi0aaHV6klsOHXwaslyEldK7fi0ALDgfFpS6Ji47FxppTCdeRV29XBAHDTV3\ngvApijNZKbl0XxSNqpdJzljQMQZLhzrs2ZzP7tgNs6g2NHFxoZF7uOAMA1GXpI3fxYeaO0H4kLkN\na6EGMwBsvxc3iEZVnZ7klpBmD6fUK2K6oZCNN3xqEBeacVXVVOpbGHEljVBzJwgfMh2xteWlEoPk\nsjHKkQRKJVaTnuQWRU+Dc4ZkTuq6VoGViZc/bQBVqU4lBjVoYOfG23t+70Gwc+PtUIOGXV8PF8RB\nQ82dIHyI5kyiZhNnunJfFI0qmzgTgJ2e1A0BPYlCTutLrmgZUaghAzu3rAdQSXXKJs4EY0A81F1N\ngyYessCY/T3u5YI4aKi5E4QPUR0VR+HtZbb74uH27ouiURXeXma/vssIPDmSQLGJDLIbioYGxjhi\nkj0BLFQ+oqZIaLwyV7WQPRQl6uv2gjhoqLkThA8J6CkUi0FkF2z1ivB9aYVoVMbJWDk9qRuUSBKl\ndH/NPW+odi1OUxcqn8Rx+33HTQ6p6nX1jVkmLDV3gvAhUiSJYiZe9nkRvi+tUJxGdSorldOT3PKF\njbdAUfIwjd5kkIKFvF2r6jR1yQn4CK9YDdNUur7gDBpJN2CaCsIrVjsXxPHKhKXmThA+RIkkYGZi\nsDTbuEuOtm88stOoLnvf1eX0JLdMRWwZZNGI9FXzqtVXwbLkchOX9SQK2Rhuuvk2FLOxnvT3g0TS\nkihm41VxguNVHzV3gvAZOzfeDjWURsmIlY27OjVG0ag+dMP15fSkZ/fuc3W8sLOqNJ3tXSkDAB+6\n4XoUMpUmXp3qZKbHz2RMjiRgOkNR3V4QhwE1d4LwGbGQBcY4ik5ohpmJQ9baN56aRuWkJ710cL+r\n4ym6raefz7Qf13eDmZmCHEk0pDpZ6RjUUAoPbNvc9zG84P577oIaSsEybDO0bi+Iw4CaO0H4DM1R\nleTSCgDASsehhJMtG099oxJhG24j8CQ9hUI+gtmde/otHSUjCjWcwjt1qU5mSkMgwKFk5vs+hhdE\nLAOBAK+qr7sL4jBw1dwZY1czxl5mjB1hjM02eT7OGPsOY+xfGWMvMMY+4X2pBEG4QahMFhxLGSul\nQ5aLOHTw6ab71zcqkZ4UchmBJ2nJvmWQgqKhIxAoYXqJPQEsLjS5tK2k0SPjIYfUw/ZQVM6pr9sL\n4jDo2NwZYxKALwO4BsAqAB9jjK2q2+3TAF7knL8XwAcB3M8YUz2ulSAIF0hRe3L0kis+DKAqjDrY\nvPHUNyqRnuRW/aGEE7D6lEEK8k4TD575FoDKhSaZs1tVcEzkhqKOZN6eZ+j2gjgM3Ny5XwngCOf8\nFc55AcATAK6r24cDiDLGGAAdwCkA43GJJYgJQ9ISKGZj5dAL4fci/F/qKTeqnG1ZUJ6EdSGH3DG7\nDmowWx7S6RfDmZQNLz2KUomhGLFTnS5afS0sSxobuaEcNWBZMi66/GoA3V8Qh4Gb5r4MwNGqx284\n26r5UwAXA/gFgJ8AWM85H5/PJwQxQVRPjgJAMieBcwalReOxG5WEi1ZfW97mZhIWAJY6WdgFI9xf\n0Q5TKy+3g7OVfE2q04duuB6FbAyBMZEbSpot0xQX0G4uiMPCqwnVqwA8B+BsAJcD+FPGWKx+J8bY\nOsbYAcbYgRMnTnh0aIIgBA9s22xPjqYrf34bdjyIQk5r2RjtRhWvibez0jEoLtQfYefTQDajeFA9\nyppxAA2pTrax2Hi4L8qRxtQptxfEYeGmuR8DcE7V4+XOtmo+AeBJbnMEwKsALqp/I875bs75Gs75\nmpmZmV5rJgiiBUpm3p4cTWk124ttdNhNG5VhT8K+8C9/3f54mm0JMJ9nfVRdi9C2V1+g7JpirkzQ\nBo2QaVpGbX1uL4jDwk1z/yGAlYyx851J0hsBPFW3z+sAfh0AGGNnAHg3gFe8LJQgiM4INYlQlwhK\n6TiUSOOde6tGJYZZpoLtm7YcTaFYCOHu7Y/0U3ZtrU4t9XYGpqOkWTj8nGfH6oWFw88hECjBMmov\noG4viMOio7M+59xkjH0GwDMAJABf4Zy/wBi71Xn+YQB/AuAxxthPADAAGznnJwdYN0EMjT2fX4fp\nFW8DzF6kYxkaPvqZr7fcf27DWrzrXfOA4mgKLAmvHZ7BZ+d21+z3V1/6OE6d1HDb/3jYs1ork6O1\n922moUNR8vjWY78FjkrDDgespo2qMgnbqE554gt/AHWJHQgdOv0Nz2SQlVrtWurH8QuOmufsS1/B\nk4/9NlAK4K2fnYHbt3p3Ydn9R7ciFsvhxrsea7mP7rg/5o1gbX1OvSsufa1c37FXzsR/9vDn2w2u\nYlM45/sB7K/b9nDV178A8BvelkYQ48H0L/0CseWHYJpBBAIWJMnEri23Yf3W5sMD5yyxEL/gX2Ga\nCsAZZKWAs5O/UrPPF2fX4T2/8Q+QX3mvp7XKerphchQAUvM69HwE4bN+1vCaQk7Dwju1vjDlSdho\npmb7s3v3YcllPwDAUSrZ7SPz1rmensM7pzREUktxMlV7gTqZDGBJJobQEntUWFHyKOWCzd6iZ2ZW\nvg79rMN4YNvm8mRuPUFdXEBr7RbmDRVLslGEpt8o12el+/Pb6YfeM7EIYkKQtSSyydPxWx/5R/zl\n9k/irPd/H/Fga6WvsIJ94X9/EHJ0CS6+8hsNErkpx4LXazOsgGO29ZtVk6MA8Ief77B6tPZagA07\nHsT+/e9tyAZ96bnvYdWvmZj/yQfw/67/mhclN3DLHzt34nWC68/uqP3k873vXep5MLWkJRAIlCCl\nW6+ElR23SnWm9qJWfbF/du8+mLHZkQZnk/0AQXRAjlQW6Ri5zotVxCKiy953dZVErraJi9d7ra5Q\nIguwPBomMTNxSHX1xYJiSKK3IGwvKTSpr1+UsP1+Ma31xdtWF0Wxdv2Wlvt86IbrUWySCTtMqLkT\nRBtE/JyVthfppMwQOGeQ64YrqpG0BAqZirSwmWOg4gRPqG08X7pFTI6aRoMKuSfMdKwm0xSojMEb\nOW+kj/1gNamvH3bMroUatH8u7VbCyloCpqPoaceonSypuRNEG2JO/FwxZY+dbti6C4Wc3vaOTA4n\na+6emxl3icUukmTiZY/Mpk4dOYhAoNR3aIagZEShKHncP7uuvE3WM/bK0fAST47RD1Y6CjWYwfaq\n+vphWq1MNLdaafrs3n22uijd+QJqZ8KOzsmSmjtBtKHisFiZuGunGa84LFb++JsZd8laAqWS/ecX\n9chsSphWFdLeTDIWUrb647Qq0YqkJ2tWjo4S07ngTqvetLGwMxRTKgVaXrxfPrgfkmQ2rCNoWp8x\nWidLau4E0QaR27mQr3iVl9KxpppxoNphsaKSaGbcpYQTyCbOBOCd2VTQeZ9kn6EZgnTWnl8IRyr1\nSZEEzKw3wz79kskIp8jmnjndIn7W2cQZkFpcvMWF2M2cw6idLKm5E0QbRPyccFgEKprxL25sHA7Q\nQ7UOi0C1Ztxukvdtvg2KmkPh+Fn2+L3eevy+GxTdaKri6BURviHmB57duw9qxDsHyH5ZyNjDKIpH\n3z9JT6FYCMJcmGm5Elb8DMXPtB2jdrKk5k4QbaiOnxOIrNB4Ewlz0JFBCitYoNq4y25CS1RbcVJI\n6vb4vUdmUwE91VHF0Q2zO/egkI+U6/vJj74HWS7CSnkzpt8vn9vxCIrFoGfB2bKWQDEbh5nSIEkW\nTh052LiPngHnDGmpswtm2ckyOho5JDV3gmhDvcMiABiOSVYk0jicIkfTNVawQKNxl8gczWYUFDPx\nlkMA3de60OAR0y/FKjOsJWJMfwxkkIJiNoaAR3JI8bMuD6M1mQsRcw533nNfx/eznSxHJ4ek5k4Q\nLSg7LNZ5lc8X7D/6ZnLIeitYQfUkrLiDP5nndvZmZKFvOWQ3Ko5usNJxyI72W8gg02MggxRYLTxz\nusWWvKZhGTEYzlxDsMlciB3a7f57PMrgbGruBNGCssNine/KprndKOS0htWbQPM7faDWuEvSDRTy\nYWyc2w3LiEKWi3j+QH9yyBcPPgNJNmF5JIMUWIYONZjFjtm1kPUsOAcSOW8mbL3ATMeghgzs3LK+\nr/eJC8mrEYY8cw5KpUCDHLKXOQdrhE6W1NwJogWtHBYBezig/uP2nl1bW949W2l7EvYLG2+BpCVg\nZu0GIYY4hB1Br0w5dghiSMErxPzC0mDAHpLIa9iw40FPj9EPZioCxjiicq6v99GcifB8Ooi167c0\nXVX8/IH99pxDFxdQWw45GidLau4E0YJWDouAPRyg1n3cLhx/ranDIlDRjE9FSlCq7u7TWXuII9wi\nAs91rbqj4sh6axclQjjCkaKzMnM8lDKCstww1J/cUPgBnXJ+1s2GU8QFuJvUKbHmQDhJDhNq7gTR\ngmbxcwLLiEJRs5i7+1PlbbGw0EA3LiISTVeP5qEGM+W7v/k8B+foWw6p6GlwzpC0vL1zP1WQnPfP\nQAk3H3IaJYm8GB/v78692g8IEMMptcEbZRlkF+sIxJoD4SQ5TKi5E0QLmsXPCcTd20yk8odetoLN\nNv5ZCc24euZbACrDHZvmdqOY1/qWQ0p6CoWcjg1bd/X1PvXcte0hFAshKKfNQ1HzngVhe8Wq1VfB\nMmVIfcoNG/yADB2SbOLFg8+U91GcOYf5Lvq0OnNu0/H7YUDNnSBa0Cx+TpB1VkfWrN50rGBPW7m6\nYX+hGQ9PH3VeX1GcFD3I3rRVHIO5qy5m4wg7HuVeBWF7hVdyQyVS6wdUcOYupqqsncWcw6a60JV2\nrF2/BYVstOnk+6Ch5k4QTWgVPyc4kbHHUEWGKFCRQd50821NX1PMxCEr9sVgIVP507My8bLVbK+o\n4YTnMkiBma7UnU2PXwREv3LD+++5C0qw1g8o3UQOKWvJnuYczMwU5BEEe1NzJ4gmiJzMehmkYPbe\nR1EshCBHK3dknSYchYSuWAziczsq0XBmSoei5nDvplt6qvWLG9dBVgoDGzKpft/5Qn+qnkFgGbG+\n3Beb+QElrUZrZyWc6Mkr30pHoUa8s3Z2CzV3gmiCUDe0c1gsZirDAW4WEYlJ1PpFMMW0PdQxHeyt\ncU45159BrRwtOkMxtja/Q6LTCBByw3bpSe1o5gckrJ1l5+cr/IDMHqwXLEP31NrZLdTcCaIJlcnR\n1soIs2o45aXnvtfRCrbg3BnW391n0xW5YS+IcX8hq/QaITcsZsdLKSPIO/W1S09qRzM/IEBcvB3r\nBccPqGh0n4maT9kXXa+snd0yfgNoBDFE/nL7J8FLwMe3/FnN9lY5mdVYRhTq2Wl86y+uxTmX2ReD\ndlawYhK1VDeOfzIPrAAQXXkI3/qLRtll+vWz8Ht19dXWmnFUHIMZMlnIMpwLjI0bZD3JXADL0Nl9\n8fF7P4HwWW83bI+cf6LBDwiwk57Cy36Kb/3FtZi+xFbjVE+Eu8VNNOMgoOZOTDRTl/wYvNT4AVbS\nOjssGm9PIXJOrJx2nzWW4GSq9Z3+8Rwwc2oZkm/VRrRtnNuNbz/x61BjJyAHa/XukpLrmBMq6SkU\nu1RxdMOla65F6u1XkTl25kDev1+WXLgalvX9jnLD+MX/CiWUhlVssg7hzQvxkd+vlbxm3jwdoZnX\nyz/fTOJ0HO/hApqWop5aO7uFcT6aCZI1a9bwAwcOjOTYBAHY4+Q8fjc4DyCQnKvRs3/3278MKxfF\ndb/ztyOsEPjWVz+M0MxruOaa51vu850n/y14ScJv3/B/hljZePHdp9bAysRx3Y1/0/T5Pbu2YsUl\njyH56nvw/6z95pCrA/b/9WoUEqfj+pue6bxzBxhjP+Kcr+m0H425ExPLSwf3Q5KsBuMu2yAqWQ7F\nHiUV467WOaFKJAGrC6dCP2Jm21snmyeOtlU/DZpiJuaZtbNbqLkTE0u1X3e1cZfIyfTaYbEXCmXj\nrubP37vpFkfFMfoL0SgpdXBf1MPCWK3RBG4YeGXt3A3U3ImJpXqCq/rrck5mavShFGXjrhbGYkI+\nKeSUk4pptE5PAireM81M4IaBV9bO3UDNnZhY5GgapRID55WcUKDS6A0XOZmDZj7v5IRWrYStppzq\nlB6fAI1RkHc06s3SkwCRkCVhyYWN1hDDQNg29Gvt3A3U3ImJRdJSKOSiKNQZd3WTkzlo7t7+iLMS\ntrkxlupclE6OJoN5bEg6tgihJtGHQGdriEEjnCT7tXbuBmruxMQi7ALMOuMu22HRXU7mMCi2mSyU\n9FQ51WmSsbRplEoBSNHmckgp0pt1gFfM5+GJtXM3UHMnJpJn9+6zvULSsQbjrm5zMgeNmY5DadXc\ntcTYrhwdJnds3oZCTm9ITwKqTOAGZKzmhk1zuxs+IQ4aau7ERPLCv/w1ZLkI09DLxl33bb7NkUEu\njLQR1GMZUaihNLY3kUMqkcG5QS42WrlDnjp8EJJkjUwGKaj/hDhoXDV3xtjVjLGXGWNHGGOzLfb5\nIGPsOcbYC4yxv/O2TILwlilHCFMwwmW/kCWqVZWTOfrxdoFwK5wOsprtcxvWOqlO41PrKLHS8Yb0\nJACIOpOY+TYmcMPAC2vnbujY3BljEoAvA7gGwCoAH2OMrarbZwrAgwB+m3N+CYCPDqBWgvCMkDOx\nlc7JNTmhveRkDhqhhNHCtZNx0xG72fdiZuVHrFRjehIAhCOOCdyIveirPyEOAzd37lcCOMI5f4Vz\nXgDwBIDr6va5CcCTnPPXAYBzftzbMgnCW8qZozkJJ/MlZ1umrGboJidz0Cxk7SYuQpwFmmNL3IuZ\nlR/JO+lJ8WCtO6QUtU3gLG16FGWVqf6EOAzcXMqWATha9fgNAO+v2+ddABTG2PcBRAHs4px/tf6N\nGGPrAKwDgHPPbe22RxCDxpZBatiw40EAwNNPXwpJN4AA7zonc9BcuuZamMX/0zAZpzjKi+pUp0mm\nlfuipCVRyEVxx+ZtoyirjLgIh3q0du4Wr34rZADvA/BhAFcB+G+MsXfV78Q53805X8M5XzMzM+PR\noQmieyQtAbNKZWJmbe/uQTss9kKrnFBZNxpSnSaZlNmYngQIyevoJ53FJ0R1SHJIN839GIBzqh4v\nd7ZV8waAZzjnac75SQB/D+C93pRIEN6jRBIwq1QmQm4oa4MLmu4HK9Mohwxo4yXZHDV2epJWTk8C\nhOQ1ORZe9Bvn9qCQD9ufEIeAm+b+QwArGWPnM8ZUADcCeKpun28D+FXGmMwYi8AetjnkbakE4Q33\nz66DouRRqlKZWEYUajADVTs10sUurbDlkAZ2bllf3qZE2me2TiLV6UkAcOjg07b6qYd4vEEgPiEO\ng47NnXNuAvgMgGdgN+xvcM5fYIzdyhi71dnnEIDvAfgxgH8G8CjnvLUBNUGMkNMcIUwhVVHEiJxQ\nWS72lJM5aEwjAsY4YpI9qbpz4+1QQ+mGVKdJp+TIIQWxoGMClx69CRzQfkGa17gac+ec7+ecv4tz\nfgHnfKuz7WHO+cNV++zknK/inF/KOf/SoAomiH6pZI5W9ATZTMUKdhylhSLHVHOsa2MhC4xxFEe8\nMGfcMA0dslLAFzfaC77GyQQOqHxCnNuwduDHoml2YuIQDpDzmYpD3ztVpovjKC1MOVa1wrpWC9lN\nPjfhbpD1iAtz3Lk+K44JXMocjzt38QlRrFEYJNTciYnDNtuKYHbnnvK2O+d2o+hka54cUNB0P7x7\n9bWwLLk8GVf2J8/Tn3A1Yn1CxPl0FtCTtuR1665RllVGfEIUn8AGCf1mEBNHK0VMMRMfW4dFWw4Z\nQ/jMV7Hv8asQOf8lmKaCi1dfM+rSxgrxaUx/14vY9/hVCE+/MVbqJ/EJUY029+f3EmruxMQhh5tn\njmZfvwDpoxePoCJ3ZI9egFJJhhI7ATCO9LF31YR6E8Dszj1IvnkhmGRCiZ2AZQaRO3beqMsqc+fc\nbmSNJUM5FuN8NB9B16xZww8cODCSYxOTy47ZtVjzG3+Ld15agxtu/6tRl0MQXcMY+xHnfE2n/ejO\nnZgolgbtX/nCGCpiCMJLqLkTE0U5c3QMFTEE4SXU3ImJQphtieBpgvAr1NyJiUKOGigWQrh7O5lt\nEf6GmjsxUUgRyhwlJgNq7sREIUcSMMfAIZAgBg01d2Ji2D67DmowTZmjxERAzZ2YGKaDAGOVwGmC\n8DPU3ImJoZw5SmZbxARAzZ2YGFTHDVIEThOEn6HmTkwMkp6CWVRx6ZprR10KQQwcau7ExCBpSRSy\ncTLbIiYCau7ExKBEkmOZj0oQg4CaOzER3H/PXVBDKZJBEhMDNXdiItCsFBjjJIMkJgZq7sREoJcz\nR9UOexKEP6DmTkwEwaidOSqCpgnC79BvOjERSLoBy5Lx7tUkgyQmA2ruxEQgaSkUMjGSQRITgzzq\nAobN3gdvROpkDJ/4o/FLuF/MfGPX7yJvhPHxLY8O9bg7t6zHhe8+goBqD7twHsDxQxdg3X9/uGY/\nObKAojE91NoIYpRM1J373Ia1OO2iH2JqxbFRl+Irnt27D1MXH0D0wleHfuxpNYPY8pegROchhZPQ\nT38FS5bP1+yzZ9dWqOEUrDTJIInJYaKa+3TE9hSRtOSIK/EXLx58BpJsQoksDP3YIhP12D9fid+8\n7oco5KKQ636+hROvIxAowTS0oddHEKNiopq7FrblcEo4MeJK/MVU0P6+quEkHn/soaEeu5yJmuEA\ngGImDkmr/fnGHDfIvBEcam0EMUomqrmrUdsVUA2lsX123Yir8Q9BvQAACARKWDj83FCPLekpFPIR\nzO7cAwCw0jGodRdvVcsDAIysNNTaCGKUTFRzlzSj/PV0kGxfvUJ27p4BQHfukod2bC2JYpVfjGVE\nISsFfHH6jR/yAAAb7UlEQVRj5eIt62mUSgFMrbx8qLURxChx1dwZY1czxl5mjB1hjM222e+XGWMm\nY+wG70r0joCWgGXZd29auDjiavyDrCXL39egVhjuscMJWFWZqEUjDACIV7kMSHoShWwMN91821Br\nI4hR0rG5M8YkAF8GcA2AVQA+xhhb1WK/HQD+l9dFeoUSSSD7ztkAAFXPjbga/yBpCeQSZ6BUCkDR\njc4v8Igds+ugBrOwDL28zcjYKUuRSOUiI0cSMMkNkpgw3Ny5XwngCOf8Fc55AcATAK5rst9/BvBN\nAMc9rM8zdm68HWooDfOdGZhFFZKeGnVJvkENJ2AaUyjkoggM8fu61JkfLVSZgc0XSgAAOWoPFT27\ndx/UcAJWOja0ughiHHDT3JcBOFr1+A1nWxnG2DIAHwEwXKlEF8RCFhjjKBoaCtk4ySE94osb10FW\nCrCMKMxMHPIQ5ZBhzR5ay2Yqmaib5najkNPKF++XDu6HJFkwU3rT9yAIv+LVhOqXAGzknJfa7cQY\nW8cYO8AYO3DixAmPDu0OrewKqMBKx6FESA7pBVOOdLxohGEZMaiRJJ7du28ox1Y0W/00n6+dHC9m\n45Cdn28sZP9K5tMkgyQmCzfN/RiAc6oeL3e2VbMGwBOMsZ8DuAHAg4yxBhMPzvluzvkazvmamZmZ\nHkvujaBmj7Ev5AAzHYMaMrBzy/qh1uBHwmF7bNvIKDANHZJk4sWDzwzl2HLUQLEQwt3bH6nZbqVj\nUCL2J7OQJuqbOKcNYsJx09x/CGAlY+x8xpgK4EYAT1XvwDk/n3O+gnO+AsBeALdzzodz++YSKWrA\nNBVccsWHYRoRMMYRk2hStV/E2PZ8oYRCOgSgsqhp0EiRBIrZxolSy4hCUbOYu/tTjgySoRghXxli\nsujY3DnnJoDPAHgGwCEA3+Ccv8AYu5UxduugC/QKSUugmLVdAfOGHdggVqwSvSPpKRRyGjbN7YaR\ns++OxaKmQaNEEjDTjc294MghZyKSU18Ud2zeNpSaCGJccPVZlXO+H8D+um0Pt9j35v7L8h45kkAx\nuRQAsJCXsRyAqtGde7/IVXfPKTMEzhkUPT3w426fXYcrfyONdJNM1GzGvniHIwXIGskgiclkIlao\nPrBtsx2O7MjhVq2+CpYpQx6iJtuvKJGKzHDD1l0o5PShyEzFCuNmmagnMvYqWVXPQCEZJDGhTERz\nl9LzCAQ4zJQt7fjQDdeTHNID5u7+FBQ1B6vq7rmYiUMaghJJrDAWd+nVzN77KIqFMOTpk5DlIkyD\nZJDE5DERzT2mNYYj25pskkP2w0zEthwQY9xAc+OuQSBWGC+0GAEqZOKInPZGQ30EMSlMRHMPOq6A\nyapwZMuIQQ2nsGfX1lGVtegJO0v8q++emxl3DQJJT8EsqvjsjuaJWlY6Bkm2L+rpHMkgicljIpq7\nrKdhWRIuqgpHNg3NDnA4cbTNK4l2qLq9iEiMcQMV466pAediSFoShSYySIFIXeIcSObI6peYPCai\nuQccV8DqcOS8M0SjkxyyZyQ9hWIhhNl7K7mpwrgrHBmsHFKJJGC1UcGYKXuitZDXsWHHgwOthSDG\nkYlo7kpkoaERiCGaIMkhe0bSEjVe6kCVcVeVx7vX7NyyHmrIqJnIrSfjDBWRDJKYVHw/GPn4Yw9h\n6bIUcm+fW7N9yYWrYVnfhxwdvCbba/7qSx9HaNnr5ceFN5fjo3d8zdVrd39+Hc5YdRgItLUBckV4\n6gTSb/9SzbZNc7vx9P73NJVDPn7vJ6Cd/0rT97JSU/hPf/jtlsf60y3rcM5lPwMkE+++omIC14qF\nDMN5sK0mCGIS8f2d+6kjByFJFqy6RnDTzbehkI0tSjmkfsELCEZPICAXEIyehHbh865fu+SMBPSZ\n1xGQC33/y6emkX7t7IZjVBt3VaOd/zOE4m83vI8SSSC+4vm2k7DTp+Whn/5zSGoWLGAhfepsnJoP\ntdz/czseQeLnlyL1+pmuvzcE4Sd8f+decQVsbATWkDTZXrJn11asuCSF1KuX4T+tfRLffOSjiF/4\nL9i58XZXY8uSbqCQD+O3PvKPA6vRSscQPv3nDduVSALZE+fh+t+tNRZ7fO6TOOPK79ekJzW8Vs+C\nc+An/3g5Ns05CpkOeV/tPgkQhN/x/Z27iH0zso3XMVuTncTjj42tDX0D5omjCARKKDoLc4qpCBgD\n4iF32aWSloDZRmXiBbZxVw73brqlvK0cltJkmKRZelI9kp5EIa9VGjtBEG3xfXNXdAOlUgDyzDkN\nz5mGBkmycOrwwRFU1htC3SPUPjnHp1wLucuEbWW25SVi0dB0kJe3ibAUoWKppj49qRnkEUMQ3eH7\n5h7QUyhko1i7fkvDcyLAIRrmDc+NK0LdI9Q+p5z/3WTCzm1YCzWYaasy8YKKcVflglMJS2m0C6hP\nT2qGEh78RYkg/ITvm7scWWh5x5dM20M1oUh+mCX1heQsyFpy4WoAwGXvuxqmqUByYYI2HbHNtooD\nXo5fbdwlKF+U8s1/5VpNwgLAfZtvg6LmB35RIgg/4evmbocjJ1u6AlratD1ks4jkkLKzIOumm28D\n4JigZdyZoAn/+mZmW14ijLuqLzgiLOXi1dc0fU11elI9S1T7YkEeMQThHl839xcPPgNJNmG1cAW8\nY/M2FHLRRSWHlCKNE6KWSxM0NWrbBbyTHUhpNdgXnEpN1WEpzahOT6pHDO/kqoKwCYJoj6+bu4h7\nayaDFJiZGGRtccghH3/sIajhJEpG7ScRy4hCDaVw/z13tX19QE+hWAziziEoTqxMrCaEXI60nxCt\nTk+qR3GGd07m+194RRCTgq+bu4h7SzeRQQqsdBxKOIln945V5GtTTh22F2SZdQuybBM0jojVftxd\n1pINdgGDwjKiUIMZzG1Y64SlGLCM1qtFq9OT6pGiKRTyYWyc2zOwegnCb/i6uSt6GpwzJK3Wd+5W\nSocsF3Ho4NNDrKw3hKpHqHwEQg6pd5BDypEErCEpToTkcTrCnLCUEqxUa7uAyiRs45iRPaRDShmC\n6AZfN3c7HFnHhq27Wu4jhmxiwfH/yC9UPULlI0jm7aGMoN5a9bNz4+1Qg+myFe6gEcZdWtgsh6Vk\nM8GW+1cmYRvlkHZUHjV3gugGfzf3SKNrYT2GE+QQ0twtAholcjSNUikAS5uu2X7R5VfDsmTI0dbD\nMmIRUbHJIqJBsJCxZZeKni2HpaSyrO1rmql+dsyuhRrMwkqRDJIgusHXzV11EY6cMkPgnEHRx18O\nKWlJFHI67ti8rWa7nQnb3gStvIhowDJIwed2PIJiMQhZN8phKUtWrm77GisTg1IX0TetOtr8NMkg\nCaIbfNvcv7hxHWSl0HHhy4atu1DIaQjo4y+HbLcEv1MmrBjLXhiifX0xG0NASzipSRVtfits1U8a\n22cr7pBhbTjafILwG75t7iLmrWC0nkwVFBdBWPaze/dBCSdbjj3bmbDJlpmwsm4vIrrkig8Pssza\nmtJxKJEEpA6pSYLyJKxa+bUUF6VhaPMJwk/4trkLSV0623nhSykdb7k6clw4dPBpyHIRVqr5gizL\nyYQtHH+t6fOSlkQxG2+5iGgQmOkY1FAawUjn4TGgMgmrV3nS2FF+w9HmE4Sf8G1zl/UMOAfm851N\nwUxDh6Lk8YWNt3Tcd1QINU+rBVl5Q5igNVf9yENwg6zHTEXAGLeDyNvIIAWVSdiKJ41MMkiC6Anf\nNndJT6Ho0v+7aNjDAVOR8ZVDCjWPUPfUk8w6mbBN5JD2IqLU0I23qh0g260SFohJ2GpPGjmSJDdI\ngugB3zZ3OeL+ji+dtXXi1Ra144ZYkJUymzdJ9fTzbBO0JqofJTOPQIA3rGwdNIl85UKUzDTaCjSj\nWKX6sbX5BkpD0uYThJ/wbXNXXI7zAsB8xh66qR4OGDcCehKFnNZyQdba9VtQyDY3QdMjrb3UB8mq\n1VfBMuWm2vxWiElYwE6XYgxD0+YThJ9Y1Bmqu7bchhWXv4iAZNZs5wC0qRxSLhe+zO7cg6efvgxS\ntHF15Nfv/QQULYcbPv31ruv70y3rsPzi1/HTF87D3dsf6fr11cguFmSZmammJmhiEZEI+BgWH7rh\nenz329vAJLNBm98KMx2DfuYRfOfJX8HZ77MnxXPp1itbCYJozqK+cz99Kgt9+g2AcfBSoPwPpQCM\nk+fgnePu7twBWw4pNVHMaBccgX7Bv/ZU38x0DtGzD+PsmNl55w4okSSsTPvzEZmw9SZoctSAZUm4\naPW1fdfRLcaRVTCOXOJ6/8Sx05A+tRy8FECpEELqrV/CmySDJIiucXXnzhi7GsAuABKARznnc3XP\n/y6AjQAYgBSA2zjnvXXELlA0+6/+xX+6pG/HQCsdR2imUUaohBNQ1Dzu23wb7trWXZC27AzzqFp/\nK4e+sPEWvPeqfMcl+GZKgySZOHRwf43kMaClUMjG8ZtDlEEKbrzzsa72/+QfN06AX3+TR8UQxATR\n8c6dMSYB+DKAawCsAvAxxtiqut1eBfDvOeeXAfgTAEMRJXtpBWtb1Gaxo2p15PbZdVBD9gSlSAPq\nqj7HBEt2EYHXDqHiEaqeVuSdBVvRUK3qR2kTNUgQhD9xMyxzJYAjnPNXOOcFAE8AuK56B875P3DO\n33Ee/gDAcm/LbI6XVrAiLGJp1fDudLBidNWLkkZyJgalPq0NxLGFqqcV6bIJWsUTXQR8UP4oQUwW\nbpr7MgBHqx6/4WxrxScBNDVHZ4ytY4wdYIwdOHHihPsqW+ClFWzWiXALV7lDauHK190qaZ7duw9q\nZAEA+rY2EMcWqp5WpKUoOGfl4SAAWDj8nL2IqEXUIEEQ/sTTCVXG2H+A3dw3Nnuec76bc76Gc75m\nZmamr2N5bQU7n3dWR2qV2TtVt8fKLUtuqqRpx/MH9tt2AZYMNZTCA9s291ybpKdQyEcwu7P98NOd\n99xnZ8JWfVLQw064NClOCGKicNPcjwE4p+rxcmdbDYyx9wB4FMB1nPN5b8przdKgXXqncWi33L39\nERQLIchVTVzSUzCLKvKppU2VNO2YclKTMqeWIxAoQUr3/i2RuojHK2Zi5eEgoLJiNdlhSIcgCH/h\nprn/EMBKxtj5jDEVwI0AnqregTF2LoAnAXycc/5T78tsRIxDi+EUL6iXQ9pWtXGYVQtrXNfnDO8U\n3joLABCLdD8hK+hm+MlKx6FGEmU5pKLZAR/qzLk9H58giMVHx+bOOTcBfAbAMwAOAfgG5/wFxtit\njLFbnd3+CMA0gAcZY88xxg4MrGIHxbGCPVXw7o7UzNQ2ccWxqhVhzztm17p+r7Jx2Ul7rDvYoxxy\nx+w6e/jJ5YSoyIR9/sB+AEBAT6GQjWLt+i09HZ8giMWJK50753w/gP112x6u+vpTAD7lbWntEVaw\n3WrP22EZUahn22ERshTG6l8zkD12IYqOkkakArmuL68hfsF7YVl/CznaW9KTUO8INU8nCk5ikRgW\nkkkGSRATyaJdoToIK1jTGb+fDjLEpJydOWpo5aGfSMT9SlNR300339YxAq8dYnjH7fCTkEuGtIKj\n2Em69tghCMI/LN7mPgB/chHlpoWL0MJ2I88bKhYy9rdJjbpfB69U5bdamXjNJGc3CPWOUPN0Yj4P\ncG4PW718cD8kyYRFMkiCmDgWZXPfPrsOajANy/D2jnTBGTlR9VzZMiCVZ2Wf8YDuTg5576ZboKg5\nmI5MU3i+PP5Y90NIcjSFYiHk2nhs09xuFPIaJD1ZXqmaT3X2UicIwl8syuY+HQQYA0yX49Bu+eyO\n3TCLKiQ9BVk3YFky3u2YbRUzccguh1amg/Z4d9EZ/7Y9XyycOnyw65qkiHsZpMDMxCFrifJK1VYB\nHwRB+JdF2dw1Z2HOIKxgC9k4JC1pyyAzsbIBl5V2H6Jdlmmm7XFykUIUDXeO/KtHiSS6nhA103Eo\n4aSj2GFIS2Q9QBCTxqJs7qozDn1qAP7kliOHlCMJmJmpyvZ0FGowjZ0bb+9cn7P8/0TWHhYRKUTh\nSGMEXjuEcVm3vjCWoUNRc5CnTqKQ03HnPfd19XqCIBY/i7K5S1EDpqngsvdd7fl7W0YUasiAGk6h\nVNVUi4YGxjhioc6LkSTdQLEQxuy9j9rvqU2jVApA6lIOKYzLzC5X4YpVu+HTftH1kA5BEP5gcTZ3\nLYFCJl7jWe4VookHAiUU05XM0ZwzxKKFOsshRX2COzZva/B8cYMwLhMqHrcI2aQkWSSDJIgJZVE2\ndzuVaDB3pHlDrfq6Mqa/4CwwVfXOcshm+a1mxv2YvUAYly10uf7pZL7i505WvwQxmSy65n7/PXdB\nDaUG1rQW8hVliVFltnXJFR+GaSodgzfmNqyFGsw01GcZMShNIvDaIYzLPruju+yTjXN7UMjbSh23\nK1sJgvAXi665a1YKjHGYhtZ55x5YtfoqWKaMUimAqZWXl7d/6IbrUXSUNO2Yjtjj5PVulaZhe74c\nOtjU6r4pwrisF8Tq3U4BHwRB+JPF19ydMe9ByCABu4kXsjEUsjHcdPNtNc+Z6RjCp/0C39n3fnxn\n3/vxrb9snNAVK1vrx8kLjhwyHiw1vKYVwrisF6x03DYu606gQxCET1h0zb1UYjDmlyOVc2/i1S2p\nn16C1MuXNm5/9TxkTy1HKR8BC5QQO/swvrhxXc0+wqIgURfcJBYSBasi8Nqxc8t6qCGj5+GnxKvL\nsPDyGmyaG0qcLUEQY8aiW7r4B//Nlhfio4M7xsc2PNZ0+8e3PFr++mtzf4jwlX+HqbrRoYCeQrEY\nbBgnT5khOwIv6i6ur9q4rBdu/qP+Q8MJgli8LLo793EhnXUyVyO1d+K2XUCj/HDD1l0o5HTXFgbV\nxmUEQRDdQs29R+bzHJyjJowaaG8XUOzCHVKEe1SrdwiCINxCzb1HNs3tRjGvQapyity58XaooTRK\nLdwqrXQMisssVkk3YJkyVq2+ypN6CYKYLKi590ExW7swKRaynHHy5nYBlqFDUfINk7DNsGWQsYGs\nwiUIwv9Qc+8D+0680twrMs3m4+Si6cddWMXUG5cRBEF0AzX3PjBTUShqDvduugVAZZw8mW/+bTXK\ncX3t5ZB7dm1tMC4jCILoBmrufSDCOEQ4h3CrvHj1NU33ny/YC5g6ySHNE0dt4zKKxyMIokeoufeB\nWIUqwjkkLYFim3HyTXO7UchpkLT2cX26kEG2GN4hCILoBDX3PjiRsb3dhVOkm9BuN/40qmZ7BiQH\nEEZCEMRkQN2jD2bvfRTFQhiSnsID2zbbdgEd/NOtdAxqB627rBsolQJYcuFqL8slCGKCoObeJ4VM\nHJKWgJSeRyBQgpVqbxdgGVEoahZzd3+q5T6ynmxqXEYQBOEWau59YmVsOWRME26Q7d0qhb/6TKS1\nFa8USaBIMkiCIPqAmnufWEYUajCDUMwed09l27tVViZhm8shn927D2o42XKVK0EQhBuoufeJmbJX\nJKlnHINlSViysv04uZiEVbTmcX0vHdwPSbIGFkZCEMRkQM29TzLiTnzqTVfj5GISVo42l0PGQrYW\nPj+gMBKCICYDau59spCxh2EkyXKdmlTMxFrKIUNOmIeRITdIgiB6x1VzZ4xdzRh7mTF2hDE22+R5\nxhh7wHn+x4yxK7wvdTz53I5HYBbtu/dOMkiBmYlDCTeXQ8rRNEolhmJk2rMaCYKYPDo2d8aYBODL\nAK4BsArAxxhjq+p2uwbASuffOgAPeVznWCNCrM0OMkiBZUShhtLYPtvoDilpSRRyUdyxeZunNRIE\nMVm4uXO/EsARzvkrnPMCgCcAXFe3z3UAvsptfgBgijF2lse1ji2Wsyo174Rgd0JMwk6rjd9+WWsd\n9kEQBOEWNwO7ywAcrXr8BoD3u9hnGYA3+6pukWA6wzFG1t04uZiEXf6Bf8Rff+d9Nc8FtQRSpybm\nukgQxIAY6qwdY2wd7GEbnHvuucM89EA5eXQJGL8CF15+tav93+ExnPb6KrBgruG5YmoaC6+d4XWJ\nBEFMGG6a+zEA51Q9Xu5s63YfcM53A9gNAGvWrOFdVTrG3Ponj3S1/5333DegSgiCIGzcjLn/EMBK\nxtj5jDEVwI0Anqrb5ykAv++oZj4AIME5n4ghGYIgiHGk450759xkjH0GwDMAJABf4Zy/wBi71Xn+\nYQD7AVwL4AiADIBPDK5kgiAIohOuxtw55/thN/DqbQ9Xfc0BfNrb0giCIIheoRWqBEEQPoSaO0EQ\nhA+h5k4QBOFDqLkTBEH4EGruBEEQPoTZQpcRHJixEwBe6/HlSwGc9LCcxcIknvcknjMwmec9iecM\ndH/e53HOZzrtNLLm3g+MsQOc8zWjrmPYTOJ5T+I5A5N53pN4zsDgzpuGZQiCIHwINXeCIAgfslib\n++5RFzAiJvG8J/Gcgck870k8Z2BA570ox9wJgiCI9izWO3eCIAiiDYuuuXcK6/YDjLFzGGP/mzH2\nImPsBcbYemf7EsbYs4yxw87/p426Vq9hjEmMsYOMse86jyfhnKcYY3sZYy8xxg4xxv7NhJz3Z53f\n7+cZY19njIX8dt6Msa8wxo4zxp6v2tbyHBljm5ze9jJj7Kp+jr2omrvLsG4/YAK4k3O+CsAHAHza\nOc9ZAH/DOV8J4G+cx35jPYBDVY8n4Zx3Afge5/wiAO+Fff6+Pm/G2DIAdwBYwzm/FLad+I3w33k/\nBqA+oq3pOTp/4zcCuMR5zYNOz+uJRdXc4S6se9HDOX+Tc/4vztcp2H/sy2Cf6587u/05gOtHU+Fg\nYIwtB/BhAI9Wbfb7OccB/DsAfwYAnPMC53wBPj9vBxlAmDEmA4gA+AV8dt6c878HcKpuc6tzvA7A\nE5zzPOf8Vdj5GFf2euzF1txbBXH7FsbYCgCrAfwTgDOqEq7eAuC3sNUvAbgbQKlqm9/P+XwAJwD8\nT2c46lHGmAafnzfn/BiA+wC8DuBN2Olt/ws+P2+HVufoaX9bbM19omCM6QC+CeC/cM6T1c85ASm+\nkToxxn4TwHHO+Y9a7eO3c3aQAVwB4CHO+WoAadQNRfjxvJ1x5utgX9zOBqAxxn6veh8/nnc9gzzH\nxdbcXQVx+wHGmAK7sX+Nc/6ks/ltxthZzvNnATg+qvoGwK8A+G3G2M9hD7f9GmPsL+Hvcwbsu7M3\nOOf/5DzeC7vZ+/28/yOAVznnJzjnRQBPAvi38P95A63P0dP+ttiau5uw7kUPY4zBHoM9xDn/QtVT\nTwH4A+frPwDw7WHXNig455s458s55ytg/1z/lnP+e/DxOQMA5/wtAEcZY+92Nv06gBfh8/OGPRzz\nAcZYxPl9/3XYc0t+P2+g9Tk+BeBGxliQMXY+gJUA/rnno3DOF9U/2EHcPwXwMwBbRl3PgM7xV2F/\nVPsxgOecf9cCmIY9u34YwP8HYMmoax3Q+X8QwHedr31/zgAuB3DA+XnvA3DahJz3HwN4CcDzAP4C\nQNBv5w3g67DnFIqwP6V9st05Atji9LaXAVzTz7FphSpBEIQPWWzDMgRBEIQLqLkTBEH4EGruBEEQ\nPoSaO0EQhA+h5k4QBOFDqLkTBEH4EGruBEEQPoSaO0EQhA/5v9M5xt2myXf0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22b915b3f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dqn.test(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas = np.array([env.data[i:i+10] for i in range(0, env.data.size-10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = np.array([dqn.forward(env.data[i:i+10]) for i in range(0, env.data.size-10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(dqn.model.predict(datas[:, np.newaxis, :]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22c1d746208>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n2dD/Azc/XaOBhR+6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c1d746240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(env.data)\n",
    "plt.scatter(range(10, 100), env.data[10:100], c=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rewards = [(env.data[i+10] - env.data[i+9])*(v-0.5)*2 for (i, v) in enumerate(pred)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22c1e811c18>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22c1e79ce10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.57142857142857"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
